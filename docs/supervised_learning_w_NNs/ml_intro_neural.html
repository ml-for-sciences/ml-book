
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Computational neurons &#8212; Machine Learning for Scientists</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Training" href="ml_training_regularization.html" />
    <link rel="prev" title="Supervised Learning with Neural Networks" href="ml_supervised_w_NNs.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/cluster.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning for Scientists</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../structuring_data/ml_without_neural_network.html">
   Structuring Data without Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-1.html">
     Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-2.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-3.html">
     t-SNE as a Nonlinear Visualization Technique
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-4.html">
     Clustering Algorithms: the example of
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/pca.html">
     Exercise: Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/Dimensionality_reduction.html">
     Exercise: Dimensionality Reduction
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs.html">
   Supervised Learning without Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-1.html">
     Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-2.html">
     Binary Classification and Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-3.html">
     More than two classes: Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Linear-regression.html">
     Exercise: Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Classification.html">
     Exercise: Classification without Neural Networks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="ml_supervised_w_NNs.html">
   Supervised Learning with Neural Networks
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Computational neurons
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_training_regularization.html">
     Training
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_convolutional.html">
     Advanced Layers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_rnn.html">
     Recurrent neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Classification-2.html">
     Exercise: Dense Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="NN-opt-reg.html">
     Exercise: Machine Learning Optimizers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="NN-opt-reg.html#exercise-learning-rate-scheduling">
     Exercise: Learning Rate Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="NN-opt-reg.html#exercise-regularizing-neural-networks">
     Exercise: Regularizing Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="CNNs.html">
     Exercise: Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="exoplanets_RNN_CNN.html">
     Exercise: Discovery of Exoplanets with RNNs and CNNs
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../unsupervised_learning/ml_unsupervised.html">
   Unsupervised Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-1.html">
     Restricted Boltzmann Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-2.html">
     Training an RNN without Supervision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-3.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-4.html">
     Generative Adversarial Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Denoising.html">
     Exercise: Denoising with Restricted Boltzmann Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Molecule_gen_RNN.html">
     Exercise: Molecule Generation with an RNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Anomaly_Detection_RNN_AE_VAE.html">
     Exercise: Anomaly Detection
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../interpretability/ml_interpretability.html">
   Interpretability of Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-1.html">
     Dreaming and the Problem of Extrapolation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-2.html">
     Adversarial Attacks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-3.html">
     Interpreting Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/Transfer-learning-attacks.html">
     Exercise: Transfer Learning and Adversarial Attacks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning.html">
   Reinforcement Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-1.html">
     Exploration versus Exploitation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-2.html">
     Finite Markov Decision Process
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-3.html">
     Policies and Value Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-4.html">
     Temporal-difference Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-5.html">
     Function Approximation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../conclusion/ml_conclusion.html">
   Concluding Remarks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  About us
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../about_us.html">
   Who we are
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/docs/supervised_learning_w_NNs/ml_intro_neural.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-simple-network-structure">
   A simple network structure
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="computational-neurons">
<span id="sec-supervised"></span><h1>Computational neurons<a class="headerlink" href="#computational-neurons" title="Permalink to this headline">¶</a></h1>
<p>The basic building block of a neural network is the neuron. Let us
consider a single neuron which we assume to be connected to <span class="math notranslate nohighlight">\(k\)</span> neurons
in the preceding layer, see <a class="reference internal" href="#fig-nn-act"><span class="std std-numref">Fig. 13</span></a> left side. The neuron
corresponds to a function <span class="math notranslate nohighlight">\(f:\mathbb{R}^k\to \mathbb{R}\)</span> which is a
composition of a linear function <span class="math notranslate nohighlight">\(q:\mathbb{R}^k\to \mathbb{R}\)</span> and a
non-linear (so-called <em>activation function</em>) <span class="math notranslate nohighlight">\(g:\mathbb{R}\to \mathbb{R}\)</span>. Specifically,</p>
<div class="math notranslate nohighlight">
\[ f(z_1,\ldots,z_k)
    =
    g(q(z_1,\ldots,z_k)),\]</div>
<p>where <span class="math notranslate nohighlight">\(z_1, z_2, \dots, z_k\)</span> are the outputs
of the neurons from the preceding layer to which the neuron is
connected.</p>
<p>The linear function is parametrized as</p>
<div class="math notranslate nohighlight">
\[q(z_1,\ldots,z_k) = \sum_{j=1}^k w_jz_j + b.\]</div>
<p>Here, the real numbers
<span class="math notranslate nohighlight">\(w_1, w_2, \dots, w_k\)</span> are called <em>weights</em> and can be thought of as the
“strength” of each respective connection between neurons in the
preceding layer and this neuron. The real parameter <span class="math notranslate nohighlight">\(b\)</span> is known as the
<em>bias</em> and is simply a constant offset <a class="footnote-reference brackets" href="#id2" id="id1">2</a>. The weights and biases are
the variational parameters we will need to optimize when we train the
network.</p>
<p>The activation function <span class="math notranslate nohighlight">\(g\)</span> is crucial for the neural network to be able
to approximate any smooth function, since so far we merely performed a
linear transformation. For this reason, <span class="math notranslate nohighlight">\(g\)</span> has to be nonlinear. In
analogy to biological neurons, <span class="math notranslate nohighlight">\(g\)</span> represents the property of the neuron
that it “spikes”, i.e., it produces a noticeable output only when the
input potential grows beyond a certain threshold value. The most common
choices for activation functions, shown in <a class="reference internal" href="#fig-nn-act"><span class="std std-numref">Fig. 13</span></a>,
include:</p>
<div class="figure align-default" id="fig-nn-act">
<img alt="../../_images/act_functions.png" src="../../_images/act_functions.png" />
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text"><strong>Left: schematic of a single neuron and
its functional form. Right: examples of the commonly used activation
functions: ReLU, sigmoid function and hyperbolic
tangent.</strong></span><a class="headerlink" href="#fig-nn-act" title="Permalink to this image">¶</a></p>
</div>
<ul class="simple">
<li><p><em>ReLU</em>: ReLU stands for rectified linear unit and is zero for all
numbers smaller than zero, while a linear function for all positive
numbers.</p></li>
<li><p><em>Sigmoid</em>: The sigmoid function, usually taken as the logistic
function, is a smoothed version of the step function.</p></li>
<li><p><em>Hyperbolic tangent</em>: The hyperbolic tangent function has a similar
behaviour as sigmoid but has both positive and negative values.</p></li>
<li><p><em>Softmax</em>: The softmax function is a common activation function for
the last layer in a classification problem (see below).</p></li>
</ul>
<p>The choice of activation function is part of the neural network
architecture and is therefore not changed during training (in contrast
to the variational parameters weights and bias, which are adjusted
during training). Typically, the same activation function is used for
all neurons in a layer, while the activation function may vary from
layer to layer. Determining what a good activation function is for a
given layer of a neural network is typically a heuristic rather than
systematic task.</p>
<p>Note that the softmax provides a special case of an activation function
as it explicitly depends on the output of the <span class="math notranslate nohighlight">\(q\)</span> functions in the other
neurons of the same layer. Let us label by <span class="math notranslate nohighlight">\(l=1,\ldots,n \)</span> the <span class="math notranslate nohighlight">\(n\)</span>
neurons in a given layer and by <span class="math notranslate nohighlight">\(q_l\)</span> the output of their respective
linear transformation. Then, the <em>softmax</em> is defined as</p>
<div class="math notranslate nohighlight">
\[g_l(q_1,\ldots, q_n)= \frac{e^{-q_{l}}}{\sum_{l'=1}^ne^{-q_{l'}}}\]</div>
<p>for the output of neuron <span class="math notranslate nohighlight">\(l\)</span>. A useful property of softmax is that
<span class="math notranslate nohighlight">\(\sum_l g_l(q_1,\ldots, q_n)=1,\)</span> so that the layer output can be
interpreted as a probability distribution. The softmax function is thus
a continuous generalization of the argmax function introduced in the
previous chapter.</p>
<div class="section" id="a-simple-network-structure">
<h2>A simple network structure<a class="headerlink" href="#a-simple-network-structure" title="Permalink to this headline">¶</a></h2>
<p>Now that we understand how a single neuron works, we can connect many of
them together and create an artificial neural network. The general
structure of a simple (feed-forward) neural network is shown in
<a class="reference internal" href="#fig-simple-network"><span class="std std-numref">Fig. 14</span></a>. The first and last layers are the input
and output layers (blue and violet, respectively, in
<a class="reference internal" href="#fig-simple-network"><span class="std std-numref">Fig. 14</span></a>) and are called <em>visible layers</em> as they
are directly accessed. All the other layers in between them are neither
accessible for input nor providing any direct output, and thus are
called <em>hidden layers</em> (green layer in <a class="reference internal" href="#fig-simple-network"><span class="std std-numref">Fig. 14</span></a>.</p>
<div class="figure align-default" id="fig-simple-network">
<img alt="../../_images/simple_network.png" src="../../_images/simple_network.png" />
<p class="caption"><span class="caption-number">Fig. 14 </span><span class="caption-text"><strong>Architecture and variational
parameters.</strong></span><a class="headerlink" href="#fig-simple-network" title="Permalink to this image">¶</a></p>
</div>
<p>Assuming we can feed the input to the network as a vector, we denote the
input data with <span class="math notranslate nohighlight">\({\boldsymbol{x}}\)</span>. The network then transforms this
input into the output <span class="math notranslate nohighlight">\({\boldsymbol{F}}({\boldsymbol{x}})\)</span>, which in
general is also a vector. As a simple and concrete example, we write the
complete functional form of a neural network with one hidden layer as
shown in <a class="reference internal" href="#fig-simple-network"><span class="std std-numref">Fig. 14</span></a>,</p>
<div class="math notranslate nohighlight" id="equation-eq-2-layer-nn">
<span class="eqno">(34)<a class="headerlink" href="#equation-eq-2-layer-nn" title="Permalink to this equation">¶</a></span>\[{\boldsymbol{F}}({\boldsymbol{x}})
    =
    {\boldsymbol{g}}^{[2]}\left(
    W^{[2]}{\boldsymbol{g}}^{[1]}
    \left(W^{[1]}{\boldsymbol{x}}+{\boldsymbol{b}}^{[1]}\right)+{\boldsymbol{b}}^{[2]}
    \right).\]</div>
<p>Here, <span class="math notranslate nohighlight">\(W^{[n]}\)</span> and
<span class="math notranslate nohighlight">\({\boldsymbol{b}}^{[n]}\)</span> are the weight matrix and bias vectors of the
<span class="math notranslate nohighlight">\(n\)</span>-th layer. Specifically, <span class="math notranslate nohighlight">\(W^{[1]}\)</span> is the <span class="math notranslate nohighlight">\(k\times l\)</span> weight matrix
of the hidden layer with <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(l\)</span> the number of neurons in the input
and hidden layer, respectively. <span class="math notranslate nohighlight">\(W_{ij}^{[1]}\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-the entry of
the weight vector of the <span class="math notranslate nohighlight">\(i\)</span>-th neuron in the hidden layer, while
<span class="math notranslate nohighlight">\(b_i^{[1]}\)</span> is the bias of this neuron. The <span class="math notranslate nohighlight">\(W_{ij}^{[2]}\)</span> and
<span class="math notranslate nohighlight">\({\boldsymbol{b}}_i^{[2]}\)</span> are the respective quantities for the output
layer. This network is called <em>fully connected</em> or <em>dense</em>, because each
neuron in a given layer takes as input the output from all the neurons
in the previous layer, in other words all weights are allowed to be
non-zero.</p>
<p>Note that for the evaluation of such a network, we first calculate all
the neurons’ values of the first hidden layer, which feed into the
neurons of the second hidden layer and so on until we reach the output
layer. This procedure, which is possible only for feed-forward neural
networks, is obviously much more efficient than evaluating the nested
function of each output neuron independently.</p>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>Note that this bias is unrelated to the bias we learned about in
regression.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/supervised_learning_w_NNs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="ml_supervised_w_NNs.html" title="previous page">Supervised Learning with Neural Networks</a>
    <a class='right-next' id="next-link" href="ml_training_regularization.html" title="next page">Training</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Machine Learning for Science Team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>