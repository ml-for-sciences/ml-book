
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Introduction &#8212; Machine Learning for Scientists</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Structuring Data without Neural Networks" href="../structuring_data/ml_without_neural_network.html" />
    <link rel="prev" title="Machine Learning for Scientists" href="../index.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/cluster.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning for Scientists</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../structuring_data/ml_without_neural_network.html">
   Structuring Data without Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-1.html">
     Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-2.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-3.html">
     t-SNE as a Nonlinear Visualization Technique
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-4.html">
     Clustering Algorithms: the example of
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/pca.html">
     Exercise: Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/Dimensionality_reduction.html">
     Exercise: Dimensionality Reduction
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs.html">
   Supervised Learning without Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-1.html">
     Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-2.html">
     Binary Classification and Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-3.html">
     More than two classes: Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Linear-regression.html">
     Exercise: Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Classification.html">
     Exercise: Classification without Neural Networks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_w_NNs/ml_supervised_w_NNs.html">
   Supervised Learning with Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/ml_intro_neural.html">
     Computational neurons
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/ml_training_regularization.html">
     Training
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/ml_convolutional.html">
     Advanced Layers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/ml_rnn.html">
     Recurrent neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/Classification-2.html">
     Exercise: Dense Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html">
     Exercise: Machine Learning Optimizers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html#exercise-learning-rate-scheduling">
     Exercise: Learning Rate Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html#exercise-regularizing-neural-networks">
     Exercise: Regularizing Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/CNNs.html">
     Exercise: Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/exoplanets_RNN_CNN.html">
     Exercise: Discovery of Exoplanets with RNNs and CNNs
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../unsupervised_learning/ml_unsupervised.html">
   Unsupervised Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-1.html">
     Restricted Boltzmann Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-2.html">
     Training an RNN without Supervision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-3.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-4.html">
     Generative Adversarial Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Denoising.html">
     Exercise: Denoising with Restricted Boltzmann Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Molecule_gen_RNN.html">
     Exercise: Molecule Generation with an RNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Anomaly_Detection_RNN_AE_VAE.html">
     Exercise: Anomaly Detection
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../interpretability/ml_interpretability.html">
   Interpretability of Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-1.html">
     Dreaming and the Problem of Extrapolation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-2.html">
     Adversarial Attacks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-3.html">
     Interpreting Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/Transfer-learning-attacks.html">
     Exercise: Transfer Learning and Adversarial Attacks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning.html">
   Reinforcement Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-1.html">
     Exploration versus Exploitation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-2.html">
     Finite Markov Decision Process
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-3.html">
     Policies and Value Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-4.html">
     Temporal-difference Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-5.html">
     Function Approximation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../conclusion/ml_conclusion.html">
   Concluding Remarks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  About us
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../about_us.html">
   Who we are
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/docs/introduction/introduction.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-machine-learning-for-the-sciences">
   Why machine learning for the sciences?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview-and-learning-goals">
   Overview and learning goals
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resources">
   Resources
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prerequisites">
   Prerequisites
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZLMLLKHZE0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZLMLLKHZE0');
</script>
<div class="section" id="introduction">
<span id="sec-introduction"></span><h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<div class="section" id="why-machine-learning-for-the-sciences">
<h2>Why machine learning for the sciences?<a class="headerlink" href="#why-machine-learning-for-the-sciences" title="Permalink to this headline">¶</a></h2>
<p>Machine learning and artificial neural networks are everywhere and
change our daily life more profoundly than we might be aware of.
However, these concepts are not a particularly recent invention. Their
foundational principles emerged already in the 1940s. The <em>perceptron</em>,
the predecessor of the artificial neuron, the basic unit of many neural
networks to date, was invented by Frank Rosenblatt in 1958, and even
cast into a hardware realization by IBM.</p>
<p>It then took half a century for these ideas to become technologically
relevant. Now, artificial intelligence based on neural-network
algorithms has become an integral part of data processing with
widespread applications. The reason for its tremendous success is
twofold. First, the availability of big and structured data caters to
machine learning applications. Second, the realization that deep
(feed-forward) networks (made from many “layers” of artificial neurons)
with many variational parameters are tremendously more powerful than
few-layer ones was a big leap, the “deep learning revolution”. Machine
learning refers to algorithms that infer information from data in an
implicit way. If the algorithms are inspired by the functionality of
neural activity in the brain, the term <em>cognitive</em> or <em>neural</em> computing
is used. <em>Artificial neural networks</em> refer to a specific, albeit most
broadly used, ansatz for machine learning. Another field that concerns
iteself with inferring information from data is statistics. In that
sense, both machine learning and statistics have the same goal. However,
the way this goal is achieved is markedly different: while statistics
uses insights from mathematics to extract information, machine learning
aims at optimizing a variational function using available data through
learning. The mathematical foundations of machine learning with neural
networks are poorly understood: we do not know why deep learning works.
Nevertheless, there are some exact results for special cases. For
instance, certain classes of neural networks are a complete basis of
smooth functions, that is, when equipped with enough variational
parameters, they can approximate any smooth high-dimensional function
with arbitrarily precision. Other variational functions with this
property we commonly use are Taylor or Fourier series (with the
coefficients as “variational” parameters). We can think of neural
networks as a class or variational functions, for which the parameters
can be efficiently optimized with respect to a desired objective.</p>
<p>As an example, this objective can be the classification of handwritten
digits from ‘0’ to ‘9’. The input to the neural network would be an
image of the number, encoded in a vector of grayscale values. The output
is a probability distribution saying how likely it is that the image
shows a ‘0’, ‘1’, ‘2’, and so on. The variational parameters of the
network are adjusted until it accomplishes that task well. This is a
classical example of <em>supervised learning</em>. To perform the network
optimization, we need data consisting of input data (the pixel images)
and labels (the integer number shown on the respective image).</p>
<p>Our hope is that the optimized network also recognizes handwritten
digits it has not seen during the learning. This property of a network
is called <em>generalization</em>. It stands in opposition to a tendency called
<em>overfitting</em>, which means that the network has learned specificities of
the data set it was presented with, rather than the abstract features
necessary to identify the respective digit. An illustrative example of
overfitting is fitting a polynomial of degree <span class="math notranslate nohighlight">\(9\)</span> to <span class="math notranslate nohighlight">\(10\)</span> data points,
which will always be a perfect fit. Does this mean that this polynomial
best characterizes the behavior of the measured system? Of course not!
Fighting overfitting and creating algorithms that generalize well are
key challenges in machine learning. We will study several approaches to
achieve this goal.</p>
<div class="figure align-default" id="fig-mnist">
<a class="reference internal image-reference" href="../../_images/mnist_digits.png"><img alt="../../_images/mnist_digits.png" src="../../_images/mnist_digits.png" style="height: 150px;" /></a>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">Examples of the digits from the handwritten
MNIST dataset.</span><a class="headerlink" href="#fig-mnist" title="Permalink to this image">¶</a></p>
</div>
<p>Handwritten digit recognition has become one of the standard benchmark
problems in the field. Why so? The reason is simple: there exists a very
good and freely available data set for it, the MNIST database <a class="footnote-reference brackets" href="#id2" id="id1">2</a>, see
<a class="reference internal" href="#fig-mnist"><span class="std std-numref">Fig. 1</span></a>. This curious fact highlights an important aspect of
machine learning: it is all about data. The most efficient way to
improve machine learning results is to provide more and better data.
Thus, one should keep in mind that despite the widespread applications,
machine learning is not the hammer for every nail. It is most beneficial
if large and <strong>balanced</strong> data sets, meaning roughly that the algorithm
can learn all aspects of the problem equally, in a machine-readable way
are available.</p>
<p>This lecture is an introduction specifically targeting the use of
machine learning in different domains of science. In scientific
research, we see a vastly increasing number of applications of machine
learning, mirroring the developments in industrial technology. With
that, machine learning presents itself as a universal new tool for the
exact sciences, standing side-by-side with methods such as calculus,
traditional statistics, and numerical simulations. This poses the
question, where in the scientific workflow, summerized in
<a class="reference internal" href="#fig-scientific-workflow"><span class="std std-numref">Fig. 2</span></a>, these novel methods are best
employed.</p>
<p>Once a specific task has been identified, applying machine learning to
the sciences does, furthermore, hold its very specific challenges: (i)
scientific data has often very particular structure, such as the nearly
perfect periodicity in an image of a crystal; (ii) typically, we have
specific knowledge about correlations in the data which should be
reflected in a machine learning analysis; (iii) we want to understand
why a particular algorithm works, seeking a fundamental insight into
mechanisms and laws of nature; (iv) in the sciences we are used to
algorithms and laws that provide deterministic answers while machine
learning is intrinsically probabilistic - there is no absolute
certainty. Nevertheless, quantitative precision is paramount in many
areas of science and thus a critical benchmark for machine learning
methods.</p>
<div class="figure align-default" id="fig-scientific-workflow">
<a class="reference internal image-reference" href="../../_images/scientific_workflow.png"><img alt="../../_images/scientific_workflow.png" src="../../_images/scientific_workflow.png" style="height: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">From observations, via
abstraction to building and testing hypothesis or laws, to finally
making predictions.</span><a class="headerlink" href="#fig-scientific-workflow" title="Permalink to this image">¶</a></p>
</div>
<p><strong>A note on the concept of a model</strong><br />
In both machine learning and the sciences, models play a crucial role.
However, it is important to recognize the difference in meaning: In the
natural sciences, a model is a conceptual representation of a
phenomenon. A scientific model does not try to represent the whole
world, but only a small part of it. A model is thus a simplification of
the phenomenon and can be both a theoretical construct, for example the
ideal gas model or the Bohr model of the atom, or an experimental
simplification, such as a small version of an airplane in a wind
channel.</p>
<p>In machine learning, on the other hand, we most often use a complicated
variational function, for example a neural network, to try to
approximate a statistical model. But what is a model in statistics?
Colloquially speaking, a statistical model comprises a set of
statistical assumptions which allow us to calculate the probability
<span class="math notranslate nohighlight">\(P(x)\)</span> of <em>any</em> event <span class="math notranslate nohighlight">\(x\)</span>. The statistical model does not correspond to
the true distribution of all possible events, it simply approximates the
distribution. Scientific and statistical models thus share an important
property: neither claims to be a representation of reality.</p>
</div>
<div class="section" id="overview-and-learning-goals">
<h2>Overview and learning goals<a class="headerlink" href="#overview-and-learning-goals" title="Permalink to this headline">¶</a></h2>
<p>This lecture is an introduction to basic machine learning algorithms for
scientists and students of the sciences. We will cover</p>
<ul class="simple">
<li><p>the most fundamental machine learning algorithms,</p></li>
<li><p>the terminology of the field, succinctly explained,</p></li>
<li><p>the principles of supervised and unsupervised learning and why it is
so successful,</p></li>
<li><p>various architectures of artificial neural networks and the problems
they are suitable for,</p></li>
<li><p>how we find out what the machine learning algorithm uses to solve a
problem.</p></li>
</ul>
<p>The field of machine learning is full of lingo which to the uninitiated
obscures what is at the core of the methods. Being a field in constant
transformation, new terminology is being introduced at a fast pace. Our
aim is to cut through slang with mathematically precise and concise
formulations in order to demystify machine learning concepts for someone
with an understanding of calculus and linear algebra.</p>
<p>As mentioned above, data is at the core of most machine learning
approaches discussed in this lecture. With raw data in many cases very
complex and extremely high dimensional, it is often crucial to first
understand the data better and reduce their dimensionality. Simple
algorithms that can be used before turning to the often heavy machinery
of neural networks will be discussed in the next section,
<a class="reference internal" href="../structuring_data/ml_without_neural_network.html#sec-structuring-data"><span class="std std-ref">Structuring Data without Neural Networks</span></a>.</p>
<p>The machine learning algorithms we will focus on most can generally be
divided into two classes of algorithms, namely <em>discriminative</em> and
<em>generative</em> algorithms as illustrated in <a class="reference internal" href="#fig-overview"><span class="std std-numref">Fig. 3</span></a>.
Examples of discriminative tasks include classification problems, such
as the aforementioned digit classification or the classification into
solid, liquid and gas phases given some experimental observables.
Similarly, regression, in other words estimating relationships between
variables, is a discriminative problem. More specifically, we try to
approximate the conditional probability distribution <span class="math notranslate nohighlight">\(P(y|x)\)</span> of some
variable <span class="math notranslate nohighlight">\(y\)</span> (the label) given some input data <span class="math notranslate nohighlight">\(x\)</span>. As data is provided
in the form of input and target data for most of these tasks, these
algorithms usually employ supervised learning. Discriminative algorithms
are most straight-forwardly applicable in the sciences and we will
discuss them in Secs. <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs.html#sec-linear-methods-for-supervised-learning"><span class="std std-ref">Supervised Learning without Neural Networks</span></a>
and <a class="reference internal" href="../supervised_learning_w_NNs/ml_intro_neural.html#sec-supervised"><span class="std std-ref">Computational neurons</span></a>.</p>
<div class="figure align-default" id="fig-overview">
<img alt="../../_images/overview.png" src="../../_images/overview.png" />
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text"><strong>Overview over the plan of the lecture from the perspective of learning
probability
distributions.</strong></span><a class="headerlink" href="#fig-overview" title="Permalink to this image">¶</a></p>
</div>
<p>Generative algorithms, on the other hand, model a probability
distribution <span class="math notranslate nohighlight">\(P(x)\)</span>. These approaches are—once trained—in principle more
powerful, since we can also learn the joint probability distribution
<span class="math notranslate nohighlight">\(P(x,y)\)</span> of both the data <span class="math notranslate nohighlight">\(x\)</span> and the labels <span class="math notranslate nohighlight">\(y\)</span> and infer the
conditional probability of <span class="math notranslate nohighlight">\(y\)</span>. Still, the more targeted approach of
discriminative learning is better suited for many problems. However,
generative algorithms are useful in the natural sciences, as we can
sample from a known probability distribution, for example for image
denoising, or when trying to find new compounds/molecules resembling
known ones with given properties. These algorithms are discussed in
Sec. <a class="reference internal" href="../unsupervised_learning/ml_unsupervised.html#sec-unsupervised"><span class="std std-ref">Unsupervised Learning</span></a>. The promise of artificial <em>intelligence</em> may
trigger unreasonable expectations in the sciences. After all, scientific
knowledge generation is one of the most complex intellectual processes.
Computer algorithms are certainly far from achieving anything on that
level of complexity and will in the near future not formulate new laws
of nature independently. Nevertheless, researchers study how machine
learning can help with individual segments of the scientific workflow
(<a class="reference internal" href="#fig-scientific-workflow"><span class="std std-numref">Fig. 2</span></a>). While the type of abstraction
needed to formulate Newton’s laws of classical mechanics seems
incredibly complex, neural networks are very good at <em>implicit knowledge
representation</em>. To understand precisely how they achieve certain tasks,
however, is not an easy undertaking. We will discuss this question of
<em>interpretability</em> in Sec. <a class="reference internal" href="../interpretability/ml_interpretability.html#sec-interpretability"><span class="std std-ref">Interpretability of Neural Networks</span></a>.</p>
<p>A third class of algorithms, which does not neatly fit the framework of
approximating a statistical model and thus the distinction into
discriminative and generative algorithms is known as reinforcement
learning. Instead of approximating a statistical model, reinforcement
learning tries to optimize strategies (actions) for achieving a given
task. Reinforcement learning has gained a lot of attention with Google’s
AlphaGo Zero, a computer program that beat the best Go players in the
world. As an example for an application in the sciences, reinforcement
learning can be used to decide on what experimental configuration to
perform next. While the whole topic is beyond the scope of this lecture,
we will give an introduction to the basic concepts of reinforcement
learning in Sec. <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning.html#sec-rl"><span class="std std-ref">Reinforcement Learning</span></a>.</p>
<p>A final note on the practice of learning. While the machine learning
machinery is extremely powerful, using an appropriate architecture and
the right training details, captured in what are called
<em>hyperparameters</em>, is crucial for its successful application. Though
there are attempts to learn a suitable model and all hyperparameters as
part of the overall learning process, this is not a simple task and
requires immense computational resources. A large part of the machine
learning success is thus connected to the experience of the scientist
using the appropriate algorithms. We thus strongly encourage solving the
accompanying exercises carefully and taking advantage of the exercise
classes.</p>
</div>
<div class="section" id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this headline">¶</a></h2>
<p>While it may seem that implementing ML tasks is computationally
challenging, actually almost any ML task one might be interested in can
be done with relatively few lines of code simply by relying on external
libraries or mathematical computing systems such as Mathematica or
Matlab. At the moment, most of the external libraries are written for
the Python programming language. Here are some useful Python libraries:</p>
<ol class="simple">
<li><p><strong>TensorFlow.</strong> Developed by Google, Tensorflow is one of the most
popular and flexible library for machine learning with complex
models, with full GPU support.</p></li>
<li><p><strong>PyTorch.</strong> Developed by Facebook, Pytorch is the biggest rival
library to Tensorflow, with pretty much the same functionalities.</p></li>
<li><p><strong>Scikit-Learn.</strong> Whereas TensorFlow and PyTorch are catered for
deep learning practitioners, Scikit-Learn provides much of the
traditional machine learning tools, including linear regression and
PCA.</p></li>
<li><p><strong>Pandas.</strong> Modern machine learning is largely reliant on big
datasets. This library provides many helpful tools to handle these
large datasets.</p></li>
</ol>
</div>
<div class="section" id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h2>
<p>This course is aimed at students of the (natural) sciences with a basic
mathematics education and some experience in programming. In particular,
we assume the following prerequisites:</p>
<ul class="simple">
<li><p>Basic knowledge of calculus and linear algebra.</p></li>
<li><p>Rudimentary knowledge of statistics and probability theory
(advantageous).</p></li>
<li><p>Basic knowledge of a programming language. For the teaching
assignments, you are free to choose your preferable one. The
solutions will typically be distributed in Python in the form of
Jupyter notebooks.</p></li>
</ul>
<p>Please, don’t hesitate to ask questions if any notions are unclear.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>For further reading, we recommend the following books:</p>
<ul class="simple">
<li><p><strong>ML without neural networks</strong>: <em>The Elements of Statistical
Learning</em>, T. Hastie, R. Tisbshirani, and J. Friedman (Springer)</p></li>
<li><p><strong>ML with neural networks</strong>: <em>Neural Networks and Deep
Learning</em>, M. Nielson (<a class="reference external" href="http://neuralnetworksanddeeplearning.com">http://neuralnetworksanddeeplearning.com</a>)</p></li>
<li><p><strong>Deep Learning Theory</strong>: <em>Deep Learning</em>, I. Goodfellow, Y.
Bengio and A. Courville (<a class="reference external" href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a>)</p></li>
<li><p><strong>Reinforcement Learning</strong>: <em>Reinforcement Learning</em>, R. S.
Sutton and A. G. Barto (MIT Press)</p></li>
</ul>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>http://yann.lecun.com/exdb/mnist</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/introduction"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../index.html" title="previous page">Machine Learning for Scientists</a>
    <a class='right-next' id="next-link" href="../structuring_data/ml_without_neural_network.html" title="next page">Structuring Data without Neural Networks</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Machine Learning for Science Team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>