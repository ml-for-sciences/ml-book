
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Supervised Learning without Neural Networks &#8212; Machine Learning for Sciences</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Unsupervised Learning" href="ml_unsupervised.html" />
    <link rel="prev" title="Structuring Data without Neural Networks" href="ml_without_neural_network-1.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/cluster.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning for Sciences</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_without_neural_network-1.html">
   Structuring Data without Neural Networks
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Supervised Learning without Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_unsupervised.html">
   Unsupervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_interpretability.html">
   Interpretability of Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_reinforcement-learning.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_conclusion.html">
   Concluding Remarks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="pca.html">
   Principle Component Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Dimensionality_reduction.html">
   Dimensionality Reduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Linear-regression.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Classification.html">
   Classification without Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Classification.html#dense-neural-networks">
   Dense Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NN-opt-reg.html">
   Machine Learning Optimizers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NN-opt-reg.html#learning-rate-scheduling">
   Learning Rate Scheduling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NN-opt-reg.html#regularizing-neural-networks">
   Regularizing Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="CNNs.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exoplanets_RNN_CNN.html">
   Discovery of Exoplanets with RNNs and CNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Denoising.html">
   Denoising with Restricted Boltzmann Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Molecule_gen_RNN.html">
   Molecule Generation with an RNN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Anomaly_Detection_RNN_AE_VAE.html">
   Anomaly Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Transfer-learning-attacks.html">
   Transfer Learning and Adversarial Attacks
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/ml_supervised_wo_NNs.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression">
   Linear regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#statistical-analysis">
     Statistical analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization-and-the-bias-variance-tradeoff">
     Regularization and the bias-variance tradeoff
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example">
     Example
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-classifiers-and-their-extensions">
   Linear classifiers and their extensions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#binary-classification-and-support-vector-machines">
     Binary classification and support vector machines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#more-than-two-classes-logistic-regression">
     More than two classes: logistic regression
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="supervised-learning-without-neural-networks">
<span id="sec-linear-methods-for-supervised-learning"></span><h1>Supervised Learning without Neural Networks<a class="headerlink" href="#supervised-learning-without-neural-networks" title="Permalink to this headline">¶</a></h1>
<p><em>Supervised learning</em> is the term for a machine learning task, where we
are given a dataset consisting of input-output pairs
<span class="math notranslate nohighlight">\(\lbrace(\mathbf{x}_{1}, y_{1}), \dots, (\mathbf{x}_{m}, y_{m})\rbrace\)</span> and our
task is to “learn” a function which maps input to output
<span class="math notranslate nohighlight">\(f: \mathbf{x} \mapsto y\)</span>. Here we chose a vector-valued input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and
only a single real number as output <span class="math notranslate nohighlight">\(y\)</span>, but in principle also the
output can be vector valued. The output data that we have is called the
<em>ground truth</em> and sometimes also referred to as “labels” of the input.
In contrast to supervised learning, all algorithms presented so far were
unsupervised, because they just relied on input-data, without any ground
truth or output data.</p>
<p>Within the scope of supervised learning, there are two main types of
tasks: <em>Classification</em> and <em>Regression</em>. In a classification task, our
output <span class="math notranslate nohighlight">\(y\)</span> is a discrete variable corresponding to a classification
category. An example of such a task would be to distinguish stars with a
planetary system (exoplanets) from those without given time series of
images of such objects. On the other hand, in a regression problem, the
output <span class="math notranslate nohighlight">\(y\)</span> is a continuous number or vector. For example predicting the
quantity of rainfall based on meteorological data from the previous
days.</p>
<p>In this section, we first familiarize ourselves with linear methods for
achieving these tasks. Neural networks, in contrast, are a non-linear
method for supervised classification and regression tasks.</p>
<div class="section" id="linear-regression">
<span id="sec-linear-regression"></span><h2>Linear regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h2>
<p>Linear regression, as the name suggests, simply means to fit a linear
model to a dataset. Consider a dataset consisting of input-output pairs
<span class="math notranslate nohighlight">\(\lbrace(\mathbf{x}_{1}, y_{1}), \dots, (\mathbf{x}_{m}, y_{m})\rbrace\)</span>, where
the inputs are <span class="math notranslate nohighlight">\(n\)</span>-component vectors
<span class="math notranslate nohighlight">\(\boldsymbol{x}^{T} = (x_1, x_2, \dots , x_n)\)</span> and the output <span class="math notranslate nohighlight">\(y\)</span> is a
real-valued number. The linear model then takes the form</p>
<div class="math notranslate nohighlight" id="equation-eqn-univariate-linear-model">
<span class="eqno">(8)<a class="headerlink" href="#equation-eqn-univariate-linear-model" title="Permalink to this equation">¶</a></span>\[     f(\boldsymbol{x}|\boldsymbol \beta) = \beta_0 + \sum_{j=1}^{n} \beta_{j}x_{j}\]</div>
<p>or in matrix notation</p>
<div class="math notranslate nohighlight" id="equation-eqn-univariate-linear-model-matrix-form">
<span class="eqno">(9)<a class="headerlink" href="#equation-eqn-univariate-linear-model-matrix-form" title="Permalink to this equation">¶</a></span>\[    f(\boldsymbol{x}|\boldsymbol \beta) = \tilde{\mathbf{x}}^{T}\boldsymbol \beta\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{\tilde{x}}^{T} = (1, x_1, x_2, \dots , x_n)\)</span> and
<span class="math notranslate nohighlight">\(\boldsymbol \beta = (\beta_0, \dots, \beta_n)^{T}\)</span> are <span class="math notranslate nohighlight">\((n+1)\)</span> dimensional row
vectors.</p>
<p>The aim then is to find parameters <span class="math notranslate nohighlight">\(\hat{\boldsymbol \beta}\)</span> such that
<span class="math notranslate nohighlight">\(f(\mathbf{x}|\hat{\boldsymbol \beta})\)</span> is a good <em>estimator</em> for the output value
<span class="math notranslate nohighlight">\(y\)</span>. In order to quantify what it means to be a “good” estimator, one
need to specify a real-valued <em>loss function</em> <span class="math notranslate nohighlight">\(L(\boldsymbol \beta)\)</span>, sometimes
also called a <em>cost function</em>. The good set of parameters
<span class="math notranslate nohighlight">\(\hat{\boldsymbol \beta}\)</span> is then the minimizer of this loss function</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol \beta} = \mathop{\mathrm{argmin}}_{\boldsymbol \beta} L(\boldsymbol \beta).\]</div>
<p>There are many, inequivalent, choices for this loss function. For our
purpose, we choose the loss function to be <em>residual sum of squares</em>
(RSS) defined as</p>
<div class="math notranslate nohighlight" id="equation-eqn-rss">
<span class="eqno">(10)<a class="headerlink" href="#equation-eqn-rss" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
    \textrm{RSS}(\boldsymbol \beta) &amp;= \sum_{i=1}^{m} [y_{i} -  f(\mathbf{x}_{i}|\boldsymbol \beta)]^{2} \\
    &amp;= \sum_{i=1}^{m} \left(y_{i} -  \beta_0 -\sum_{j=1}^{n} \beta_{j}x_{ij}\right)^{2},
\end{split}\end{split}\]</div>
<p>where the sum runs over the <span class="math notranslate nohighlight">\(m\)</span> samples of the dataset.
This loss function is sometimes also called the <em>L2-loss</em> and can be
seen as a measure of the distance between the output values from the
dataset <span class="math notranslate nohighlight">\(y_i\)</span> and the corresponding predictions
<span class="math notranslate nohighlight">\(f(\mathbf{x}_i|\boldsymbol \beta)\)</span>.</p>
<p>It is convenient to define the <span class="math notranslate nohighlight">\(m\)</span> by <span class="math notranslate nohighlight">\((n+1)\)</span> data matrix
<span class="math notranslate nohighlight">\(\widetilde{X}\)</span>, each row of which corresponds to an input sample
<span class="math notranslate nohighlight">\(\mathbf{\tilde{x}}^{T}_{i}\)</span>, as well as the output vector
<span class="math notranslate nohighlight">\(\mathbf{Y}^{T} = (y_{1}, \dots, y_{m})\)</span>. With this notation,
Eq. <a class="reference internal" href="#equation-eqn-rss">(10)</a> can be expressed succinctly as a matrix equation</p>
<div class="math notranslate nohighlight">
\[\textrm{RSS}(\boldsymbol \beta) = (\mathbf{Y} - \widetilde{X}\boldsymbol \beta)^{T}(\mathbf{Y} - \widetilde{X}\boldsymbol \beta).\]</div>
<p>The minimum of <span class="math notranslate nohighlight">\(\textrm{RSS}(\boldsymbol \beta)\)</span> can be easily solved by
considering the partial derivatives with respect to <span class="math notranslate nohighlight">\(\boldsymbol \beta\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
  &amp;\frac{\partial \textrm{RSS}}{\partial \boldsymbol \beta} = -2 \widetilde{X}^{T}(\mathbf{Y} - \widetilde{X}\boldsymbol \beta), \\
  &amp;\frac{\partial^{2} \textrm{RSS}}{\partial \boldsymbol \beta\partial \boldsymbol \beta^{T}} = 2 \widetilde{X}^{T}\widetilde{X}.
\end{split}\end{split}\]</div>
<p>At the minimum, <span class="math notranslate nohighlight">\(\frac{\partial \textrm{RSS}}{\partial \boldsymbol \beta} = 0\)</span> and
<span class="math notranslate nohighlight">\(\frac{\partial^{2} \textrm{RSS}}{\partial \boldsymbol \beta\partial \boldsymbol \beta^{T}}\)</span>
is positive-definite. Assuming <span class="math notranslate nohighlight">\(\widetilde{X}^{T}\widetilde{X}\)</span> is
full-rank and hence invertible, we can obtain the solution
<span class="math notranslate nohighlight">\(\hat{\boldsymbol \beta}\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-eqn-lg-rss-solution">
<span class="eqno">(11)<a class="headerlink" href="#equation-eqn-lg-rss-solution" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
    &amp;\left.\frac{\partial \textrm{RSS}}{\partial \boldsymbol \beta}\right|_{\boldsymbol \beta=\hat{\boldsymbol \beta}} = 0, \\
      \implies &amp;\widetilde{X}^{T}\widetilde{X}\hat{\boldsymbol \beta} = \widetilde{X}^{T}\mathbf{Y}, \\
    \implies 
 &amp; \hat{\boldsymbol \beta }=  (\widetilde{X}^{T}\widetilde{X})^{-1} \widetilde{X}^{T} \mathbf{Y}.
\end{split}\end{split}\]</div>
<p>If <span class="math notranslate nohighlight">\(\widetilde{X}^{T}\widetilde{X}\)</span> is not full-rank,
which can happen if certain data features are perfectly correlated
(e.g., <span class="math notranslate nohighlight">\(x_1 = 2x_3\)</span>), the solution to
<span class="math notranslate nohighlight">\(\widetilde{X}^{T}\widetilde{X}\boldsymbol \beta = \widetilde{X}^{T}\mathbf{Y}\)</span> can
still be found, but it would not be unique. Note that the RSS is not the
only possible choice for the loss function and a different choice would
lead to a different solution.</p>
<p>What we have done so far is uni-variate linear regression, that is
linear regression where the output <span class="math notranslate nohighlight">\(y\)</span> is a single real-valued number.
The generalisation to the multi-variate case, where the output is a
<span class="math notranslate nohighlight">\(p\)</span>-component vector <span class="math notranslate nohighlight">\(\mathbf{y}^{T} = (y_1, \dots y_p)\)</span>, is
straightforward. The model takes the form</p>
<div class="math notranslate nohighlight" id="equation-eqn-multivariate-linear-model">
<span class="eqno">(12)<a class="headerlink" href="#equation-eqn-multivariate-linear-model" title="Permalink to this equation">¶</a></span>\[    f_{k}(\mathbf{x}|\beta) = \beta_{0k} + \sum_{j=1}^{n} \beta_{jk}x_{j}.\]</div>
<p>where the parameters <span class="math notranslate nohighlight">\(\beta_{jk}\)</span> now have an additional index
<span class="math notranslate nohighlight">\(k = 1, \dots, p\)</span>. Considering the parameters <span class="math notranslate nohighlight">\(\beta\)</span> as a <span class="math notranslate nohighlight">\((n+1)\)</span> by
<span class="math notranslate nohighlight">\(p\)</span> matrix, we can show that the solution takes the same form as before
[Eq. <a class="reference internal" href="#equation-eqn-lg-rss-solution">(11)</a>] with <span class="math notranslate nohighlight">\(Y\)</span> as a <span class="math notranslate nohighlight">\(m\)</span> by <span class="math notranslate nohighlight">\(p\)</span> output
matrix.</p>
<div class="section" id="statistical-analysis">
<h3>Statistical analysis<a class="headerlink" href="#statistical-analysis" title="Permalink to this headline">¶</a></h3>
<p>Let us stop here and evaluate the quality of the method we have just
introduced. At the same time, we will take the opportunity to introduce
some statistics notions, which will be useful throughout the book.</p>
<p>Up to now, we have made no assumptions about the dataset we are given,
we simply stated that it consisted of input-output pairs,
<span class="math notranslate nohighlight">\(\{(\mathbf{x}_{1}, y_{1}), \dots,\)</span> <span class="math notranslate nohighlight">\((\mathbf{x}_{m}, y_{m})\}\)</span>. In order to
assess the accuracy of our model in a mathematically clean way, we have
to make an additional assumption. The output data <span class="math notranslate nohighlight">\(y_1\ldots, y_m\)</span> may
arise from some measurement or observation. Then, each of these values
will generically be subject to errors <span class="math notranslate nohighlight">\(\epsilon_1,\cdots, \epsilon_m\)</span> by
which the values deviate from the “true” output without errors,</p>
<div class="math notranslate nohighlight" id="equation-eqn-true-linear-b">
<span class="eqno">(13)<a class="headerlink" href="#equation-eqn-true-linear-b" title="Permalink to this equation">¶</a></span>\[\begin{split}
        y_i &amp;= y^{\textrm{true}}_i + \epsilon_i,\qquad i=1,\cdots,m.
\end{split}\]</div>
<p>We assume that this error <span class="math notranslate nohighlight">\(\epsilon\)</span> is a Gaussian random
variable with mean <span class="math notranslate nohighlight">\(\mu = 0\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, which we denote by
<span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span>. Assuming that a linear model
in Eq. <a class="reference internal" href="#equation-eqn-univariate-linear-model">(8)</a> is a suitable model for our
dataset, we are interested in the following question: How does our
solution <span class="math notranslate nohighlight">\(\hat{\boldsymbol \beta}\)</span> as given in Eq. <a class="reference internal" href="#equation-eqn-lg-rss-solution">(11)</a> compare with the true solution
<span class="math notranslate nohighlight">\(\boldsymbol \beta^{\textrm{true}}\)</span> which obeys</p>
<div class="math notranslate nohighlight" id="equation-eqn-true-linear">
<span class="eqno">(14)<a class="headerlink" href="#equation-eqn-true-linear" title="Permalink to this equation">¶</a></span>\[\begin{split}
        y_i = \beta_0^{\textrm{true}} + \sum_{j=1}^{n} \beta_{j}^{\textrm{true}}x_{ij} + \epsilon_i,\qquad
        i=1,\ldots,m?
\end{split}\]</div>
<p>In order to make statistical statements about this question, we have to
imagine that we can fix the inputs <span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span> of our dataset and
repeatedly draw samples for our outputs <span class="math notranslate nohighlight">\(y_i\)</span>. Each time we will obtain
a different value for <span class="math notranslate nohighlight">\(y_i\)</span> following Eq. <a class="reference internal" href="#equation-eqn-true-linear">(14)</a>, in other words the <span class="math notranslate nohighlight">\(\epsilon_i\)</span> are
uncorrelated random numbers. This allows us to formalise the notion of
an <em>expectation value</em> <span class="math notranslate nohighlight">\(E(\cdots)\)</span> as the average over an infinite
number of draws. For each draw, we obtain a new dataset, which differs
from the other ones by the values of the outputs <span class="math notranslate nohighlight">\(y_i\)</span>. With each of
these datasets, we obtain a different solution <span class="math notranslate nohighlight">\(\hat{\boldsymbol \beta}\)</span> as
given by Eq. <a class="reference internal" href="#equation-eqn-lg-rss-solution">(11)</a>. The expectation value
<span class="math notranslate nohighlight">\(E(\hat{\boldsymbol \beta})\)</span> is then simply the average value we obtained
across an infinite number of datasets. The deviation of this average
value from the “true” value given perfect data is called the <em>bias</em> of
the model,</p>
<div class="math notranslate nohighlight" id="equation-eqn-bias">
<span class="eqno">(15)<a class="headerlink" href="#equation-eqn-bias" title="Permalink to this equation">¶</a></span>\[    \textrm{Bias}(\hat{\boldsymbol \beta}) = E(\hat{\boldsymbol \beta})-\boldsymbol \beta^{\textrm{true}}.\]</div>
<p>For the linear regression we study here, the bias is exactly zero,
because</p>
<div class="math notranslate nohighlight" id="equation-eqn-lg-rss-unbiased">
<span class="eqno">(16)<a class="headerlink" href="#equation-eqn-lg-rss-unbiased" title="Permalink to this equation">¶</a></span>\[\begin{split}    \begin{split}
      E(\hat{\boldsymbol \beta}) &amp;= E\left((\widetilde{X}^{T}\widetilde{X})^{-1} \widetilde{X}^{T} (\mathbf{Y}^{\textrm{true}}+\mathbf{\epsilon})\right)\\
      &amp;=\boldsymbol \beta^{\textrm{true}},
    \end{split}\end{split}\]</div>
<p>where the second line follows because <span class="math notranslate nohighlight">\(E(\mathbf{\epsilon}) = \mathbf{0}\)</span> and
<span class="math notranslate nohighlight">\((\widetilde{X}^{T}\widetilde{X})^{-1} \widetilde{X}^{T} \mathbf{Y}^{\textrm{true}} = \boldsymbol \beta^{\textrm{true}}\)</span>.
Equation <a class="reference internal" href="#equation-eqn-lg-rss-unbiased">(16)</a> implies linear regression is unbiased.
Note that other machine learning algorithms will in general be biased.</p>
<p>What about the standard error or uncertainty of our solution? This
information is contained in the <em>covariance matrix</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \textrm{Var}(\hat{\boldsymbol \beta}) &amp;= E\left([\hat{\boldsymbol \beta} - E(\hat{\boldsymbol \beta})][\hat{\boldsymbol \beta} - E(\hat{\boldsymbol \beta})]^{T} \right).
\end{split}\]</div>
<p>The covariance matrix can be computed for the case of linear regression using the solution in Eq. <a class="reference internal" href="#equation-eqn-lg-rss-solution">(11)</a>, the expectation value in Eq. <a class="reference internal" href="#equation-eqn-lg-rss-unbiased">(16)</a> and the assumption in Eq. <a class="reference internal" href="#equation-eqn-true-linear">(14)</a> that <span class="math notranslate nohighlight">\(Y = Y^{\textrm{true}} + \mathbf{\epsilon}\)</span> yielding</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
    \textrm{Var}(\hat{\boldsymbol \beta}) &amp;= E\left([\hat{\boldsymbol \beta} - E(\hat{\boldsymbol \beta})][\hat{\boldsymbol \beta} - E(\hat{\boldsymbol \beta})]^{T} \right)\\
     &amp;= E\left( \left[ (\widetilde{X}^{T}\widetilde{X})^{-1} \widetilde{X}^{T} \mathbf{\epsilon} \right] \left[ (\widetilde{X}^{T}\widetilde{X})^{-1} \widetilde{X}^{T} \mathbf{\epsilon}\right]^{T} \right) \\
    &amp;= E\left( (\widetilde{X}^{T}\widetilde{X})^{-1} \widetilde{X}^{T} \mathbf{\epsilon} \mathbf{\epsilon}^{T} \widetilde{X} (\widetilde{X}^{T}\widetilde{X})^{-1}  \right).
\end{split}\end{split}\]</div>
<p>This expression can be simplified by using the fact that
our input matrices <span class="math notranslate nohighlight">\(\widetilde{X}\)</span> are independent of the draw such that</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
\textrm{Var}(\hat{\boldsymbol \beta})
    &amp;= (\widetilde{X}^{T}\widetilde{X})^{-1} \widetilde{X}^{T} E(\mathbf{\epsilon} \mathbf{\epsilon}^{T}) \widetilde{X} (\widetilde{X}^{T}\widetilde{X})^{-1} \\
    &amp;= (\widetilde{X}^{T}\widetilde{X})^{-1} \widetilde{X}^{T} \sigma^2 I \widetilde{X} (\widetilde{X}^{T}\widetilde{X})^{-1} \\
    &amp;= \sigma^2 (\widetilde{X}^{T}\widetilde{X})^{-1}.
\end{split}\end{split}\]</div>
<p>Here, the second line follows from the fact that different samples are uncorrelated, which implies that
<span class="math notranslate nohighlight">\(E(\mathbf{\epsilon} \mathbf{\epsilon}^{T}) = \sigma^2 I\)</span> with <span class="math notranslate nohighlight">\(I\)</span> the identity
matrix. The diagonal elements of <span class="math notranslate nohighlight">\(\sigma^2 (\widetilde{X}^{T}\widetilde{X})^{-1}\)</span> then correspond to the
variance</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
    \textrm{Var}(\hat{\boldsymbol \beta}) &amp;= E\left([\hat{\boldsymbol \beta} - E(\hat{\boldsymbol \beta})][\hat{\boldsymbol \beta} - E(\hat{\boldsymbol \beta})]^{T} \right)\\
     &amp;= \sigma^2 (\widetilde{X}^{T}\widetilde{X})^{-1}.
\end{split}\end{split}\]</div>
<p>of the individual parameters <span class="math notranslate nohighlight">\(\beta_i\)</span>. The standard error or uncertainty is then <span class="math notranslate nohighlight">\(\sqrt{\textrm{Var}(\hat{\beta}_{i})}\)</span>.</p>
<p>There is one more missing element: we have not explained how to obtain
the variances <span class="math notranslate nohighlight">\(\sigma^2\)</span> of the outputs <span class="math notranslate nohighlight">\(y\)</span>. In an actual machine
learning task, we would not know anything about the true relation, as
given in Eq. <a class="reference internal" href="#equation-eqn-true-linear">(14)</a>, governing our dataset. The only
information we have access to is a single dataset. Therefore, we have to
estimate the variance using the samples in our dataset, which is given
by</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}^2 = \frac{1}{m - n - 1}\sum_{i=1}^{m} (y_{i} - f(\mathbf{x}_i|\hat{\boldsymbol \beta}))^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> are the output values from our dataset and
<span class="math notranslate nohighlight">\(f(\mathbf{x}_i|\hat{\boldsymbol \beta})\)</span> is the corresponding prediction. Note
that we normalized the above expression by <span class="math notranslate nohighlight">\((m - n - 1)\)</span> instead of <span class="math notranslate nohighlight">\(m\)</span>
to ensure that <span class="math notranslate nohighlight">\(E(\hat{\sigma}^2) = \sigma^2\)</span>, meaning that
<span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>Our ultimate goal is not simply to fit a model to the dataset. We want
our model to generalize to inputs not within the dataset. To assess how
well this is achieved, let us consider the prediction
<span class="math notranslate nohighlight">\(\tilde{\mathbf{a}}^{T} \hat{\boldsymbol \beta}\)</span> on a new random input-output pair
<span class="math notranslate nohighlight">\((\mathbf{a},y_{0})\)</span>. The output is again subject to an error
<span class="math notranslate nohighlight">\(y_{0} = \tilde{\mathbf{a}}^{T}\boldsymbol \beta^{\textrm{true}} + \epsilon\)</span>. In
order to compute the expected error of the prediction, we compute the
expectation value of the loss function over these previously unseen
data. This is also known as the <em>test or generalization error</em> . For the
square-distance loss function, this is the <em>mean square error</em> (MSE)</p>
<div class="math notranslate nohighlight" id="equation-eqn-mse-generalisation-error">
<span class="eqno">(17)<a class="headerlink" href="#equation-eqn-mse-generalisation-error" title="Permalink to this equation">¶</a></span>\[\begin{split}    \begin{split}
        \textrm{MSE}(\hat{\boldsymbol \beta}) =&amp;E\left((y_{0} - \tilde{\mathbf{a}}^{T} \hat{\boldsymbol \beta})^2\right) \\
        = &amp;E\left((\epsilon + \tilde{\mathbf{a}}^{T}\boldsymbol \beta^{\textrm{true}} - \tilde{\mathbf{a}}^{T}\hat{\boldsymbol \beta})^2\right) \\
        = &amp;E(\epsilon^2) +  [\tilde{\mathbf{a}}^{T}(\boldsymbol \beta^{\textrm{true}} - E(\hat{\boldsymbol \beta}))]^2 + E\left( [\tilde{\mathbf{a}}^{T}(\hat{\boldsymbol \beta} - E(\hat{\boldsymbol \beta}))]^2\right) \\
        = &amp;\sigma^2 + [\tilde{\mathbf{a}}^{T}\textrm{Bias}(\hat{\boldsymbol \beta})]^2 + \tilde{\mathbf{a}}^{T} \textrm{Var}(\hat{\boldsymbol \beta}) \tilde{\mathbf{a}}.
    \end{split}\end{split}\]</div>
<p>There are three terms in the expression. The first term is the irreducible or intrinsic uncertainty of the dataset. The
second term represents the bias and the third term is the variance of
the model. For RSS linear regression, the estimate is unbiased so that</p>
<div class="math notranslate nohighlight">
\[\begin{split}%E\left((y_{0} - \mathbf{a}^{T} \hat{\boldsymbol \beta})^2\right) &amp;= \sigma^2 +  E\left( [\mathbf{a}^{T}(\hat{\boldsymbol \beta} - E(\hat{\boldsymbol \beta}))]^2\right)\\
        \textrm{MSE}(\hat{\boldsymbol \beta}) = \sigma^2 + \tilde{\mathbf{a}}^{T} \textrm{Var}(\hat{\boldsymbol \beta}) \tilde{\mathbf{a}}.\end{split}\]</div>
<p>Based on the assumption that the dataset indeed derives from a linear model as given by Eq. <a class="reference internal" href="#equation-eqn-true-linear">(14)</a> with a Gaussian error, it can be shown that the RSS solution, Eq. <a class="reference internal" href="#equation-eqn-lg-rss-solution">(11)</a>, gives the minimum error among all unbiased linear estimators, Eq. <a class="reference internal" href="#equation-eqn-univariate-linear-model">(8)</a>. This is known as the Gauss-Markov theorem.</p>
<p>This completes our error analysis of the method.</p>
</div>
<div class="section" id="regularization-and-the-bias-variance-tradeoff">
<h3>Regularization and the bias-variance tradeoff<a class="headerlink" href="#regularization-and-the-bias-variance-tradeoff" title="Permalink to this headline">¶</a></h3>
<p>Although the RSS solution has the minimum error among unbiased linear
estimators, the expression for the generalisation error,
Eq. <a class="reference internal" href="#equation-eqn-mse-generalisation-error">(17)</a>, suggests that we can
actually still reduce the error by sacrificing some bias in our
estimate.</p>
<div class="figure align-default" id="fig-bias-variance-tradeoff">
<img alt="../_images/Bias-Variance-Tradeoff.png" src="../_images/Bias-Variance-Tradeoff.png" />
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text"><strong>Schematic depiction of the bias-variance tradeoff.</strong></span><a class="headerlink" href="#fig-bias-variance-tradeoff" title="Permalink to this image">¶</a></p>
</div>
<p>A possible way to reduce generalisation error is actually to drop some
data features. From the <span class="math notranslate nohighlight">\(n\)</span> data features
<span class="math notranslate nohighlight">\(\lbrace x_{1}, \dots x_{n} \rbrace\)</span>, we can pick a reduced set
<span class="math notranslate nohighlight">\(\mathcal{M}\)</span>. For example, we can choose
<span class="math notranslate nohighlight">\(\mathcal{M} = \lbrace x_{1}, x_{3}, x_{7} \rbrace\)</span>, and define our new
linear model as</p>
<div class="math notranslate nohighlight" id="equation-eqn-univariate-subset-linear-model">
<span class="eqno">(18)<a class="headerlink" href="#equation-eqn-univariate-subset-linear-model" title="Permalink to this equation">¶</a></span>\[    f(\boldsymbol{x}|\boldsymbol \beta) = \beta_0 + \sum_{j \in \mathcal{M}} \beta_{j}x_{j}.\]</div>
<p>This is equivalent to fixing some parameters to zero, i.e.,
<span class="math notranslate nohighlight">\(\beta_k = 0\)</span> if <span class="math notranslate nohighlight">\(x_{k} \notin \mathcal{M}\)</span>. Minimizing the RSS with
this constraint results in a biased estimator but the reduction in model
variance can sometimes help to reduce the overall generalisation error.
For a small number of features <span class="math notranslate nohighlight">\(n \sim 20\)</span>, one can search exhaustively
for the best subset of features that minimises the error, but beyond
that the search becomes computationally unfeasible.</p>
<p>A common alternative is called <em>ridge regression</em>. In this method, we consider the same linear model given in Eq. <a class="reference internal" href="#equation-eqn-univariate-linear-model">(8)</a> but with a modified loss function</p>
<div class="math notranslate nohighlight" id="equation-eqn-ridge">
<span class="eqno">(19)<a class="headerlink" href="#equation-eqn-ridge" title="Permalink to this equation">¶</a></span>\[   % L_{\textrm{ridge}}(\boldsymbol \beta) = \sum_{i=1}^{m} \left(y_{i} -  \beta_0 +\sum_{j=1}^{n} \beta_{j}x_{ij}\right)^{2} + \lambda \sum_{j=0}^{n} \beta_{j}^{2},
    L_{\textrm{ridge}}(\boldsymbol \beta) = \sum_{i=1}^{m} \left[y_{i} -   f(\boldsymbol{x}_i|\boldsymbol \beta)\right]^{2} + \lambda \sum_{j=0}^{n} \beta_{j}^{2},\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span> is a positive parameter. This is almost the same as
the RSS apart from the term proportional to <span class="math notranslate nohighlight">\(\lambda\)</span> [c.f.
Eq. <a class="reference internal" href="#equation-eqn-rss">(10)</a>{]. The effect of this new term is to penalize
large parameters <span class="math notranslate nohighlight">\(\beta_j\)</span> and bias the model towards smaller absolute
values. The parameter <span class="math notranslate nohighlight">\(\lambda\)</span> is an example of a <em>hyper-parameter</em>,
which is kept fixed during the training. On fixing <span class="math notranslate nohighlight">\(\lambda\)</span> and
minimising the loss function, we obtain the solution</p>
<div class="math notranslate nohighlight" id="equation-eqn-ridge-solution">
<span class="eqno">(20)<a class="headerlink" href="#equation-eqn-ridge-solution" title="Permalink to this equation">¶</a></span>\[    \hat{\boldsymbol \beta}_{\textrm{ridge}} = (\widetilde{X}^{T}\widetilde{X} + \lambda I)^{-1}\widetilde{X}^{T}\mathbf{Y},\]</div>
<p>from which we can see that as <span class="math notranslate nohighlight">\(\lambda \rightarrow \infty\)</span>,
<span class="math notranslate nohighlight">\(\hat{\boldsymbol \beta}_{\textrm{ridge}} \rightarrow \mathbf{0}\)</span>. By computing the
bias and variance,</p>
<div class="math notranslate nohighlight" id="equation-eqn-ridge-bias-variance">
<span class="eqno">(21)<a class="headerlink" href="#equation-eqn-ridge-bias-variance" title="Permalink to this equation">¶</a></span>\[\begin{split}    \begin{split}
        \textrm{Bias}(\hat{\boldsymbol \beta}_{\textrm{ridge}}) &amp;= -\lambda (\widetilde{X}^{T}\widetilde{X} + \lambda I)^{-1} \boldsymbol \beta^{\textrm{true}}\\
        \textrm{Var}(\hat{\boldsymbol \beta}_{\textrm{ridge}}) &amp;= \sigma^2 (\widetilde{X}^{T}\widetilde{X} + \lambda I)^{-1} \widetilde{X}^{T}\widetilde{X}(\widetilde{X}^{T}\widetilde{X} + \lambda I)^{-1},
    \end{split}\end{split}\]</div>
<p>it is also obvious that increasing <span class="math notranslate nohighlight">\(\lambda\)</span> increases
the bias, while reducing the variance. This is the tradeoff between bias
and variance. By appropriately choosing <span class="math notranslate nohighlight">\(\lambda\)</span> it is possible that
generalisation error can be reduced. We will introduce in the next
section a common strategy how to find the optimal value for <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>The techniques presented here to reduce the generalization error, namely
dropping of features and biasing the model to small parameters, are part
of a large class of methods known as <em>regularization</em>. Comparing the two
methods, we can see a similarity. Both methods actually reduce the
complexity of our model. In the former, some parameters are set to zero,
while in the latter, there is a constraint which effectively reduces the
magnitude of all parameters. A less complex model has a smaller variance
but larger bias. By balancing these competing effects, generalisation
can be improved, as illustrated schematically in <a class="reference internal" href="#fig-bias-variance-tradeoff"><span class="std std-numref">Fig. 6</span></a>.</p>
<p>In the next chapter, we will see that these techniques are useful beyond
applications to linear methods. We illustrate the different concepts in
the following example.</p>
<div class="figure align-default" id="fig-ml-workflow">
<img alt="../_images/ML_Workflow.png" src="../_images/ML_Workflow.png" />
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text"><strong>Machine Learning Workflow.</strong></span><a class="headerlink" href="#fig-ml-workflow" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h3>
<p>We illustrate the concepts of linear regression using a medical dataset.
In the process, we will also familiarize ourselves with the standard
machine learning workflow [see <a class="reference internal" href="#fig-ml-workflow"><span class="std std-numref">Fig. 7</span></a>]. For this example, we are given <span class="math notranslate nohighlight">\(10\)</span>
data features, namely age, sex, body mass index, average blood pressure,
and six blood serum measurements from <span class="math notranslate nohighlight">\(442\)</span> diabetes patients, and our
task is train a model <span class="math notranslate nohighlight">\(f(\mathbf{x}|\boldsymbol \beta)\)</span>
[Eq. <a class="reference internal" href="#equation-eqn-univariate-linear-model">(8)</a>] to predict a quantitative
measure of the disease progression after one year.</p>
<p>Recall that the final aim of a machine-learning task is not to obtain
the smallest possible value for the loss function such as the RSS, but
to minimise the generalisation error on unseen data [c.f.
Eq. <a class="reference internal" href="#equation-eqn-mse-generalisation-error">(17)</a>]. The standard approach
relies on a division of the dataset into three subsets: training set,
validation set and test set. The standard workflow is summarised in the Box
below.</p>
<div class="admonition-ml-workflow admonition" id="alg-ml-workflow">
<p class="admonition-title">ML Workflow</p>
<ol>
<li><p>Divide the dataset into training set <span class="math notranslate nohighlight">\(\mathcal{T}\)</span>, validation set
<span class="math notranslate nohighlight">\(\mathcal{V}\)</span> and test set <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>. A common ratio for the
split is <span class="math notranslate nohighlight">\(70 : 15 : 15\)</span>.</p></li>
<li><p>Pick the hyperparameters, e.g., <span class="math notranslate nohighlight">\(\lambda\)</span> in Eq. <a class="reference internal" href="#equation-eqn-ridge">(19)</a>.</p></li>
<li><p>Train the model with only the training set, in other words minimize
the loss function on the training set. [This corresponds to Eq. <a class="reference internal" href="#equation-eqn-lg-rss-solution">(11)</a> or
<a class="reference internal" href="#equation-eqn-ridge-solution">(20)</a> for the linear regression, where <span class="math notranslate nohighlight">\(\widetilde{X}\)</span> only contains the training set.]</p></li>
<li><p>Evaluate the MSE (or any other chosen metric) on the validation set,
[c.f. Eq. <a class="reference internal" href="#equation-eqn-mse-generalisation-error">(17)</a>]</p>
<div class="math notranslate nohighlight">
\[\textrm{MSE}_{\textrm{validation}}(\hat{\boldsymbol \beta}) = \frac{1}{|\mathcal{V}|}\sum_{j\in\mathcal{V}} (y_j - f(\mathbf{x}_j|\hat{\boldsymbol \beta}))^2.\]</div>
<p>This is known as the <em>validation error</em>.</p>
</li>
<li><p>Pick a different value for the hyperparameters and repeat steps <span class="math notranslate nohighlight">\(3\)</span>
and <span class="math notranslate nohighlight">\(4\)</span>, until validation error is minimized.</p></li>
<li><p>Evaluate the final model on the test set</p>
<div class="math notranslate nohighlight">
\[\textrm{MSE}_{\textrm{test}}(\hat{\boldsymbol \beta}) = \frac{1}{|\mathcal{S}|}\sum_{j\in\mathcal{S}} (y_j - f(\mathbf{x}_j|\hat{\boldsymbol \beta}))^2.\]</div>
</li>
</ol>
</div>
<p>It is important to note that the test set <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> was not involved
in optimizing either parameters <span class="math notranslate nohighlight">\(\boldsymbol \beta\)</span> or the hyperparameters such
as <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>Applying this procedure to the diabetes dataset <a class="footnote-reference brackets" href="#id4" id="id1">4</a>, we obtain the
results in <a class="reference internal" href="#fig-regression-on-diabetes-dataset"><span class="std std-numref">Fig. 8</span></a>. We compare RSS linear
regression with the ridge regression, and indeed we see that by
appropriately choosing the regularisation hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span>, the
generalisation error can be minimized.</p>
<p>As side remark regarding the ridge regression, we can see on the left of
<a class="reference internal" href="#fig-ridge-parameters"><span class="std std-numref">Fig. 9</span></a>, that as <span class="math notranslate nohighlight">\(\lambda\)</span> increases, the
magnitude of the parameters, Eq. <a class="reference internal" href="#equation-eqn-ridge-solution">(20)</a>, <span class="math notranslate nohighlight">\(\hat{\boldsymbol \beta}_{\textrm{ridge}}\)</span>
decreases. Consider on the other hand, a different form of
regularisation, which goes by the name <em>lasso regression</em>, where the
loss function is given by</p>
<div class="math notranslate nohighlight" id="equation-eqn-lasso">
<span class="eqno">(22)<a class="headerlink" href="#equation-eqn-lasso" title="Permalink to this equation">¶</a></span>\[    %L_{\textrm{lasso}}(\boldsymbol \beta) = \sum_{i=1}^{m} \left(y_{i} -  \beta_0 +\sum_{j=1}^{n} \beta_{j}x_{ij}\right)^{2} + \alpha \sum_{j=0}^{n} |\beta_{j}|.
    L_{\textrm{lasso}}(\boldsymbol \beta) = \sum_{i=1}^{m} \left[y_{i} -  f(\boldsymbol{x}_i|\boldsymbol \beta)\right]^{2} + \alpha \sum_{j=0}^{n} |\beta_{j}|.\]</div>
<p>Despite the similarities, lasso regression has a very different
behaviour as depicted on the right of <a class="reference internal" href="#fig-ridge-parameters"><span class="std std-numref">Fig. 9</span></a>. Notice that as <span class="math notranslate nohighlight">\(\alpha\)</span> increases
some parameters actually vanish and can be ignored completely. This
actually corresponds to dropping certain data features completely and
can be useful if we are interested in selecting the most important
features in a dataset.</p>
<div class="figure align-default" id="fig-regression-on-diabetes-dataset">
<img alt="../_images/diabetes_ridge_regression.png" src="../_images/diabetes_ridge_regression.png" />
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text"><strong>Ridge Regression on Diabetes patients dataset.</strong> Left: Validation
error versus <span class="math notranslate nohighlight">\(\lambda\)</span>. Right: Test data versus the prediction from the
trained model. If the prediction were free of any error, all the points
would fall on the blue line.</span><a class="headerlink" href="#fig-regression-on-diabetes-dataset" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-ridge-parameters">
<img alt="../_images/ridge_variables.png" src="../_images/ridge_variables.png" />
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text"><strong>Evolution of the model parameters.</strong> Increasing the hyperparameter
<span class="math notranslate nohighlight">\(\lambda\)</span> or <span class="math notranslate nohighlight">\(\alpha\)</span> leads to a reduction of the absolute value of the
model parameters, here shown for the ridge (left) and Lasso (right)
regression for the Diabetes dataset.</span><a class="headerlink" href="#fig-ridge-parameters" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="linear-classifiers-and-their-extensions">
<h2>Linear classifiers and their extensions<a class="headerlink" href="#linear-classifiers-and-their-extensions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="binary-classification-and-support-vector-machines">
<h3>Binary classification and support vector machines<a class="headerlink" href="#binary-classification-and-support-vector-machines" title="Permalink to this headline">¶</a></h3>
<p>In a classification problem, the aim is to categorize the inputs into
one of a finite set of classes. Formulated as a supervised learning
task, the dataset again consists of input-output pairs, i.e.
<span class="math notranslate nohighlight">\(\lbrace(\mathbf{x}_{1}, y_{1}), \dots, (\mathbf{x}_{m}, y_{m})\rbrace\)</span> with
<span class="math notranslate nohighlight">\(\mathbf{x}\in \mathbb{R}^n\)</span>. However, unlike regression problems, the
output <span class="math notranslate nohighlight">\(y\)</span> is a discrete integer number representing one of the classes.
In a binary classification problem, in other words a problem with only
two classes, it is natural to choose <span class="math notranslate nohighlight">\(y\in\{-1, 1\}\)</span>.</p>
<p>We have introduced linear regression in the previous section as a method
for supervised learning when the output is a real number. Here, we will
see how we can use the same model for a binary classification task. If
we look at the regression problem, we first note that geometrically</p>
<div class="math notranslate nohighlight" id="equation-eqn-univariate-linear-model-b">
<span class="eqno">(23)<a class="headerlink" href="#equation-eqn-univariate-linear-model-b" title="Permalink to this equation">¶</a></span>\[     f(\boldsymbol{x}|\boldsymbol \beta) = \beta_0 + \sum_{j=1}^{n} \beta_{j}x_{j} = 0\]</div>
<p>defines a hyperplane perpendicular to the vector with elements
<span class="math notranslate nohighlight">\(\beta_{j\geq1}\)</span>. If we fix the length <span class="math notranslate nohighlight">\(\sum_{j=1}^n \beta_j^2=1\)</span>, then
<span class="math notranslate nohighlight">\(f(\mathbf{x}|\boldsymbol \beta)\)</span> measures the (signed) distance of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to the
hyperplane with a sign depending on which side of the plane the point
<span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> lies. To use this model as a classifier, we thus define</p>
<div class="math notranslate nohighlight" id="equation-eqn-binaryclassifier">
<span class="eqno">(24)<a class="headerlink" href="#equation-eqn-binaryclassifier" title="Permalink to this equation">¶</a></span>\[F(\mathbf{x}|\boldsymbol \beta) = \mathrm{sign} f(\mathbf{x}|\boldsymbol \beta),\]</div>
<p>which yields <span class="math notranslate nohighlight">\(\{+1, -1\}\)</span>. If the two classes are (completely) linearly separable, then the goal of the
classification is to find a hyperplane that separates the two classes in
feature space. Specifically, we look for parameters <span class="math notranslate nohighlight">\(\boldsymbol \beta\)</span>, such
that</p>
<div class="math notranslate nohighlight" id="equation-eqn-separable">
<span class="eqno">(25)<a class="headerlink" href="#equation-eqn-separable" title="Permalink to this equation">¶</a></span>\[y_i \tilde{\mathbf{x}}_i^T\boldsymbol \beta &gt; M, \quad \forall i,\]</div>
<p>where <span class="math notranslate nohighlight">\(M\)</span> is called the <em>margin</em>. The optimal solution <span class="math notranslate nohighlight">\(\hat{\boldsymbol \beta}\)</span> then maximizes this margin. Note that
instead of fixing the norm of <span class="math notranslate nohighlight">\(\beta_{j\geq1}\)</span> and maximizing <span class="math notranslate nohighlight">\(M\)</span>, it is
customary to minimize <span class="math notranslate nohighlight">\(\sum_{j=1}^n \beta_j^2\)</span> setting <span class="math notranslate nohighlight">\(M=1\)</span> in
Eq. <a class="reference internal" href="#equation-eqn-separable">(25)</a>.</p>
<p>In most cases, the two classes are not completely separable. In order to
still find a good classifier, we allow some of the points <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> to
lie within the margin or even on the wrong side of the hyperplane. For
this purpose, we rewrite the optimization constraint
Eq. <a class="reference internal" href="#equation-eqn-separable">(25)</a> to</p>
<div class="math notranslate nohighlight" id="equation-eqn-notseparable">
<span class="eqno">(26)<a class="headerlink" href="#equation-eqn-notseparable" title="Permalink to this equation">¶</a></span>\[y_i \tilde{\mathbf{x}}_i^T\boldsymbol \beta &gt; (1-\xi_i), \textrm{with } \xi_i \geq 0, \quad \forall i.\]</div>
<p>We can now define the optimization problem as finding</p>
<div class="math notranslate nohighlight" id="equation-eqn-optimalclassifierbeta">
<span class="eqno">(27)<a class="headerlink" href="#equation-eqn-optimalclassifierbeta" title="Permalink to this equation">¶</a></span>\[\min_{\boldsymbol \beta,\{\xi_i\}} \frac12 \sum_{j=1}^{n} \beta_j^2 + C\sum_i \xi_i\]</div>
<p>subject to the constraint Eq. <a class="reference internal" href="#equation-eqn-notseparable">(26)</a>. Note that the second term with
hyperparameter <span class="math notranslate nohighlight">\(C\)</span> acts like a regularizer, in particular a lasso
regularizer. As we have seen in the example of the previous section,
such a regularizer tries to set as many <span class="math notranslate nohighlight">\(\xi_i\)</span> to zero as possible.</p>
<div class="figure align-default" id="fig-svm">
<img alt="../_images/SVM_overlap.png" src="../_images/SVM_overlap.png" />
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text"><strong>Binary classification.</strong> Hyperplane separating the two classes and
margin <span class="math notranslate nohighlight">\(M\)</span> of the linear binary classifier. The support vectors are
denoted by a circle around them.</span><a class="headerlink" href="#fig-svm" title="Permalink to this image">¶</a></p>
</div>
<p>We can solve this constrained minimization problem by introducing
Lagrange multipliers <span class="math notranslate nohighlight">\(\alpha_i\)</span> and <span class="math notranslate nohighlight">\(\mu_i\)</span> and solving</p>
<div class="math notranslate nohighlight" id="equation-eqn-svm-lagrange">
<span class="eqno">(28)<a class="headerlink" href="#equation-eqn-svm-lagrange" title="Permalink to this equation">¶</a></span>\[\min_{\beta, \{\xi_i\}} \frac12 \sum_{j=1}^{n} \beta_j^2 + C\sum_i \xi_i - \sum_i \alpha_i [y_i \tilde{\mathbf{x}}_i^T\boldsymbol \beta - (1-\xi_i)] - \sum_i\mu_i\xi_i,\]</div>
<p>which yields the conditions</p>
<div class="math notranslate nohighlight" id="equation-eqn-svm-derivatives">
<span class="eqno">(29)<a class="headerlink" href="#equation-eqn-svm-derivatives" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
  \beta_j &amp;=&amp; \sum_i \alpha_i y_i x_{ij},\\
  0 &amp;=&amp; \sum_i \alpha_i y_i\\
  \alpha_i &amp;=&amp; C-\mu_i, \quad \forall i.
\end{aligned}\end{split}\]</div>
<p>It is numerically simpler to solve the dual problem</p>
<div class="math notranslate nohighlight" id="equation-eqn-svm-dual">
<span class="eqno">(30)<a class="headerlink" href="#equation-eqn-svm-dual" title="Permalink to this equation">¶</a></span>\[\min_{\{\alpha_i\}} \frac12 \sum_{i,i'} \alpha_i \alpha_{i'} y_i y_{i'} \mathbf{x}_i^T \mathbf{x}_{i'} - \sum_i \alpha_i\]</div>
<p>subject to <span class="math notranslate nohighlight">\(\sum_i \alpha_i y_i =0\)</span> and <span class="math notranslate nohighlight">\(0\leq \alpha_i \leq C\)</span> <a class="footnote-reference brackets" href="#id5" id="id2">5</a>. Using Eq. <a class="reference internal" href="#equation-eqn-svm-derivatives">(29)</a>, we can reexpress <span class="math notranslate nohighlight">\(\beta_j\)</span> to find</p>
<div class="math notranslate nohighlight" id="equation-eqn-svm-f">
<span class="eqno">(31)<a class="headerlink" href="#equation-eqn-svm-f" title="Permalink to this equation">¶</a></span>\[f(\mathbf{x}|\{\alpha_i\}) = \sum_i{}' \alpha_i y_i \mathbf{x}^T \mathbf{x}_i + \beta_0,\]</div>
<p>where the sum only runs over the points <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>, which lie within the margin, as all other points have <span class="math notranslate nohighlight">\(\alpha_i\equiv0\)</span> [see Eq. <a class="reference internal" href="#equation-eqn-firstkkt">(33)</a>]. These points are thus called the <em>support vectors</em> and are denoted in <a class="reference internal" href="#fig-svm"><span class="std std-numref">Fig. 10</span></a> with a circle around them. Finally, note that we can use Eq. <a class="reference internal" href="#equation-eqn-firstkkt">(33)</a> again to find <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
<p><strong>The Kernel trick and support vector machines</strong><br />
We have seen in our discussion of PCA that most data is not separable
linearly. However, we have also seen how the kernel trick can help us in
such situations. In particular, we have seen how a non-linear function
<span class="math notranslate nohighlight">\(\boldsymbol \Phi(\mathbf{x})\)</span>, which we first apply to the data <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, can help
us separate data that is not linearly separable. Importantly, we never
actually use the non-linear function <span class="math notranslate nohighlight">\(\boldsymbol \Phi(\mathbf{x})\)</span>, but only the
kernel. Looking at the dual optimization problem
Eq. <a class="reference internal" href="#equation-eqn-svm-dual">(30)</a> and the resulting classifier
Eq. <a class="reference internal" href="#equation-eqn-svm-f">(31)</a>, we see that, as in the case of Kernel PCA, only
the kernel <span class="math notranslate nohighlight">\(K(\mathbf{x}, \mathbf{y}) = \boldsymbol \Phi(\mathbf{x})^T\boldsymbol \Phi(\mathbf{y})\)</span>
enters, simplifying the problem. This non-linear extension of the binary
classifier is called a <em>support vector machine</em>.</p>
</div>
<div class="section" id="more-than-two-classes-logistic-regression">
<h3>More than two classes: logistic regression<a class="headerlink" href="#more-than-two-classes-logistic-regression" title="Permalink to this headline">¶</a></h3>
<p>In the following, we are interested in the case of <span class="math notranslate nohighlight">\(p\)</span> classes with
<span class="math notranslate nohighlight">\(p&gt;2\)</span>. After the previous discussion, it seems natural for the output to
take the integer values <span class="math notranslate nohighlight">\(y = 1, \dots, p\)</span>. However, it turns out to be
helpful to use a different, so-called <em>one-hot encoding</em>. In this
encoding, the output <span class="math notranslate nohighlight">\(y\)</span> is instead represented by the <span class="math notranslate nohighlight">\(p\)</span>-dimensional
unit vector in <span class="math notranslate nohighlight">\(y\)</span> direction <span class="math notranslate nohighlight">\(\mathbf{e}^{(y)}\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-eqn-one-hot-encoding">
<span class="eqno">(32)<a class="headerlink" href="#equation-eqn-one-hot-encoding" title="Permalink to this equation">¶</a></span>\[\begin{split}    y \longrightarrow \mathbf{e}^{(y)} =
    \begin{bmatrix}
        e^{(y)}_1 \\
        \vdots \\
        e^{(y)}_y \\
        \vdots \\
        e^{(y)}_{p}
    \end{bmatrix}
    =
    \begin{bmatrix}
        0 \\
        \vdots \\
        1 \\
        \vdots \\
        0
    \end{bmatrix},\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(e^{(y)}_l = 1\)</span> if <span class="math notranslate nohighlight">\(l = y\)</span> and zero for all other <span class="math notranslate nohighlight">\(l=1,\ldots, p\)</span>. A main advantage of this encoding is that we are
not forced to choose a potentially biasing ordering of the classes as we
would when arranging them along the ray of integers.</p>
<p>A linear approach to this problem then again mirrors the case for linear
regression. We fit a multi-variate linear model,
Eq. <a class="reference internal" href="#equation-eqn-multivariate-linear-model">(12)</a>, to the one-hot encoded
dataset <span class="math notranslate nohighlight">\(\lbrace(\mathbf{x}_{1}, \mathbf{e}^{(y_1)}), \dots, (\mathbf{x}_{m}, \mathbf{e}^{(y_m)})\rbrace\)</span>.
By minimising the RSS, Eq. <a class="reference internal" href="#equation-eqn-rss">(10)</a>, we obtain the solution</p>
<div class="math notranslate nohighlight">
\[\hat{\beta} = (\widetilde{X}^{T}\widetilde{X})^{-1} \widetilde{X}^{T} Y,\]</div>
<p>where <span class="math notranslate nohighlight">\(Y\)</span> is the <span class="math notranslate nohighlight">\(m\)</span> by <span class="math notranslate nohighlight">\(p\)</span> output matrix. The prediction given an input
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is then a <span class="math notranslate nohighlight">\(p\)</span>-dimensional vector
<span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}|\hat{\beta}) = \tilde{\mathbf{x}}^{T} \hat{\beta}\)</span>. On a
generic input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, it is obvious that the components of this
prediction vector would be real valued, rather than being one of the
one-hot basis vectors. To obtain a class prediction
<span class="math notranslate nohighlight">\(F(\mathbf{x}|\hat{\beta}) = 1, \dots, p\)</span>, we simply take the index of the
largest component of that vector, i.e.,</p>
<div class="math notranslate nohighlight">
\[F(\mathbf{x}|\hat{\beta}) = \textrm{argmax}_{k} f_{k}(\mathbf{x}|\hat{\beta}).\]</div>
<p>The <span class="math notranslate nohighlight">\(\textrm{argmax}\)</span> function is a non-linear function and is a first
example of what is referred to as <em>activation function</em>.</p>
<p>For numerical minimization, it is better to use a smooth activation
function. Such an activation function is given by the <em>softmax</em> function</p>
<div class="math notranslate nohighlight">
\[F_k(\mathbf{x}|\hat{\beta})= \frac{e^{-f_k(\mathbf{x}|\hat{\beta})}}{\sum_{k'=1}^pe^{-f_{k'}(\mathbf{x}|\hat{\beta})}}.\]</div>
<p>Importantly, the output of the softmax function is a probability
<span class="math notranslate nohighlight">\(P(y = k|\mathbf{x})\)</span>, since <span class="math notranslate nohighlight">\(\sum_k F_k(\mathbf{x}|\hat{\beta}) = 1\)</span>. This
extended linear model is referred to as <em>logistic regression</em> <a class="footnote-reference brackets" href="#id6" id="id3">6</a>.</p>
<p>The current linear approach based on classification of one-hot encoded
data generally works poorly when there are more than two classes. We
will see in the next chapter that relatively straightforward non-linear
extensions of this approach can lead to much better results.</p>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id1">4</a></span></dt>
<dd><p><a class="reference external" href="https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html">https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html</a></p>
</dd>
<dt class="label" id="id5"><span class="brackets"><a class="fn-backref" href="#id2">5</a></span></dt>
<dd><p>Note that the constraints for the minimization are not equalities,
but actually inequalities. A solution thus has to fulfil the
additional Karush-Kuhn-Tucker constraints</p>
<div class="math notranslate nohighlight" id="equation-eqn-firstkkt">
<span class="eqno">(33)<a class="headerlink" href="#equation-eqn-firstkkt" title="Permalink to this equation">¶</a></span>\[\begin{split} \begin{aligned}
    \alpha_i [y_i \tilde{\mathbf{x}}_i^T\boldsymbol \beta - (1-\xi_i)]&amp;=&amp;0,\\
    \mu_i\xi_i &amp;=&amp; 0,\\
    y_i \tilde{\mathbf{x}}_i^T\boldsymbol \beta - (1-\xi_i)&amp;&gt;&amp; 0.
  \end{aligned}\end{split}\]</div>
</dd>
<dt class="label" id="id6"><span class="brackets"><a class="fn-backref" href="#id3">6</a></span></dt>
<dd><p>Note that the softmax function for two classes is the logistic
function.</p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="ml_without_neural_network-1.html" title="previous page">Structuring Data without Neural Networks</a>
    <a class='right-next' id="next-link" href="ml_unsupervised.html" title="next page">Unsupervised Learning</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Eliska<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>