
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Kernel PCA &#8212; Machine Learning for Scientists</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="t-SNE as a Nonlinear Visualization Technique" href="ml_without_neural_network-3.html" />
    <link rel="prev" title="Principle Component Analysis" href="ml_without_neural_network-1.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/cluster.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning for Scientists</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="ml_without_neural_network.html">
   Structuring Data without Neural Networks
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="ml_without_neural_network-1.html">
     Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_without_neural_network-3.html">
     t-SNE as a Nonlinear Visualization Technique
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_without_neural_network-4.html">
     Clustering Algorithms: the example of
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="pca.html">
     Exercise: Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Dimensionality_reduction.html">
     Exercise: Dimensionality Reduction
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs.html">
   Supervised Learning without Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-1.html">
     Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-2.html">
     Binary Classification and Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-3.html">
     More than two classes: Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Linear-regression.html">
     Exercise: Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Classification.html">
     Exercise: Classification without Neural Networks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_w_NNs/ml_supervised_w_NNs.html">
   Supervised Learning with Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/ml_intro_neural.html">
     Computational neurons
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/ml_training_regularization.html">
     Training
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/ml_convolutional.html">
     Advanced Layers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/ml_rnn.html">
     Recurrent neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/Classification-2.html">
     Exercise: Dense Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html">
     Exercise: Machine Learning Optimizers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html#exercise-learning-rate-scheduling">
     Exercise: Learning Rate Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html#exercise-regularizing-neural-networks">
     Exercise: Regularizing Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/CNNs.html">
     Exercise: Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/exoplanets_RNN_CNN.html">
     Exercise: Discovery of Exoplanets with RNNs and CNNs
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../unsupervised_learning/ml_unsupervised.html">
   Unsupervised Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-1.html">
     Restricted Boltzmann Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-2.html">
     Training an RNN without Supervision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-3.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-4.html">
     Generative Adversarial Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Denoising.html">
     Exercise: Denoising with Restricted Boltzmann Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Molecule_gen_RNN.html">
     Exercise: Molecule Generation with an RNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Anomaly_Detection_RNN_AE_VAE.html">
     Exercise: Anomaly Detection
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../interpretability/ml_interpretability.html">
   Interpretability of Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-1.html">
     Dreaming and the Problem of Extrapolation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-2.html">
     Adversarial Attacks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-3.html">
     Interpreting Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/Transfer-learning-attacks.html">
     Exercise: Transfer Learning and Adversarial Attacks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning.html">
   Reinforcement Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-1.html">
     Exploration versus Exploitation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-2.html">
     Finite Markov Decision Process
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-3.html">
     Policies and Value Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-4.html">
     Temporal-difference Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-5.html">
     Function Approximation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../conclusion/ml_conclusion.html">
   Concluding Remarks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  About us
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../about_us.html">
   Who we are
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/docs/structuring_data/ml_without_neural_network-2.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZLMLLKHZE0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZLMLLKHZE0');
</script>
<div class="section" id="kernel-pca">
<h1>Kernel PCA<a class="headerlink" href="#kernel-pca" title="Permalink to this headline">¶</a></h1>
<p>PCA performs a linear transformation on the data. However, there are
cases where such a transformation is unable to produce any meaningful
result. Consider for instance the fictitious dataset with <span class="math notranslate nohighlight">\(2\)</span> classes
and <span class="math notranslate nohighlight">\(2\)</span> data features as shown on the left of <a class="reference internal" href="#fig-kernel-pca"><span class="std std-numref">Fig. 5</span></a>. We see by naked eye that it should be
possible to separate this data well, for instance by the distance of the
datapoint from the origin, but it is also clear that a linear function
cannot be used to compute it. In this case, it can be helpful to
consider a non-linear extension of PCA, known as <em>kernel PCA</em>.</p>
<p>The basic idea of this method is to apply to the data
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{n}\)</span> a chosen non-linear vector-valued
transformation function <span class="math notranslate nohighlight">\(\mathbf{\Phi}(\mathbf{x})\)</span> with</p>
<div class="math notranslate nohighlight" id="equation-eqn-kernel-pca-transformation">
<span class="eqno">(3)<a class="headerlink" href="#equation-eqn-kernel-pca-transformation" title="Permalink to this equation">¶</a></span>\[    \mathbf{\Phi}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{N},\]</div>
<p>which is a map from the original <span class="math notranslate nohighlight">\(n\)</span>-dimensional space (corresponding to the <span class="math notranslate nohighlight">\(n\)</span>
original data features) to a <span class="math notranslate nohighlight">\(N\)</span>-dimensional feature space. Kernel PCA
then simply involves performing the standard PCA on the transformed data
<span class="math notranslate nohighlight">\(\mathbf{\Phi}(\mathbf{x})\)</span>. Here, we will assume that the transformed data is
centered, i.e.,</p>
<div class="math notranslate nohighlight">
\[\sum_i \Phi(\mathbf{x}_i) = 0\]</div>
<p>to have simpler formulas.</p>
<div class="figure align-default" id="fig-kernel-pca">
<img alt="../../_images/circles_pca_kpca.png" src="../../_images/circles_pca_kpca.png" />
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text"><strong>Kernel PCA versus PCA.</strong></span><a class="headerlink" href="#fig-kernel-pca" title="Permalink to this image">¶</a></p>
</div>
<p>In practice, when <span class="math notranslate nohighlight">\(N\)</span> is large, it is not efficient or even possible to
explicitly perform the transformation <span class="math notranslate nohighlight">\(\mathbf{\Phi}\)</span>. Instead we can make
use of a method known as the kernel trick. Recall that in standard PCA,
the primary aim is to find the eigenvectors and eigenvalues of the
covariance matrix <span class="math notranslate nohighlight">\(C\)</span> . In the case of kernel PCA, this matrix becomes</p>
<div class="math notranslate nohighlight">
\[C = \sum_{i=1}^{m} \mathbf{\Phi}(\mathbf{x}_{i})\mathbf{\Phi}(\mathbf{x}_{i})^T,\]</div>
<p>with the eigenvalue equation</p>
<div class="math notranslate nohighlight" id="equation-eqn-pca-eigenvalue-equation">
<span class="eqno">(4)<a class="headerlink" href="#equation-eqn-pca-eigenvalue-equation" title="Permalink to this equation">¶</a></span>\[\sum_{i=1}^{m} \mathbf{\Phi}(\mathbf{x}_{i})\mathbf{\Phi}(\mathbf{x}_{i})^T \mathbf{v}_{j} = \lambda_{j}\mathbf{v}_{j}.\]</div>
<p>By writing the eigenvectors <span class="math notranslate nohighlight">\(\mathbf{v}_{j}\)</span> as a linear combination of the transformed data features</p>
<div class="math notranslate nohighlight">
\[\mathbf{v}_{j} = \sum_{i=1}^{m} a_{ji}\mathbf{\Phi}(\mathbf{x}_{i}),\]</div>
<p>we see that finding the eigenvectors is equivalent to finding the coefficients
<span class="math notranslate nohighlight">\(a_{ji}\)</span>. On substituting this form back into Eq. <a class="reference internal" href="#equation-eqn-pca-eigenvalue-equation">(4)</a>, we find</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^{m} \mathbf{\Phi}(\mathbf{x}_{i})\mathbf{\Phi}(\mathbf{x}_{i})^T \left[ \sum_{i=1}^{m} a_{ji}\mathbf{\Phi}(\mathbf{x}_{j}) \right] = \lambda_{j} \left[ \sum_{i=1}^{m} a_{ji}\mathbf{\Phi}(\mathbf{x}_{i}) \right].\]</div>
<p>By multiplying both sides of the equation by <span class="math notranslate nohighlight">\(\mathbf{\Phi}(\mathbf{x}_{k})^{T}\)</span>
we arrive at</p>
<div class="math notranslate nohighlight" id="equation-eqn-kernel-pca-eigen-equation">
<span class="eqno">(5)<a class="headerlink" href="#equation-eqn-kernel-pca-eigen-equation" title="Permalink to this equation">¶</a></span>\[\begin{split}    \begin{split}
        \sum_{i=1}^{m} \mathbf{\Phi}(\mathbf{x}_{k})^{T} \mathbf{\Phi}(\mathbf{x}_{i})\mathbf{\Phi}(\mathbf{x}_{i})^T \left[ \sum_{l=1}^{m} a_{jl}\mathbf{\Phi}(\mathbf{x}_{l}) \right] &amp;= \lambda_{j} \mathbf{\Phi}(\mathbf{x}_{k})^{T} \left[ \sum_{l=1}^{m} a_{jl} \mathbf{\Phi}(\mathbf{x}_{l}) \right] \\
        \sum_{i=1}^{m} \left[ \mathbf{\Phi}(\mathbf{x}_{k})^{T} \mathbf{\Phi}(\mathbf{x}_{i}) \right]   \sum_{l=1}^{m} a_{jl} \left[ \mathbf{\Phi}(\mathbf{x}_{i})^T \mathbf{\Phi}(\mathbf{x}_{l}) \right] &amp;= \lambda_{j} \sum_{l=1}^{m} a_{jl} \left[ \mathbf{\Phi}(\mathbf{x}_{k})^{T} \mathbf{\Phi}(\mathbf{x}_{l}) \right] \\
        \sum_{i=1}^{m} K(\mathbf{x}_{k},\mathbf{x}_{i})   \sum_{l=1}^{m} a_{jl} K(\mathbf{x}_{i},\mathbf{x}_{l}) &amp;= \lambda_{j} \sum_{l=1}^{m} a_{jl} K(\mathbf{x}_{k},\mathbf{x}_{l}), 
    \end{split}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(K(\mathbf{x},\mathbf{y}) = \mathbf{\Phi}(\mathbf{x})^{T} \mathbf{\Phi}(\mathbf{y})\)</span> is known as
the <em>kernel</em>. Thus we see that if we directly specify the kernels we can
avoid explicit performing the transformation <span class="math notranslate nohighlight">\(\mathbf{\Phi}\)</span>. In matrix
form, we find the eigenvalue equation
<span class="math notranslate nohighlight">\(K^{2}\mathbf{a}_{j} = \lambda_{j} K \mathbf{a}_{j}\)</span>, which simplifies to</p>
<div class="math notranslate nohighlight">
\[K \mathbf{a}_{j} = \lambda_{j} \mathbf{a}_{j}.\]</div>
<p>Note that this simplification requires <span class="math notranslate nohighlight">\(\lambda_j \neq 0\)</span>, which will be the case for relevant
principle components. (If <span class="math notranslate nohighlight">\(\lambda_{j} = 0\)</span>, then the corresponding
eigenvectors would be irrelevant components to be discarded.) After
solving the above equation and obtaining the coefficients <span class="math notranslate nohighlight">\(a_{jl}\)</span>, the
kernel PCA transformation is then simply given by the overlap with the
eigenvectors <span class="math notranslate nohighlight">\(\mathbf{v}_{j}\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[\mathbf{x} \rightarrow \mathbf{\Phi}(\mathbf{x}) \rightarrow y_{j} = \mathbf{v}_{j}^{T}\mathbf{\Phi}(\mathbf{x}) = \sum_{i=1}^{m} a_{ji} \mathbf{\Phi}(\mathbf{x}_{i})^{T} \mathbf{\Phi}(\mathbf{x}) = \sum_{i=1}^{m} a_{ji} K(\mathbf{x}_{i},\mathbf{x}),\]</div>
<p>where once again the explicit <span class="math notranslate nohighlight">\(\mathbf{\Phi}\)</span> transformation is avoided.</p>
<p>A common choice for the kernel is known as the radial basis function
kernel (RBF) defined by</p>
<div class="math notranslate nohighlight">
\[K_{\textrm{RBF}}(\mathbf{x},\mathbf{y}) = \exp\left( -\gamma \| \mathbf{x} - \mathbf{y}\|^{2} \right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is a tunable parameter. Using the RBF kernel, we compare
the result of kernel PCA with that of standard PCA, as shown on the
right of <a class="reference internal" href="#fig-kernel-pca"><span class="std std-numref">Fig. 5</span></a>. It is clear that kernel PCA leads to a
meaningful separation of the data while standard PCA completely fails.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/structuring_data"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="ml_without_neural_network-1.html" title="previous page">Principle Component Analysis</a>
    <a class='right-next' id="next-link" href="ml_without_neural_network-3.html" title="next page">t-SNE as a Nonlinear Visualization Technique</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Machine Learning for Science Team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>