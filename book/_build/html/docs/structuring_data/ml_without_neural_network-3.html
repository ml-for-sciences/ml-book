
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>t-SNE as a Nonlinear Visualization Technique &#8212; Machine Learning for Scientists</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Clustering Algorithms: the example of \(k\)-means" href="ml_without_neural_network-4.html" />
    <link rel="prev" title="Kernel PCA" href="ml_without_neural_network-2.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/cluster.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning for Scientists</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="ml_without_neural_network.html">
   Structuring Data without Neural Networks
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="ml_without_neural_network-1.html">
     Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_without_neural_network-2.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     t-SNE as a Nonlinear Visualization Technique
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_without_neural_network-4.html">
     Clustering Algorithms: the example of
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="pca.html">
     Exercise: Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Dimensionality_reduction.html">
     Exercise: Dimensionality Reduction
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs.html">
   Supervised Learning without Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-1.html">
     Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-2.html">
     Binary Classification and Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-3.html">
     More than two classes: Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Linear-regression.html">
     Exercise: Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Classification.html">
     Exercise: Classification without Neural Networks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_w_NNs/ml_supervised_w_NNs.html">
   Supervised Learning with Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/ml_intro_neural.html">
     Supervised Learning with Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/ml_training_regularization.html">
     Training
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/Classification-2.html">
     Exercise: Dense Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html">
     Exercise: Machine Learning Optimizers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html#exercise-learning-rate-scheduling">
     Exercise: Learning Rate Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html#exercise-regularizing-neural-networks">
     Exercise: Regularizing Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/CNNs.html">
     Exercise: Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/exoplanets_RNN_CNN.html">
     Exercise: Discovery of Exoplanets with RNNs and CNNs
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../unsupervised_learning/ml_unsupervised.html">
   Unsupervised Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-1.html">
     Restricted Boltzmann Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-2.html">
     Training an RNN without Supervision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-3.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-4.html">
     Generative Adversarial Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Denoising.html">
     Exercise: Denoising with Restricted Boltzmann Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Molecule_gen_RNN.html">
     Exercise: Molecule Generation with an RNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Anomaly_Detection_RNN_AE_VAE.html">
     Exercise: Anomaly Detection
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../interpretability/ml_interpretability.html">
   Interpretability of Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-1.html">
     Dreaming and the Problem of Extrapolation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-2.html">
     Adversarial Attacks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-3.html">
     Interpreting Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/Transfer-learning-attacks.html">
     Exercise: Transfer Learning and Adversarial Attacks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning.html">
   Reinforcement Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-1.html">
     Exploration versus Exploitation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-2.html">
     Finite Markov Decision Process
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-3.html">
     Policies and Value Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-4.html">
     Temporal-difference Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-5.html">
     Function Approximation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../conclusion/ml_conclusion.html">
   Concluding Remarks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  About us
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../about_us.html">
   Who we are
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/docs/structuring_data/ml_without_neural_network-3.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="t-sne-as-a-nonlinear-visualization-technique">
<h1>t-SNE as a Nonlinear Visualization Technique<a class="headerlink" href="#t-sne-as-a-nonlinear-visualization-technique" title="Permalink to this headline">¶</a></h1>
<p>We studied (kernel) PCA as an example for a method that reduces the
dimensionality of a dataset and makes features apparent by which data
points can be efficiently distinguished. Often, it is desirable to more
clearly cluster similar data points and visualize this clustering in a
low (two- or three-) dimensional space. We focus our attention on a
relatively recent algorithm (from 2008) that has proven very performant.
It goes by the name t-distributed stochastic neighborhood embedding
(t-SNE).</p>
<p>The basic idea is to think of the data (images, for instance) as objects
<span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> in a very high-dimensional space and characterize their
relation by the Euclidean distance <span class="math notranslate nohighlight">\(||\mathbf{x}_i-\mathbf{x}_j||\)</span> between them.
These pairwise distances are mapped to a probability distribution
<span class="math notranslate nohighlight">\(p_{ij}\)</span>. The same is done for the distances <span class="math notranslate nohighlight">\(||\mathbf{y}_i-\mathbf{y}_j||\)</span> of
the images of the data points <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> in the target low-dimensional
space. Their probability distribution is denoted <span class="math notranslate nohighlight">\(q_{ij}\)</span>. The mapping
is optimized by changing the locations <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> so as to minimize the
distance between the two probability distributions. Let us substantiate
these words with formulas.</p>
<p>The probability distribution in the space of data points is given as the
symmetrized version (joint probability distribution)</p>
<div class="math notranslate nohighlight">
\[p_{ij}=\frac{p_{i|j}+p_{j|i}}{2}\]</div>
<p>of the conditional probabilities</p>
<div class="math notranslate nohighlight">
\[p_{j|i}=\frac{\mathrm{exp}\left(-||\mathbf{x}_i-\mathbf{x}_j||^2/2\sigma_i^2\right)}
{\sum_{k\neq i}\mathrm{exp}\left(-||\mathbf{x}_i-\mathbf{x}_k||^2/2\sigma_i^2\right)},\]</div>
<p>where the choice of variances <span class="math notranslate nohighlight">\(\sigma_i\)</span> will be explained momentarily.
Distances are thus turned into a Gaussian distribution. Note that
<span class="math notranslate nohighlight">\(p_{j|i}\neq p_{i|j}\)</span> while <span class="math notranslate nohighlight">\(p_{ji}= p_{ij}\)</span>.</p>
<p>The probability distribution in the target space is chosen to be a
Student t-distribution</p>
<div class="math notranslate nohighlight">
\[q_{ij}=\frac{
(1+||\mathbf{y}_i-\mathbf{y}_j||^2)^{-1}
}{
\sum_{k\neq l}
(1+||\mathbf{y}_k-\mathbf{y}_l||^2)^{-1}
}.\]</div>
<p>This choice has several advantages: (i) it is symmetric upon
interchanging <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, (ii) it is numerically more efficiently
evaluated because there are no exponentials, (iii) it has ‘fatter’ tails
which helps to produce more meaningful maps in the lower dimensional
space.</p>
<div class="figure align-default" id="fig-pca-vs-tsne">
<img alt="../../_images/pca_tSNE.png" src="../../_images/pca_tSNE.png" />
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text"><strong>PCA vs. t-SNE</strong> Application of both methods on 5000 samples from the MNIST handwritten digit dataset. We see that perfect clustering cannot be achieved with either method, but t-SNE delivers the much better result.</span><a class="headerlink" href="#fig-pca-vs-tsne" title="Permalink to this image">¶</a></p>
</div>
<p>Let us now discuss the choice of <span class="math notranslate nohighlight">\(\sigma_i\)</span>. Intuitively, in dense
regions of the dataset, a smaller value of <span class="math notranslate nohighlight">\(\sigma_i\)</span> is usually more
appropriate than in sparser regions, in order to resolve the distances
better. Any particular value of <span class="math notranslate nohighlight">\(\sigma_i\)</span> induces a probability
distribution <span class="math notranslate nohighlight">\(P_i\)</span> over all the other data points. This distribution has
an <em>entropy</em> (here we use the Shannon entropy, in general it is a
measure for the “uncertainty” represented by the distribution)</p>
<div class="math notranslate nohighlight">
\[H(P_i)=-\sum_j p_{j|i}\, \mathrm{log}_2 \,p_{j|i}.\]</div>
<p>The value of <span class="math notranslate nohighlight">\(H(P_i)\)</span> increases as <span class="math notranslate nohighlight">\(\sigma_i\)</span> increases, i.e., the more uncertainty
is added to the distances. The algorithm searches for the <span class="math notranslate nohighlight">\(\sigma_i\)</span>
that result in a <span class="math notranslate nohighlight">\(P_i\)</span> with fixed perplexity</p>
<div class="math notranslate nohighlight">
\[\mathrm{Perp}(P_i)=2^{H(P_i)}.\]</div>
<p>The target value of the perplexity is chosen a priory and is the main parameter that controls the outcome of
the t-SNE algorithm. It can be interpreted as a smooth measure for the
effective number of neighbors. Typical values for the perplexity are
between 5 and 50.</p>
<p>Finally, we have to introduce a measure for the similarity between the
two probability distributions <span class="math notranslate nohighlight">\(p_{ij}\)</span> and <span class="math notranslate nohighlight">\(q_{ij}\)</span>. This defines a
so-called <em>loss function</em>. Here, we choose the <em>Kullback-Leibler</em>
divergence</p>
<div class="math notranslate nohighlight" id="equation-eqn-kl">
<span class="eqno">(6)<a class="headerlink" href="#equation-eqn-kl" title="Permalink to this equation">¶</a></span>\[L(\{\mathbf{y}_i\})=\sum_i\sum_jp_{ij}\mathrm{log}\frac{p_{ij}}{q_{ij}},\]</div>
<p>which we will frequently encounter during this lecture. The symmetrized <span class="math notranslate nohighlight">\(p_{ij}\)</span> ensures that <span class="math notranslate nohighlight">\(\sum_j p_{ij}&gt;1/(2n)\)</span>, so that
each data point makes a significant contribution to the cost function.
The minimization of <span class="math notranslate nohighlight">\(L(\{\mathbf{y}_i\})\)</span> with respect to the positions
<span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> can be achieved with a variety of methods. In the simplest
case it can be gradient descent, which we will discuss in more detail in
a later chapter. As the name suggests, it follows the direction of
largest gradient of the cost function to find the minimum. To this end
it is useful that these gradients can be calculated in a simple form</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial \mathbf{y}_i}
=4\sum_j (p_{ij}-q_{ij})(\mathbf{y}_i-\mathbf{y}_j)(1+||\mathbf{y}_i-\mathbf{y}_j||^2)^{-1}.\]</div>
<p>By now, t-SNE is implemented as standard in many packages. They involve
some extra tweaks that force points <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> to stay close together at
the initial steps of the optimization and create a lot of empty space.
This facilitates the moving of larger clusters in early stages of the
optimization until a globally good arrangement is found. If the dataset
is very high-dimensional it is advisable to perform an initial
dimensionality reduction (to somewhere between 10 and 100 dimensions,
for instance) with PCA before running t-SNE.</p>
<p>While t-SNE is a very powerful clustering technique, it has its
limitations. (i) The target dimension should be 2 or 3, for much larger
dimensions ansatz for <span class="math notranslate nohighlight">\(q_{ij}\)</span> is not suitable. (ii) If the dataset is
intrinsically high-dimensional (so that also the PCA pre-processing
fails), t-SNE may not be a suitable technique. (iii) Due to the
stochastic nature of the optimization, results are not reproducible. The
result may end up looking very different when the algorithm is
initialized with some slightly different initial values for <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/structuring_data"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="ml_without_neural_network-2.html" title="previous page">Kernel PCA</a>
    <a class='right-next' id="next-link" href="ml_without_neural_network-4.html" title="next page">Clustering Algorithms: the example of <span class="math notranslate nohighlight">\(k\)</span>-means</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By ml-for-science team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>