
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Exercise: Dimensionality Reduction &#8212; Machine Learning for Scientists</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Supervised Learning without Neural Networks" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs.html" />
    <link rel="prev" title="Exercise: Principle Component Analysis" href="pca.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/cluster.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning for Scientists</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="ml_without_neural_network.html">
   Structuring Data without Neural Networks
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="ml_without_neural_network-1.html">
     Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_without_neural_network-2.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_without_neural_network-3.html">
     t-SNE as a Nonlinear Visualization Technique
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_without_neural_network-4.html">
     Clustering Algorithms: the example of
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="pca.html">
     Exercise: Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Exercise: Dimensionality Reduction
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs.html">
   Supervised Learning without Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-1.html">
     Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-2.html">
     Binary Classification and Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-3.html">
     More than two classes: Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Linear-regression.html">
     Exercise: Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Classification.html">
     Exercise: Classification without Neural Networks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_w_NNs/ml_supervised_w_NNs.html">
   Supervised Learning with Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/ml_intro_neural.html">
     Supervised Learning with Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/ml_training_regularization.html">
     Training
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/Classification-2.html">
     Exercise: Dense Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html">
     Exercise: Machine Learning Optimizers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html#exercise-learning-rate-scheduling">
     Exercise: Learning Rate Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html#exercise-regularizing-neural-networks">
     Exercise: Regularizing Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/CNNs.html">
     Exercise: Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/exoplanets_RNN_CNN.html">
     Exercise: Discovery of Exoplanets with RNNs and CNNs
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../unsupervised_learning/ml_unsupervised.html">
   Unsupervised Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-1.html">
     Restricted Boltzmann Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-2.html">
     Training an RNN without Supervision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-3.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/ml_unsupervised-4.html">
     Generative Adversarial Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Denoising.html">
     Exercise: Denoising with Restricted Boltzmann Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Molecule_gen_RNN.html">
     Exercise: Molecule Generation with an RNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unsupervised_learning/Anomaly_Detection_RNN_AE_VAE.html">
     Exercise: Anomaly Detection
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../interpretability/ml_interpretability.html">
   Interpretability of Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-1.html">
     Dreaming and the Problem of Extrapolation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-2.html">
     Adversarial Attacks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-3.html">
     Interpreting Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/Transfer-learning-attacks.html">
     Exercise: Transfer Learning and Adversarial Attacks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning.html">
   Reinforcement Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-1.html">
     Exploration versus Exploitation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-2.html">
     Finite Markov Decision Process
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-3.html">
     Policies and Value Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-4.html">
     Temporal-difference Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-5.html">
     Function Approximation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../conclusion/ml_conclusion.html">
   Concluding Remarks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  About us
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../about_us.html">
   Who we are
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/docs/structuring_data/Dimensionality_reduction.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principle-component-analysis">
   Principle Component Analysis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-pca">
   Kernel PCA
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-discriminant-analysis">
   Linear Discriminant Analysis
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lda-algorithm">
     LDA algorithm
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#t-sne">
   t-SNE
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#t-sne-algorithm">
     t-SNE algorithm
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Libraries we need for this task</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>


<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">linalg</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">pdist</span><span class="p">,</span> <span class="n">squareform</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">svd</span>

<span class="kn">import</span> <span class="nn">sklearn</span> <span class="kn">as</span> <span class="nn">sk</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span><span class="p">,</span> <span class="n">load_breast_cancer</span><span class="p">,</span> <span class="n">load_digits</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span><span class="p">,</span> <span class="n">KernelPCA</span>
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
</pre></div>
</div>
<div class="section" id="exercise-dimensionality-reduction">
<h1>Exercise: Dimensionality Reduction<a class="headerlink" href="#exercise-dimensionality-reduction" title="Permalink to this headline">¶</a></h1>
<p>Very often, we are presented with data containing many features, i.e. high dimensional data. The aim of dimensionality reduction is, as the name implies, to reduce the number of dimensions or features of the dataset.
Some reasons why we might want to reduce dimensions include:</p>
<ul class="simple">
<li><p>eliminate ``useless” features,</p></li>
<li><p>speeds up other algorithms,</p></li>
<li><p>data visualisation.</p></li>
</ul>
<p>In the following, we will compare four different methods for achieving such a task: 1) Principle Component Analysis (PCA), 2) Kernel Principle Component Analysis (kPCA) and 3) Linear Discriminant Analysis (LDA) 4) t-SNE. For this exercise, we shall use the labelled Iris dataset provided within Scikit-Learn by calling <strong>sklearn.datasets.load_iris()</strong>. The dataset consist of 150 samples each with 4 features.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(150, 4)
</pre></div>
</div>
<div class="section" id="principle-component-analysis">
<h2>Principle Component Analysis<a class="headerlink" href="#principle-component-analysis" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">X_pca</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">X_pca</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span>
<span class="p">)</span>

</pre></div>
</div>
<p><img alt="png" src="../../_images/dr_output_4_1.png" /></p>
<p>Recall that the first principle component is given by the eigenvector <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> corresponding to the largest eigenvalue of the covariance matrix <span class="math notranslate nohighlight">\(C = X^{T}X\)</span>. The vector <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> defines a linear combination of features which transforms a datapoint as <span class="math notranslate nohighlight">\(\boldsymbol{x} \rightarrow \boldsymbol{x} \cdot \boldsymbol{w} = \boldsymbol{x}'\)</span>. What is the variance of the transformed data <span class="math notranslate nohighlight">\(\boldsymbol{x}'\)</span> in terms of the original covariance matrix?</p>
<p>If the data is transformed as</p>
<div class="math notranslate nohighlight">
\[    x \rightarrow x' = x \cdot w\]</div>
<p>then the variance of the transformed data <span class="math notranslate nohighlight">\(x'\)</span> is given by (assuming the data is centred)</p>
<div class="math notranslate nohighlight">
\[   \frac{1}{N} \sum_{i} {x'}_{i}^{2} = \frac{1}{N} \sum_{i} (x_i^{T} w)^{T} (x_i^{T} w) = 
   \frac{1}{N} w^{T} \left( \sum_{i} x_i x_i^{T} \right) w = w^{T} C w.\]</div>
<p>We have seen that the first principle component <span class="math notranslate nohighlight">\(w\)</span> corresponds to the eigenvector of the largest eigenvalue of the covariance matrix. Assuming that the data is centred, the covariance matrix is given by</p>
<div class="math notranslate nohighlight">
\[C = X^{T}X.\]</div>
<p>Notice that for any vector <span class="math notranslate nohighlight">\(v\)</span>,</p>
<div class="math notranslate nohighlight">
\[    v^{T} C v = (Xv)^{T} (Xv) \geq 0.\]</div>
<p>Since <span class="math notranslate nohighlight">\(C\)</span> is symmetric, i.e. <span class="math notranslate nohighlight">\(C = C^{T}\)</span>, there exist a complete orthonormal basis <span class="math notranslate nohighlight">\(\lbrace w_i \rbrace\)</span>  with corresponding eigenvalues <span class="math notranslate nohighlight">\(\lambda_1 &gt; \lambda_2 &gt; \lambda_3 &gt; \cdots\)</span>. The problem is then to show that</p>
<div class="math notranslate nohighlight">
\[    w_1  = \text{arg}\max\limits_{\boldsymbol{w}} \lbrace \frac{w^{T}Cw}{w^{T}w} \rbrace .\]</div>
<p>Given such an orthonormal basis, any vector <span class="math notranslate nohighlight">\(v\)</span> can be expressed in an orthonormal eigenbasis <span class="math notranslate nohighlight">\(\lbrace w_i \rbrace\)</span> of <span class="math notranslate nohighlight">\(C\)</span>, such that</p>
<div class="math notranslate nohighlight">
\[    v = \sum_{i} a_i w_i\]</div>
<p>where <span class="math notranslate nohighlight">\(a_i\)</span> are real coefficients.</p>
<p>This decomposition then allows us to write</p>
<div class="math notranslate nohighlight">
\[    R(v) = \frac{v^{T}Cv}{v^{T}v} = \sum_{i} \lambda_i \alpha_i^2.\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_i^2 = \frac{a_{i}^{2}}{\sum_{j} a_{j}^{2}}\)</span>.
Since <span class="math notranslate nohighlight">\(\alpha_i^2 &gt; 0\)</span> and <span class="math notranslate nohighlight">\(\sum_{i}\alpha_i^2 = 1\)</span>, the rayleigh quotient <span class="math notranslate nohighlight">\(R(v)\)</span> is maximised when</p>
<div class="math notranslate nohighlight">
\[\alpha_1 = 1, \alpha_2 = 0, \alpha_3 = 0, \cdots\]</div>
<p>This precisely corresponds to the case where</p>
<div class="math notranslate nohighlight">
\[    v \propto w_1\]</div>
<p>Thus, we can conclude that</p>
<div class="math notranslate nohighlight">
\[w_1  = \text{arg}\max\limits_{\boldsymbol{w}} \lbrace \frac{w^{T}Cw}{w^{T}w} \rbrace .\]</div>
<p>This result highlights the fact that PCA is essentially finding the linear combination of features which maximises the variance.</p>
</div>
<div class="section" id="kernel-pca">
<h2>Kernel PCA<a class="headerlink" href="#kernel-pca" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Polynomial Kernel</span>
<span class="n">kpca_poly</span> <span class="o">=</span> <span class="n">KernelPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">coef0</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">X_kpca_poly</span> <span class="o">=</span> <span class="n">kpca_poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># RBF Kernel</span>
<span class="n">kpca_rbf</span> <span class="o">=</span> <span class="n">KernelPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">)</span>
<span class="n">X_kpca_rbf</span> <span class="o">=</span> <span class="n">kpca_rbf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Polynomial&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">X_kpca_poly</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">X_kpca_poly</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span>
<span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;RBF&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">X_kpca_rbf</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">X_kpca_rbf</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&amp;lt;matplotlib.collections.PathCollection at 0x7fc4e47d8e90&amp;gt;
</pre></div>
</div>
<p><img alt="png" src="../../_images/dr_output_7_1.png" /></p>
<p>To get a better understanding of the inner workings of kernel PCA, let us try to implement it on our own. We shall do so in two ways. First by explicitly mapping our data to higher dimension using a non-linear map <span class="math notranslate nohighlight">\(g: R^{d} \rightarrow R^{k}\)</span> and second by using the kernel trick.</p>
<p>Let us consider a second order polynomial map <span class="math notranslate nohighlight">\( \boldsymbol{x} \rightarrow (0.5, x_1,\dots, x_d, x^2_1, \dots x^2_d, x_1 x_2, x_1 x_3, \dots)\)</span> which effectively adds a square of the number of features.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Transform the data according to the polynomial map</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span><span class="n">X</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;ij, ik-&gt;ijk&quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;X2 shape =&quot;</span><span class="p">,</span> <span class="n">X2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Perform PCA on the transformed data</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X2_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Polynomial kernel PCA (sklearn)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">X_kpca_poly</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">X_kpca_poly</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;PCA on transformed data&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">X2_pca</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">X2_pca</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>X2 shape = (150, 21)
</pre></div>
</div>
<p><img alt="png" src="../../_images/dr_output_9_2.png" /></p>
<p>Comparing the two plots above we see that indeed the two results match up exactly. Now lets see how we can implement the kernel trick.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Construct the kernel</span>
<span class="k">def</span> <span class="nf">make_polykernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">coef0</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">*=</span> <span class="n">gamma</span>
    <span class="n">K</span> <span class="o">+=</span> <span class="n">coef0</span>
    <span class="n">K</span> <span class="o">**=</span> <span class="n">degree</span>
    <span class="k">return</span> <span class="n">K</span>

<span class="c1"># Kernel PCA    </span>
<span class="k">def</span> <span class="nf">polykernel_pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">coef0</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">make_polykernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span>
                        <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">coef0</span><span class="o">=</span><span class="n">coef0</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">one_n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span> <span class="o">/</span> <span class="n">N</span>

    <span class="c1"># This step helps to standardise the data to have zero mean (c.f. PCA)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">K</span> <span class="o">-</span> <span class="n">one_n</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> <span class="o">-</span> <span class="n">K</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">one_n</span><span class="p">)</span> <span class="o">+</span> <span class="n">one_n</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">one_n</span><span class="p">)</span>

    <span class="c1"># Get the eigenvalues/eigenvectors</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">eigen_values</span><span class="p">,</span> <span class="n">eigen_vectors</span> <span class="o">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">eigen_vectors</span><span class="p">[:</span><span class="n">n</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">eigen_values</span><span class="p">[:</span><span class="n">n</span><span class="p">])))</span>

<span class="n">X2_transformed</span> <span class="o">=</span> <span class="n">polykernel_pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">coef0</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Polynomial kernel PCA (sklearn)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">X_kpca_poly</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">X_kpca_poly</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Polynomial kernel PCA  (our own version)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">X2_transformed</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">X2_transformed</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/dr_output_11_1.png" /></p>
</div>
<div class="section" id="linear-discriminant-analysis">
<h2>Linear Discriminant Analysis<a class="headerlink" href="#linear-discriminant-analysis" title="Permalink to this headline">¶</a></h2>
<p>Unlike PCA or kernel PCA, LDA uses labelled data, i.e it is a supervised algorithm. Let’s first use the sklearn’s built-in LDA module. Later we shall try to do it “ourselves”, using just numpy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lda</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;eigen&#39;</span><span class="p">)</span>
<span class="n">X_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;LD1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;LD2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;LDA using sklearn&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">X_lda</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">X_lda</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/dr_output_13_1.png" /></p>
<p>We have already seen in detail how PCA works. To get a better understanding of what actually happens when we do LDA, let us try to implement it ourselves. First, let us write down the algorithm.</p>
<div class="section" id="lda-algorithm">
<h3>LDA algorithm<a class="headerlink" href="#lda-algorithm" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>For each class k, compute the mean <span class="math notranslate nohighlight">\(\mu_k\)</span> and the within-class scatter matrix given by</p></li>
</ol>
<div class="math notranslate nohighlight">
\[W = \sum_{k} S_k \]</div>
<p>where <span class="math notranslate nohighlight">\(S_k = \sum_{i} (x_i -\mu_k)(x_i -\mu_k)^{T}\)</span>.</p>
<ol class="simple">
<li><p>Considering the class means <span class="math notranslate nohighlight">\(\mu_k\)</span> as a set of data points, compute the between-class scatter</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ B = \sum_{k} N_{k} (\mu_{k} - \mu)(\mu_{k} - \mu)^{T}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the global average and <span class="math notranslate nohighlight">\(N_k\)</span> is the size of each class.</p>
<ol class="simple">
<li><p>Compute the eigenvalues and eigenvectors of <span class="math notranslate nohighlight">\(W^{-1}B\)</span></p></li>
<li><p>Select the eigenvectors corresponding to the <span class="math notranslate nohighlight">\(l\)</span> largest eigenvalues and construct the transformation matrix</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ U = [v_1 \dots v_l] \]</div>
<p>and the data can then be transformed as <span class="math notranslate nohighlight">\(X' = XU\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Divide the dataset into its individual classes</span>
<span class="n">X_part</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">Y</span><span class="p">):</span>
    <span class="n">X_part</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">Y</span><span class="o">==</span><span class="n">e</span><span class="p">])</span>

<span class="c1"># Count the number of samples in each class</span>
<span class="n">Nk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_part</span><span class="p">])</span>

<span class="c1"># Obtain the global mean value across the whole dataset</span>
<span class="n">g_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Obtain the mean value on each class of data</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_part</span><span class="p">])</span>

<span class="c1"># Compute the within-class covariance/scatter matrix</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_part</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Compute the between-class covariance/scatter matrix</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;k, ki, kj -&gt; ij&quot;</span><span class="p">,</span> <span class="n">Nk</span><span class="p">,</span> <span class="n">mu</span><span class="o">-</span><span class="n">g_mu</span><span class="p">,</span> <span class="n">mu</span><span class="o">-</span><span class="n">g_mu</span><span class="p">)</span>

<span class="c1"># Get the eigenvectors and eigenvalues of W^-1 B</span>
<span class="n">vals</span><span class="p">,</span> <span class="n">vecs</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">vals</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">vecs</span> <span class="o">=</span> <span class="n">vecs</span><span class="p">[:,</span><span class="n">idx</span><span class="p">]</span>

<span class="c1"># Transform the data according to the two largest eigenvalues</span>
<span class="n">X_myLDA</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vecs</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">])</span>

<span class="c1"># Plot and compare with sklearn</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;LDA (sklearn)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">X_lda</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">X_lda</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;LDA  (our own version)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">X_myLDA</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">X_myLDA</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/dr_output_15_1.png" /></p>
<p>Notice that the two plots matches perfectly. Depending on how you found the eigenvalues/eigenvectors, your result might differ up to some scaling of the axes.</p>
<p>Each of the three methods so far essentially produced a new feature by considering linear/non-linear transformation of the original features. By looking only at the first principle component of each of the three methods so far, which method gave the most useful feature if one was interested in a classification task?</p>
<p>LDA would give more useful information in this case since its aim is actually separate the data from different classes. On the other hand, PCA or kernel PCA is actually agnostic to the classification of each data point. The primary aim of PCA or kernel PCA is to maximise variance. However, it is not a priori true that features with maximal variance carries the most (or even any) significance in terms of data classification.</p>
<p>Let us try to understand the rationale behind the LDA. In PCA, we know that the aim was maximise the variance across the sample, i.e. the first principle component maximises sample variance. There is actually a similar intuitive idea underlying the LDA, attributed to Fisher.</p>
<p>Consider a linear combination of features defined by <span class="math notranslate nohighlight">\(x \cdot v\)</span>. The mean and scatter matrices will then be given by <span class="math notranslate nohighlight">\(\mu_k \cdot v\)</span> and <span class="math notranslate nohighlight">\(v^{T}S v\)</span>.</p>
<p>Fisher defined the quantity called the separation as</p>
<div class="math notranslate nohighlight">
\[ S(v) = \frac{B(v)}{W(v)}\]</div>
<p>where <span class="math notranslate nohighlight">\(v\)</span> is a feature vector, i.e. <span class="math notranslate nohighlight">\(x' = Xv\)</span>, and</p>
<div class="math notranslate nohighlight">
\[B(v) = \sum_{k} N_{k} v^{T}(\mu_{k} - \mu)(\mu_{k} - \mu)^{T}v = v^{T}Bv\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[W(v) = v^{T}Wv\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[ S(v) =  \frac{v^{T}Bv}{v^{T}Wv} \]</div>
<p>Notice that if <span class="math notranslate nohighlight">\(W^{-1}Bv = \lambda v\)</span>,</p>
<div class="math notranslate nohighlight">
\[ S(v) = \frac{v^{T}WW^{-1}Bv}{v^{T}Wv} = \frac{\lambda v^{T}Wv}{v^{T}Wv} = \lambda\]</div>
<p>Lastly, to see that the largest eigenvector of <span class="math notranslate nohighlight">\(W^{-1}B\)</span> is also the the maximiser of <span class="math notranslate nohighlight">\(S\)</span>, simply note that the problem can be restated as</p>
<div class="math notranslate nohighlight">
\[ \textrm{maximise } v^{T}Bv\]</div>
<p>subject to the constraint</p>
<div class="math notranslate nohighlight">
\[ v^{T}Wv = 1 \]</div>
<p>This can be solved using the Lagrangian multiplier</p>
<div class="math notranslate nohighlight">
\[ \mathcal{L} = v^{T}Bv - \lambda v^{T}Wv \]</div>
<p>Taking partial derivative w.r.t. <span class="math notranslate nohighlight">\(v\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span> then yields</p>
<div class="math notranslate nohighlight">
\[ Bv = \lambda Wv \]</div>
<p>which is exactly the result we were looking for.</p>
<p>The key assumption in LDA is that the covariances of the different classes are equal.</p>
</div>
</div>
<div class="section" id="t-sne">
<h2>t-SNE<a class="headerlink" href="#t-sne" title="Permalink to this headline">¶</a></h2>
<p>We have already seen 3 related algorithms for dimensionality reduction. One of the key reasons for performing such a reduction is simply so that we can plot them as a 1D/2D/3D image. This is called data visualisation. Another popular method for such a task is known as t-SNE. Unlike the earlier dimensionality reduction algorithms, t-SNE is primarily a data visualisation tool and it is not trivial to extend it to new data. Before explaining further, let us first look at what t-SNE does to our Iris dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">tsne_results</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;TS1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;TS2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">tsne_results</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">tsne_results</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[t-SNE] Computing 121 nearest neighbors...
[t-SNE] Indexed 150 samples in 0.000s...
[t-SNE] Computed neighbors for 150 samples in 0.005s...
[t-SNE] Computed conditional probabilities for sample 150 / 150
[t-SNE] Mean sigma: 0.667454
[t-SNE] KL divergence after 250 iterations with early exaggeration: 47.299500
[t-SNE] KL divergence after 300 iterations: 0.093748
</pre></div>
</div>
<p><img alt="png" src="../../_images/dr_output_20_2.png" /></p>
<div class="section" id="t-sne-algorithm">
<h3>t-SNE algorithm<a class="headerlink" href="#t-sne-algorithm" title="Permalink to this headline">¶</a></h3>
<p>Let us now briefly recall how t-SNE is implemented:</p>
<p>1)Randomly map <span class="math notranslate nohighlight">\(x_i \in R^d \rightarrow y_i \in R^{l}\)</span></p>
<p>2)Compute the probabilities <span class="math notranslate nohighlight">\(p_{ij}\)</span> and <span class="math notranslate nohighlight">\(q_{ij}\)</span> corresponding to <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> respectively.</p>
<p>3)Minimise the KL divergence between <span class="math notranslate nohighlight">\(p_{ij}\)</span> and <span class="math notranslate nohighlight">\(q_{ij}\)</span> by shifting the mapped variables <span class="math notranslate nohighlight">\(y_i\)</span>, using gradient descent.</p>
<p>By analysing this procedure, we can see that t-SNE directly transform the data points in the sample. It does not actually ‘learn’ a transformation which can then be applied to new data points.</p>
<p>Even though t-SNE might not be suitable for preprocessing data, it is still useful for visualising the data and also providing useful (albeit misleading at times) information regarding the clustering properties of the data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mnist</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">X_mnist</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">data</span> 
<span class="n">Y_mnist</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">target</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_mnist</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">mnist_tsne_results</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_mnist</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;TS1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;TS2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">mnist_tsne_results</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">mnist_tsne_results</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">c</span><span class="o">=</span><span class="n">Y_mnist</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span>
<span class="p">)</span>

</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(1797, 64)
[t-SNE] Computing 121 nearest neighbors...
[t-SNE] Indexed 1797 samples in 0.003s...
[t-SNE] Computed neighbors for 1797 samples in 0.284s...
[t-SNE] Computed conditional probabilities for sample 1000 / 1797
[t-SNE] Computed conditional probabilities for sample 1797 / 1797
[t-SNE] Mean sigma: 8.394135
[t-SNE] KL divergence after 250 iterations with early exaggeration: 62.007500
[t-SNE] KL divergence after 300 iterations: 0.964586
</pre></div>
</div>
<p><img alt="png" src="../../_images/dr_output_23_2.png" /></p>
<p>There does indeed seem to be of the order 10 distinct clusters of data. Although for this case we already knew the answer, in an unsupervised setting where the data is unlabelled, such information can be extremely useful</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/structuring_data"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="pca.html" title="previous page">Exercise: Principle Component Analysis</a>
    <a class='right-next' id="next-link" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs.html" title="next page">Supervised Learning without Neural Networks</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By ml-for-science team<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>