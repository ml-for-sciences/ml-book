
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Structuring Data without Neural Networks &#8212; Machine Learning for Sciences</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Supervised Learning without Neural Networks" href="ml_supervised_wo_NNs.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/cluster.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning for Sciences</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Structuring Data without Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_supervised_wo_NNs.html">
   Supervised Learning without Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_unsupervised.html">
   Unsupervised Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_interpretability.html">
   Interpretability of Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_reinforcement-learning.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_conclusion.html">
   Concluding Remarks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="pca.html">
   Principle Component Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Dimensionality_reduction.html">
   Dimensionality Reduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Linear-regression.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Classification.html">
   Classification without Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Classification.html#dense-neural-networks">
   Dense Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NN-opt-reg.html">
   Machine Learning Optimizers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NN-opt-reg.html#learning-rate-scheduling">
   Learning Rate Scheduling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NN-opt-reg.html#regularizing-neural-networks">
   Regularizing Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="CNNs.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exoplanets_RNN_CNN.html">
   Discovery of Exoplanets with RNNs and CNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Denoising.html">
   Denoising with Restricted Boltzmann Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Molecule_gen_RNN.html">
   Molecule Generation with an RNN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Anomaly_Detection_RNN_AE_VAE.html">
   Anomaly Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Transfer-learning-attacks.html">
   Transfer Learning and Adversarial Attacks
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/ml_without_neural_network-1.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principle-component-analysis">
   Principle component analysis
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pca-algorithm">
     PCA algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example">
     Example
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-pca">
   Kernel PCA
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#t-sne-as-a-nonlinear-visualization-technique">
   t-SNE as a nonlinear visualization technique
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clustering-algorithms-the-example-of-k-means">
   Clustering algorithms: the example of
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   -means
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="structuring-data-without-neural-networks">
<span id="sec-structuring-data"></span><h1>Structuring Data without Neural Networks<a class="headerlink" href="#structuring-data-without-neural-networks" title="Permalink to this headline">¶</a></h1>
<p>Deep learning with neural networks is very much at the forefront of the
recent renaissance in machine learning. However, machine learning is not
synonymous with neural networks. There is a wealth of machine learning
approaches without neural networks, and the boundary between them and
conventional statistical analysis is not always sharp.</p>
<p>It is a common misconception that neural network techniques would always
outperform these approaches. In fact, in some cases, a simple linear
method could achieve faster and better results. Even when we might
eventually want to use a deep network, simpler approaches may help to
understand the problem we are facing and the specificity of the data so
as to better formulate our machine learning strategy. In this chapter,
we shall explore machine learning approaches without the use of neural
networks. This will further allow us to introduce basic concepts and the
general form of a machine learning workflow.</p>
<div class="section" id="principle-component-analysis">
<h2>Principle component analysis<a class="headerlink" href="#principle-component-analysis" title="Permalink to this headline">¶</a></h2>
<p>At the heart of any machine learning task is data. In order to choose
the most appropriate machine learning strategy, it is essential that we
understand the data we are working with. However, very often, we are
presented with a dataset containing many types of information, called
<em>features</em> of the data. Such a dataset is also described as being
high-dimensional. Techniques that extract information from such a
dataset are broadly summarised as <em>high-dimensional inference</em>. For
instance, we could be interested in predicting the progress of diabetes
in patients given features such as age, sex, body mass index, or average
blood pressure. Extremely high-dimensional data can occur in biology,
where we might want to compare gene expression pattern in cells. Given a
multitude of features, it is neither easy to visualise the data nor pick
out the most relevant information. This is where <em>principle component
analysis</em> (PCA) can be helpful.</p>
<p>Very briefly, PCA is a systematic way to find out which feature or
combination of features varies the most across the data samples. We can
think of PCA as approximating the data with a high-dimensional
ellipsoid, where the principal axes of this ellipsoid correspond to the
principal components. A feature, which is almost constant across the
samples, in other words has a very short principal axis, might not be
very useful. PCA then has two main applications: (1) It helps to
visualise the data in a low dimensional space and (2) it can reduce the
dimensionality of the input data to an amount that a more complex
algorithm can handle.</p>
<div class="section" id="pca-algorithm">
<h3>PCA algorithm<a class="headerlink" href="#pca-algorithm" title="Permalink to this headline">¶</a></h3>
<p>Given a dataset of <span class="math notranslate nohighlight">\(m\)</span> samples with <span class="math notranslate nohighlight">\(n\)</span> data features, we can arrange
our data in the form of a <span class="math notranslate nohighlight">\(m\)</span> by <span class="math notranslate nohighlight">\(n\)</span> matrix <span class="math notranslate nohighlight">\(X\)</span> where the element
<span class="math notranslate nohighlight">\(x_{ij}\)</span> corresponds to the value of the <span class="math notranslate nohighlight">\(j\)</span>th data feature of the <span class="math notranslate nohighlight">\(i\)</span>th
sample. We will also use the <em>feature vector</em> <span class="math notranslate nohighlight">\({x}_i\)</span> for all the <span class="math notranslate nohighlight">\(n\)</span>
features of one sample <span class="math notranslate nohighlight">\(i=1,\ldots,m\)</span>. The vector <span class="math notranslate nohighlight">\({x}_i\)</span> can take
values in the <em>feature space</em>, for example <span class="math notranslate nohighlight">\({x}_i \in \mathbb{R}^n\)</span>.
Going back to our diabetes example, we might have <span class="math notranslate nohighlight">\(10\)</span> data features.
Furthermore if we are given information regarding <span class="math notranslate nohighlight">\(100\)</span> patients, our
data matrix <span class="math notranslate nohighlight">\(X\)</span> would have <span class="math notranslate nohighlight">\(100\)</span> rows and <span class="math notranslate nohighlight">\(10\)</span> columns.</p>
<p>The procedure to perform PCA can then be described as follows:</p>
<div class="admonition-principle-component-analysis admonition">
<p class="admonition-title">Principle Component Analysis</p>
<ol>
<li><p>Center the data by subtracting from each column the mean of that
column,</p>
<div class="math notranslate nohighlight">
\[{x}_i \mapsto {x}_{i} - \frac{1}{m} \sum_{i=1}^{m} {x}_{i}.
      %  x_{ij} \longrightarrow x_{ij} - \frac{1}{m} \sum_{i=1}^{m} x_{ij}.\]</div>
<p>This ensures that the mean of each data feature is zero.</p>
</li>
<li><p>Form the <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span> (unnormalised) covariance matrix</p>
<div class="math notranslate nohighlight" id="equation-eqn-pca-covariance-matrix">
<span class="eqno">(1)<a class="headerlink" href="#equation-eqn-pca-covariance-matrix" title="Permalink to this equation">¶</a></span>\[C = {X}^{T}{X} = \sum_{i=1}^{m} {x}_{i}{x}_{i}^{T}.\]</div>
</li>
<li><p>Diagonalize the matrix to the form
<span class="math notranslate nohighlight">\(C = {X}^{T}{X} = W\Lambda W^{T}\)</span>, where the columns of <span class="math notranslate nohighlight">\(W\)</span> are the
normalised eigenvectors, or principal components, and <span class="math notranslate nohighlight">\(\Lambda\)</span> is a
diagonal matrix containing the eigenvalues. It will be helpful to
arrange the eigenvalues from largest to smallest.</p></li>
<li><p>Pick the <span class="math notranslate nohighlight">\(l\)</span> largest eigenvalues <span class="math notranslate nohighlight">\(\lambda_1, \dots \lambda_l\)</span>,
<span class="math notranslate nohighlight">\(l\leq n\)</span> and their corresponding eigenvectors
<span class="math notranslate nohighlight">\({v}_1 \dots {v}_l\)</span>. Construct the <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(l\)</span> matrix
<span class="math notranslate nohighlight">\(\widetilde{W} = [{v}_1 \dots {v}_l]\)</span>.</p></li>
<li><p>Dimensional reduction: Transform the data matrix as</p>
<div class="math notranslate nohighlight" id="equation-eqn-pca-dimensional-reduction">
<span class="eqno">(2)<a class="headerlink" href="#equation-eqn-pca-dimensional-reduction" title="Permalink to this equation">¶</a></span>\[        \widetilde{X} = X\widetilde{W}.\]</div>
</li>
</ol>
<p>The transformed data
matrix <span class="math notranslate nohighlight">\(\widetilde{X}\)</span> now has dimensions <span class="math notranslate nohighlight">\(m\)</span> by <span class="math notranslate nohighlight">\(l\)</span>.</p>
</div>
<p>We have thus reduced the dimensionality of the data from <span class="math notranslate nohighlight">\(n\)</span> to <span class="math notranslate nohighlight">\(l\)</span>.
Notice that there are actually two things happening: First, of course,
we now only have <span class="math notranslate nohighlight">\(l\)</span> data features. But second, the <span class="math notranslate nohighlight">\(l\)</span> data features
are new features and not simply a selection of the original data.
Rather, they are a linear combination of them. Using our diabetes
example again, one of the “new” data features could be the sum of the
average blood pressure and the body mass index. These new features are
automatically extracted by the algorithm.</p>
<p>But why did we have to go through such an elaborate procedure to do this
instead of simply removing a couple of features? The reason is that we
want to maximize the <em>variance</em> in our data. We will give a precise
definition of the variance later in the chapter, but briefly the
variance just means the spread of the data. Using PCA, we have
essentially obtained <span class="math notranslate nohighlight">\(l\)</span> “new” features which maximise the spread of the
data when plotted as a function of this feature. We illustrate this with
an example.</p>
</div>
<div class="section" id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h3>
<p>Let us consider a very simple dataset with just <span class="math notranslate nohighlight">\(2\)</span> data features. We
have data, from the Iris dataset <a class="footnote-reference brackets" href="#id2" id="id1">2</a>, a well known dataset on 3
different species of flowers. We are given information about the petal
length and petal width. Since there are just <span class="math notranslate nohighlight">\(2\)</span> features, it is easy to
visualise the data. In <a class="reference internal" href="#fig-iris-pca"><span class="std std-numref">Fig. 3</span></a>, we show how the data is
transformed under the PCA algorithm.</p>
<div class="figure align-default" id="fig-iris-pca">
<img alt="../_images/Iris-PCA.png" src="../_images/Iris-PCA.png" />
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text"><strong>PCA on Iris Dataset.</strong></span><a class="headerlink" href="#fig-iris-pca" title="Permalink to this image">¶</a></p>
</div>
<p>Notice that there is no dimensional reduction here since <span class="math notranslate nohighlight">\(l = n\)</span>. In
this case, the PCA algorithm amounts simply to a rotation of the
original data. However, it still produces <span class="math notranslate nohighlight">\(2\)</span> new features which are
orthogonal linear combinations of the original features: petal length
and petal width, i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
        w_1 &amp;= 0.922 \times \textrm{Petal Length} + 0.388 \times \textrm{Petal Width}, \\
        w_2 &amp;= -0.388 \times \textrm{Petal Length} + 0.922 \times \textrm{Petal Width}.
\end{split}\end{split}\]</div>
<p>We see very clearly that the first new feature <span class="math notranslate nohighlight">\(w_1\)</span>
has a much larger variance than the second feature <span class="math notranslate nohighlight">\(w_2\)</span>. In fact, if we
are interested in distinguishing the three different species of flowers,
as in a classification task, its almost sufficient to use only the data
feature with the largest variance, <span class="math notranslate nohighlight">\(w_1\)</span>. This is the essence of (PCA)
dimensional reduction.</p>
<p>Finally, it is important to note that it is not always true that the
feature with the largest variance is the most relevant for the task and
it is possible to construct counter examples where the feature with the
least variance contains all the useful information. However, PCA is
often a good guiding principle and can yield interesting insights in the
data. Most importantly, it is also <em>interpretable</em>, i.e., not only does
it separate the data, but we also learn <em>which</em> linear combination of
features can achieve this. We will see that for many neural network
algorithms, in contrast, a lack of interpretability is a big issue.</p>
</div>
</div>
<div class="section" id="kernel-pca">
<h2>Kernel PCA<a class="headerlink" href="#kernel-pca" title="Permalink to this headline">¶</a></h2>
<p>PCA performs a linear transformation on the data. However, there are
cases where such a transformation is unable to produce any meaningful
result. Consider for instance the fictitious dataset with <span class="math notranslate nohighlight">\(2\)</span> classes
and <span class="math notranslate nohighlight">\(2\)</span> data features as shown on the left of <a class="reference internal" href="#fig-kernel-pca"><span class="std std-numref">Fig. 4</span></a>. We see by naked eye that it should be
possible to separate this data well, for instance by the distance of the
datapoint from the origin, but it is also clear that a linear function
cannot be used to compute it. In this case, it can be helpful to
consider a non-linear extension of PCA, known as <em>kernel PCA</em>.</p>
<p>The basic idea of this method is to apply to the data
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{n}\)</span> a chosen non-linear vector-valued
transformation function <span class="math notranslate nohighlight">\(\mathbf{\Phi}(\mathbf{x})\)</span> with</p>
<div class="math notranslate nohighlight" id="equation-eqn-kernel-pca-transformation">
<span class="eqno">(3)<a class="headerlink" href="#equation-eqn-kernel-pca-transformation" title="Permalink to this equation">¶</a></span>\[    \mathbf{\Phi}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{N},\]</div>
<p>which is a map from the original <span class="math notranslate nohighlight">\(n\)</span>-dimensional space (corresponding to the <span class="math notranslate nohighlight">\(n\)</span>
original data features) to a <span class="math notranslate nohighlight">\(N\)</span>-dimensional feature space. Kernel PCA
then simply involves performing the standard PCA on the transformed data
<span class="math notranslate nohighlight">\(\mathbf{\Phi}(\mathbf{x})\)</span>. Here, we will assume that the transformed data is
centered, i.e.,</p>
<div class="math notranslate nohighlight">
\[\sum_i \Phi(\mathbf{x}_i) = 0\]</div>
<p>to have simpler formulas.</p>
<div class="figure align-default" id="fig-kernel-pca">
<img alt="../_images/circles_pca_kpca.png" src="../_images/circles_pca_kpca.png" />
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text"><strong>Kernel PCA versus PCA.</strong></span><a class="headerlink" href="#fig-kernel-pca" title="Permalink to this image">¶</a></p>
</div>
<p>In practice, when <span class="math notranslate nohighlight">\(N\)</span> is large, it is not efficient or even possible to
explicitly perform the transformation <span class="math notranslate nohighlight">\(\mathbf{\Phi}\)</span>. Instead we can make
use of a method known as the kernel trick. Recall that in standard PCA,
the primary aim is to find the eigenvectors and eigenvalues of the
covariance matrix <span class="math notranslate nohighlight">\(C\)</span> . In the case of kernel PCA, this matrix becomes</p>
<div class="math notranslate nohighlight">
\[C = \sum_{i=1}^{m} \mathbf{\Phi}(\mathbf{x}_{i})\mathbf{\Phi}(\mathbf{x}_{i})^T,\]</div>
<p>with the eigenvalue equation</p>
<div class="math notranslate nohighlight" id="equation-eqn-pca-eigenvalue-equation">
<span class="eqno">(4)<a class="headerlink" href="#equation-eqn-pca-eigenvalue-equation" title="Permalink to this equation">¶</a></span>\[\sum_{i=1}^{m} \mathbf{\Phi}(\mathbf{x}_{i})\mathbf{\Phi}(\mathbf{x}_{i})^T \mathbf{v}_{j} = \lambda_{j}\mathbf{v}_{j}.\]</div>
<p>By writing the eigenvectors <span class="math notranslate nohighlight">\(\mathbf{v}_{j}\)</span> as a linear combination of the transformed data features</p>
<div class="math notranslate nohighlight">
\[\mathbf{v}_{j} = \sum_{i=1}^{m} a_{ji}\mathbf{\Phi}(\mathbf{x}_{i}),\]</div>
<p>we see that finding the eigenvectors is equivalent to finding the coefficients
<span class="math notranslate nohighlight">\(a_{ji}\)</span>. On substituting this form back into Eq. <a class="reference internal" href="#equation-eqn-pca-eigenvalue-equation">(4)</a>, we find</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^{m} \mathbf{\Phi}(\mathbf{x}_{i})\mathbf{\Phi}(\mathbf{x}_{i})^T \left[ \sum_{i=1}^{m} a_{ji}\mathbf{\Phi}(\mathbf{x}_{j}) \right] = \lambda_{j} \left[ \sum_{i=1}^{m} a_{ji}\mathbf{\Phi}(\mathbf{x}_{i}) \right].\]</div>
<p>By multiplying both sides of the equation by <span class="math notranslate nohighlight">\(\mathbf{\Phi}(\mathbf{x}_{k})^{T}\)</span>
we arrive at</p>
<div class="math notranslate nohighlight" id="equation-eqn-kernel-pca-eigen-equation">
<span class="eqno">(5)<a class="headerlink" href="#equation-eqn-kernel-pca-eigen-equation" title="Permalink to this equation">¶</a></span>\[\begin{split}    \begin{split}
        \sum_{i=1}^{m} \mathbf{\Phi}(\mathbf{x}_{k})^{T} \mathbf{\Phi}(\mathbf{x}_{i})\mathbf{\Phi}(\mathbf{x}_{i})^T \left[ \sum_{l=1}^{m} a_{jl}\mathbf{\Phi}(\mathbf{x}_{l}) \right] &amp;= \lambda_{j} \mathbf{\Phi}(\mathbf{x}_{k})^{T} \left[ \sum_{l=1}^{m} a_{jl} \mathbf{\Phi}(\mathbf{x}_{l}) \right] \\
        \sum_{i=1}^{m} \left[ \mathbf{\Phi}(\mathbf{x}_{k})^{T} \mathbf{\Phi}(\mathbf{x}_{i}) \right]   \sum_{l=1}^{m} a_{jl} \left[ \mathbf{\Phi}(\mathbf{x}_{i})^T \mathbf{\Phi}(\mathbf{x}_{l}) \right] &amp;= \lambda_{j} \sum_{l=1}^{m} a_{jl} \left[ \mathbf{\Phi}(\mathbf{x}_{k})^{T} \mathbf{\Phi}(\mathbf{x}_{l}) \right] \\
        \sum_{i=1}^{m} K(\mathbf{x}_{k},\mathbf{x}_{i})   \sum_{l=1}^{m} a_{jl} K(\mathbf{x}_{i},\mathbf{x}_{l}) &amp;= \lambda_{j} \sum_{l=1}^{m} a_{jl} K(\mathbf{x}_{k},\mathbf{x}_{l}), 
    \end{split}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(K(\mathbf{x},\mathbf{y}) = \mathbf{\Phi}(\mathbf{x})^{T} \mathbf{\Phi}(\mathbf{y})\)</span> is known as
the <em>kernel</em>. Thus we see that if we directly specify the kernels we can
avoid explicit performing the transformation <span class="math notranslate nohighlight">\(\mathbf{\Phi}\)</span>. In matrix
form, we find the eigenvalue equation
<span class="math notranslate nohighlight">\(K^{2}\mathbf{a}_{j} = \lambda_{j} K \mathbf{a}_{j}\)</span>, which simplifies to</p>
<div class="math notranslate nohighlight">
\[K \mathbf{a}_{j} = \lambda_{j} \mathbf{a}_{j}.\]</div>
<p>Note that this simplification requires <span class="math notranslate nohighlight">\(\lambda_j \neq 0\)</span>, which will be the case for relevant
principle components. (If <span class="math notranslate nohighlight">\(\lambda_{j} = 0\)</span>, then the corresponding
eigenvectors would be irrelevant components to be discarded.) After
solving the above equation and obtaining the coefficients <span class="math notranslate nohighlight">\(a_{jl}\)</span>, the
kernel PCA transformation is then simply given by the overlap with the
eigenvectors <span class="math notranslate nohighlight">\(\mathbf{v}_{j}\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[\mathbf{x} \rightarrow \mathbf{\Phi}(\mathbf{x}) \rightarrow y_{j} = \mathbf{v}_{j}^{T}\mathbf{\Phi}(\mathbf{x}) = \sum_{i=1}^{m} a_{ji} \mathbf{\Phi}(\mathbf{x}_{i})^{T} \mathbf{\Phi}(\mathbf{x}) = \sum_{i=1}^{m} a_{ji} K(\mathbf{x}_{i},\mathbf{x}),\]</div>
<p>where once again the explicit <span class="math notranslate nohighlight">\(\mathbf{\Phi}\)</span> transformation is avoided.</p>
<p>A common choice for the kernel is known as the radial basis function
kernel (RBF) defined by</p>
<div class="math notranslate nohighlight">
\[K_{\textrm{RBF}}(\mathbf{x},\mathbf{y}) = \exp\left( -\gamma \| \mathbf{x} - \mathbf{y}\|^{2} \right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is a tunable parameter. Using the RBF kernel, we compare
the result of kernel PCA with that of standard PCA, as shown on the
right of <a class="reference internal" href="#fig-kernel-pca"><span class="std std-numref">Fig. 4</span></a>. It is clear that kernel PCA leads to a
meaningful separation of the data while standard PCA completely fails.</p>
</div>
<div class="section" id="t-sne-as-a-nonlinear-visualization-technique">
<h2>t-SNE as a nonlinear visualization technique<a class="headerlink" href="#t-sne-as-a-nonlinear-visualization-technique" title="Permalink to this headline">¶</a></h2>
<p>We studied (kernel) PCA as an example for a method that reduces the
dimensionality of a dataset and makes features apparent by which data
points can be efficiently distinguished. Often, it is desirable to more
clearly cluster similar data points and visualize this clustering in a
low (two- or three-) dimensional space. We focus our attention on a
relatively recent algorithm (from 2008) that has proven very performant.
It goes by the name t-distributed stochastic neighborhood embedding
(t-SNE).</p>
<p>The basic idea is to think of the data (images, for instance) as objects
<span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> in a very high-dimensional space and characterize their
relation by the Euclidean distance <span class="math notranslate nohighlight">\(||\mathbf{x}_i-\mathbf{x}_j||\)</span> between them.
These pairwise distances are mapped to a probability distribution
<span class="math notranslate nohighlight">\(p_{ij}\)</span>. The same is done for the distances <span class="math notranslate nohighlight">\(||\mathbf{y}_i-\mathbf{y}_j||\)</span> of
the images of the data points <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> in the target low-dimensional
space. Their probability distribution is denoted <span class="math notranslate nohighlight">\(q_{ij}\)</span>. The mapping
is optimized by changing the locations <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> so as to minimize the
distance between the two probability distributions. Let us substantiate
these words with formulas.</p>
<p>The probability distribution in the space of data points is given as the
symmetrized version (joint probability distribution)</p>
<div class="math notranslate nohighlight">
\[p_{ij}=\frac{p_{i|j}+p_{j|i}}{2}\]</div>
<p>of the conditional probabilities</p>
<div class="math notranslate nohighlight">
\[p_{j|i}=\frac{\mathrm{exp}\left(-||\mathbf{x}_i-\mathbf{x}_j||^2/2\sigma_i^2\right)}
{\sum_{k\neq i}\mathrm{exp}\left(-||\mathbf{x}_i-\mathbf{x}_k||^2/2\sigma_i^2\right)},\]</div>
<p>where the choice of variances <span class="math notranslate nohighlight">\(\sigma_i\)</span> will be explained momentarily.
Distances are thus turned into a Gaussian distribution. Note that
<span class="math notranslate nohighlight">\(p_{j|i}\neq p_{i|j}\)</span> while <span class="math notranslate nohighlight">\(p_{ji}= p_{ij}\)</span>.</p>
<p>The probability distribution in the target space is chosen to be a
Student t-distribution</p>
<div class="math notranslate nohighlight">
\[q_{ij}=\frac{
(1+||\mathbf{y}_i-\mathbf{y}_j||^2)^{-1}
}{
\sum_{k\neq l}
(1+||\mathbf{y}_k-\mathbf{y}_l||^2)^{-1}
}.\]</div>
<p>This choice has several advantages: (i) it is symmetric upon
interchanging <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, (ii) it is numerically more efficiently
evaluated because there are no exponentials, (iii) it has ‘fatter’ tails
which helps to produce more meaningful maps in the lower dimensional
space.</p>
<div class="figure align-default" id="fig-pca-vs-tsne">
<img alt="../_images/pca_tSNE.png" src="../_images/pca_tSNE.png" />
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text"><strong>PCA vs.\ t-SNE</strong> Application of both methods on 5000 samples from the MNIST handwritten digit dataset. We see that perfect clustering cannot be achieved with either method, but t-SNE delivers the much better result.</span><a class="headerlink" href="#fig-pca-vs-tsne" title="Permalink to this image">¶</a></p>
</div>
<p>Let us now discuss the choice of <span class="math notranslate nohighlight">\(\sigma_i\)</span>. Intuitively, in dense
regions of the dataset, a smaller value of <span class="math notranslate nohighlight">\(\sigma_i\)</span> is usually more
appropriate than in sparser regions, in order to resolve the distances
better. Any particular value of <span class="math notranslate nohighlight">\(\sigma_i\)</span> induces a probability
distribution <span class="math notranslate nohighlight">\(P_i\)</span> over all the other data points. This distribution has
an <em>entropy</em> (here we use the Shannon entropy, in general it is a
measure for the “uncertainty” represented by the distribution)</p>
<div class="math notranslate nohighlight">
\[H(P_i)=-\sum_j p_{j|i}\, \mathrm{log}_2 \,p_{j|i}.\]</div>
<p>The value of <span class="math notranslate nohighlight">\(H(P_i)\)</span> increases as <span class="math notranslate nohighlight">\(\sigma_i\)</span> increases, i.e., the more uncertainty
is added to the distances. The algorithm searches for the <span class="math notranslate nohighlight">\(\sigma_i\)</span>
that result in a <span class="math notranslate nohighlight">\(P_i\)</span> with fixed perplexity</p>
<div class="math notranslate nohighlight">
\[\mathrm{Perp}(P_i)=2^{H(P_i)}.\]</div>
<p>The target value of the perplexity is chosen a priory and is the main parameter that controls the outcome of
the t-SNE algorithm. It can be interpreted as a smooth measure for the
effective number of neighbors. Typical values for the perplexity are
between 5 and 50.</p>
<p>Finally, we have to introduce a measure for the similarity between the
two probability distributions <span class="math notranslate nohighlight">\(p_{ij}\)</span> and <span class="math notranslate nohighlight">\(q_{ij}\)</span>. This defines a
so-called <em>loss function</em>. Here, we choose the <em>Kullback-Leibler</em>
divergence</p>
<div class="math notranslate nohighlight" id="equation-eqn-kl">
<span class="eqno">(6)<a class="headerlink" href="#equation-eqn-kl" title="Permalink to this equation">¶</a></span>\[L(\{\mathbf{y}_i\})=\sum_i\sum_jp_{ij}\mathrm{log}\frac{p_{ij}}{q_{ij}},\]</div>
<p>which we will frequently encounter during this lecture. The symmetrized <span class="math notranslate nohighlight">\(p_{ij}\)</span> ensures that <span class="math notranslate nohighlight">\(\sum_j p_{ij}&gt;1/(2n)\)</span>, so that
each data point makes a significant contribution to the cost function.
The minimization of <span class="math notranslate nohighlight">\(L(\{\mathbf{y}_i\})\)</span> with respect to the positions
<span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> can be achieved with a variety of methods. In the simplest
case it can be gradient descent, which we will discuss in more detail in
a later chapter. As the name suggests, it follows the direction of
largest gradient of the cost function to find the minimum. To this end
it is useful that these gradients can be calculated in a simple form</p>
<div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial \mathbf{y}_i}
=4\sum_j (p_{ij}-q_{ij})(\mathbf{y}_i-\mathbf{y}_j)(1+||\mathbf{y}_i-\mathbf{y}_j||^2)^{-1}.\]</div>
<p>By now, t-SNE is implemented as standard in many packages. They involve
some extra tweaks that force points <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> to stay close together at
the initial steps of the optimization and create a lot of empty space.
This facilitates the moving of larger clusters in early stages of the
optimization until a globally good arrangement is found. If the dataset
is very high-dimensional it is advisable to perform an initial
dimensionality reduction (to somewhere between 10 and 100 dimensions,
for instance) with PCA before running t-SNE.</p>
<p>While t-SNE is a very powerful clustering technique, it has its
limitations. (i) The target dimension should be 2 or 3, for much larger
dimensions ansatz for <span class="math notranslate nohighlight">\(q_{ij}\)</span> is not suitable. (ii) If the dataset is
intrinsically high-dimensional (so that also the PCA pre-processing
fails), t-SNE may not be a suitable technique. (iii) Due to the
stochastic nature of the optimization, results are not reproducible. The
result may end up looking very different when the algorithm is
initialized with some slightly different initial values for <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>.</p>
</div>
<div class="section" id="clustering-algorithms-the-example-of-k-means">
<h2>Clustering algorithms: the example of <span class="math notranslate nohighlight">\(k\)</span>-means<a class="headerlink" href="#clustering-algorithms-the-example-of-k-means" title="Permalink to this headline">¶</a></h2>
<p>All of PCA, kernel-PCA and t-SNE may or may not deliver a visualization
of the dataset, where clusters emerge. They all leave it to the observer
to identify these possible clusters. In this section, we want to
introduce an algorithm that actually clusters data, i.e., it will sort
any data point into one of <span class="math notranslate nohighlight">\(k\)</span> clusters. Here the desired number of
clusters <span class="math notranslate nohighlight">\(k\)</span> is fixed a priori by us. This is a weakness but may be
compensated by running the algorithm with different values of <span class="math notranslate nohighlight">\(k\)</span> and
asses where the performance is best.</p>
<p>We will exemplify a simple clustering algorithm that goes by the name
<span class="math notranslate nohighlight">\(k\)</span>-means. The algorithm is iterative. The key idea is that data points
are assigned to clusters such that the squared distances between the
data points belonging to one cluster and the centroid of the cluster is
minimized. The centroid is defined as the arithmetic mean of all data
points in a cluster.</p>
<p>This description already suggests, that we will again minimize a loss
function (or maximize an expectation function, which just differs in the
overall sign from the loss function). Suppose we are given an assignment
of datapoints <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> to clusters <span class="math notranslate nohighlight">\(j=1,\cdots, k\)</span> that is represented
by</p>
<div class="math notranslate nohighlight">
\[\begin{split}w_{ij}=\begin{cases}
1,\qquad \mathbf{x}_i\text{ in cluster }j,\\
0,\qquad \mathbf{x}_i\text{ not in cluster }j.
\end{cases}\end{split}\]</div>
<p>Then the loss function is given by</p>
<div class="math notranslate nohighlight">
\[L(\{\mathbf{x}_i\},\{w_{ij}\})=\sum_{i=1}^m\sum_{j=1}^k w_{ij}||\mathbf{x}_i-\mathbf{\mu}_j ||^2,\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-eqn-centroids">
<span class="eqno">(7)<a class="headerlink" href="#equation-eqn-centroids" title="Permalink to this equation">¶</a></span>\[\mathbf{\mu}_j=\frac{\sum_iw_{ij}\mathbf{x}_i}{\sum_iw_{ij}}.\]</div>
<p>Naturally, we want to minimize the loss function with respect to the
assignment <span class="math notranslate nohighlight">\(w_{ij}\)</span>. However, a change in this assignment also changes
<span class="math notranslate nohighlight">\(\mathbf{\mu}_j\)</span>. For this reason, it is natural to divide each update step
in two parts. The first part updates the <span class="math notranslate nohighlight">\(w_{ij}\)</span> according to</p>
<div class="math notranslate nohighlight">
\[\begin{split}w_{ij}=\begin{cases}
1,\qquad \text{if } j=\mathrm{argmin}_l ||\mathbf{x}_i-\mathbf{\mu}_l||,\\
0,\qquad \text{else }.
\end{cases}\end{split}\]</div>
<p>That means we attach each data point to the nearest cluster centroid. The second part is a recalculation of the centroids
according to Eq. <a class="reference internal" href="#equation-eqn-centroids">(7)</a>.</p>
<p>The algorithm is initialized by choosing at random <span class="math notranslate nohighlight">\(k\)</span> distinct data
points as initial positions of the centroids. Then one repeats the above
two-part steps until convergence, i.e., until the <span class="math notranslate nohighlight">\(w_{ij}\)</span> do not change
anymore.</p>
<p>In this algorithm we use the Euclidean distance measure <span class="math notranslate nohighlight">\(||\cdot ||\)</span>. It
is advisable to standardize the data such that each feature has mean
zero and standard deviation of one when average over all data points.
Otherwise (if some features are overall numerically smaller than
others), the differences in various features may be weighted very
differently by the algorithm.</p>
<p>Furthermore, the results depend on the initialization. One should re-run
the algorithm with a few different initializations to avoid running into
bad local minima.</p>
<p>Applications of <span class="math notranslate nohighlight">\(k\)</span>-means are manifold: in economy they include marked
segmentation, in science any classification problem such as that of
phases of matter, document clustering, image compression (color
reduction), etc.. In general it helps to build intuition about the data
at hand.</p>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p><a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/iris">https://archive.ics.uci.edu/ml/datasets/iris</a></p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="introduction.html" title="previous page">Introduction</a>
    <a class='right-next' id="next-link" href="ml_supervised_wo_NNs.html" title="next page">Supervised Learning without Neural Networks</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Eliska<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>