
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Exercise: Anomaly Detection &#8212; Machine Learning for Scientists</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Interpretability of Neural Networks" href="../interpretability/ml_interpretability.html" />
    <link rel="prev" title="Exercise: Molecule Generation with an RNN" href="Molecule_gen_RNN.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/cluster.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning for Scientists</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../structuring_data/ml_without_neural_network.html">
   Structuring Data without Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-1.html">
     Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-2.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-3.html">
     t-SNE as a Nonlinear Visualization Technique
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-4.html">
     Clustering Algorithms: the example of
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/pca.html">
     Exercise: Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/Dimensionality_reduction.html">
     Exercise: Dimensionality Reduction
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs.html">
   Supervised Learning without Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-1.html">
     Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-2.html">
     Binary Classification and Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-3.html">
     More than two classes: Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Linear-regression.html">
     Exercise: Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Classification.html">
     Exercise: Classification without Neural Networks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_w_NNs/ml_supervised_w_NNs.html">
   Supervised Learning with Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/Classification-2.html">
     Exercise: Dense Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html">
     Exercise: Machine Learning Optimizers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html#exercise-learning-rate-scheduling">
     Exercise: Learning Rate Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html#exercise-regularizing-neural-networks">
     Exercise: Regularizing Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/CNNs.html">
     Exercise: Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/exoplanets_RNN_CNN.html">
     Exercise: Discovery of Exoplanets with RNNs and CNNs
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="ml_unsupervised.html">
   Unsupervised Learning
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="ml_unsupervised-1.html">
     Restricted Boltzmann Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_unsupervised-2.html">
     Training an RNN without Supervision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_unsupervised-3.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_unsupervised-4.html">
     Generative Adversarial Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Denoising.html">
     Exercise: Denoising with Restricted Boltzmann Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Molecule_gen_RNN.html">
     Exercise: Molecule Generation with an RNN
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Exercise: Anomaly Detection
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../interpretability/ml_interpretability.html">
   Interpretability of Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-1.html">
     Dreaming and the Problem of Extrapolation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-2.html">
     Adversarial Attacks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-3.html">
     Interpreting Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/Transfer-learning-attacks.html">
     Exercise: Transfer Learning and Adversarial Attacks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning.html">
   Reinforcement Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-1.html">
     Exploration versus Exploitation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-2.html">
     Finite Markov Decision Process
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-3.html">
     Policies and Value Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-4.html">
     Temporal-difference Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-5.html">
     Function Approximation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../conclusion/ml_conclusion.html">
   Concluding Remarks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  About us
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../about_us.html">
   Who we are
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/docs/unsupervised_learning/Anomaly_Detection_RNN_AE_VAE.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-and-prepare-ecg-data">
   Load and prepare ECG data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rnn-for-anomaly-detection">
   RNN for anomaly detection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#autoencoder-for-anomaly-detection">
   Autoencoder for anomaly detection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-the-model">
     Build the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#detect-anomalies">
     Detect anomalies
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variational-autoencoder-for-anomaly-detection">
   Variational Autoencoder for anomaly detection
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
</pre></div>
</div>
<div class="section" id="exercise-anomaly-detection">
<h1>Exercise: Anomaly Detection<a class="headerlink" href="#exercise-anomaly-detection" title="Permalink to this headline">¶</a></h1>
<p>This exercise is based on the <a class="reference external" href="https://www.tensorflow.org/tutorials/generative/autoencoder">tensorflow tutorial</a> about autoencoders. In this exercise, we will detect anomalies on the <a class="reference external" href="http://www.timeseriesclassification.com/description.php?Dataset=ECG5000">ECG5000 dataset</a> using an RNN, an autoencoder and a variational autoencoder. This dataset contains 5,000 <a class="reference external" href="https://en.wikipedia.org/wiki/Electrocardiography">Electrocardiograms</a>, each with 140 data points. We will use a simplified version of the dataset, where each example has been labeled either <code class="docutils literal notranslate"><span class="pre">0</span></code> (corresponding to an abnormal rhythm), or <code class="docutils literal notranslate"><span class="pre">1</span></code> (corresponding to a normal rhythm). We are interested in identifying the abnormal rhythms. For more information on anomaly detection, check out this <a class="reference external" href="https://anomagram.fastforwardlabs.com/#/">interactive example</a>.</p>
<div class="section" id="load-and-prepare-ecg-data">
<h2>Load and prepare ECG data<a class="headerlink" href="#load-and-prepare-ecg-data" title="Permalink to this headline">¶</a></h2>
<p>The dataset we will use is based on one from <a class="reference external" href="http://www.timeseriesclassification.com/description.php?Dataset=ECG5000">timeseriesclassification.com</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the dataset</span>
<span class="n">raw_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s1">&#39;data.csv&#39;</span><span class="p">)</span>
<span class="n">raw_data</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>array([-0.11252183, -2.8272038 , -3.7738969 , -4.3497511 , -4.376041  ,
       -3.4749863 , -2.1814082 , -1.8182865 , -1.2505219 , -0.47749208,
       -0.36380791, -0.49195659, -0.42185509, -0.30920086, -0.4959387 ,
       -0.34211867, -0.35533627, -0.36791303, -0.31650279, -0.41237405,
       -0.47167181, -0.41345783, -0.36461703, -0.44929829, -0.47141866,
       -0.42477658, -0.46251673, -0.55247236, -0.47537519, -0.6942    ,
       -0.7018681 , -0.59381178, -0.66068415, -0.71383066, -0.76980688,
       -0.67228161, -0.65367605, -0.63940562, -0.55930228, -0.59167032,
       -0.49322332, -0.46305183, -0.30164382, -0.23273401, -0.12505488,
       -0.15394314, -0.0243574 , -0.06560876,  0.03499926,  0.06193522,
        0.07119542,  0.12392505,  0.10312371,  0.22522849,  0.12868305,
        0.30248315,  0.25727621,  0.19635161,  0.17938297,  0.24472863,
        0.34121687,  0.32820441,  0.40604169,  0.44660507,  0.42406823,
        0.48151204,  0.4778438 ,  0.62408259,  0.57458456,  0.59801319,
        0.5645919 ,  0.607979  ,  0.62063457,  0.65625291,  0.68474806,
        0.69427284,  0.66558377,  0.57579577,  0.63813479,  0.61491695,
        0.56908343,  0.46857572,  0.44281777,  0.46827436,  0.43249295,
        0.40795792,  0.41862256,  0.36253075,  0.41095901,  0.47166633,
        0.37216676,  0.33787543,  0.22140511,  0.27399747,  0.29866408,
        0.26356357,  0.34256352,  0.41950529,  0.58660736,  0.86062387,
        1.1733446 ,  1.2581791 ,  1.4337887 ,  1.7005334 ,  1.9990431 ,
        2.1253411 ,  1.9932907 ,  1.9322463 ,  1.7974367 ,  1.5222839 ,
        1.2511679 ,  0.99873034,  0.48372242,  0.02313229, -0.19491383,
       -0.22091729, -0.24373668, -0.25469462, -0.29113555, -0.25649034,
       -0.22787425, -0.32242276, -0.28928586, -0.31816951, -0.36365359,
       -0.39345584, -0.26641886, -0.25682316, -0.28869399, -0.16233755,
        0.16034772,  0.79216787,  0.93354122,  0.79695779,  0.57862066,
        0.2577399 ,  0.22807718,  0.12343082,  0.92528624,  0.19313742,
        1.        ])
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The last element contains the labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">raw_data</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># The other data points are the electrocadriogram data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">raw_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">21</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Normalize the data to <code class="docutils literal notranslate"><span class="pre">[0,1]</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">min_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">max_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_data</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">max_val</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">test_data</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">max_val</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
<p>We separate the normal rhythms from the abnormal rhythms.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">test_labels</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>

<span class="n">normal_train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">train_labels</span><span class="p">]</span>
<span class="n">normal_test_data</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">[</span><span class="n">test_labels</span><span class="p">]</span>

<span class="n">anomalous_train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="o">~</span><span class="n">train_labels</span><span class="p">]</span>
<span class="n">anomalous_test_data</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">[</span><span class="o">~</span><span class="n">test_labels</span><span class="p">]</span>
</pre></div>
</div>
<p>Plot a normal ECG.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">140</span><span class="p">),</span> <span class="n">normal_train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;A Normal ECG&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/ad_output_11_0.png" /></p>
<p>Plot an anomalous ECG.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">140</span><span class="p">),</span> <span class="n">anomalous_train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;An Anomalous ECG&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/ad_output_13_0.png" /></p>
</div>
<div class="section" id="rnn-for-anomaly-detection">
<h2>RNN for anomaly detection<a class="headerlink" href="#rnn-for-anomaly-detection" title="Permalink to this headline">¶</a></h2>
<p>Since we have access to the labels of the dataset, we can frame the anomaly detection as a supervised learning problem. Similar to the detection of exoplanets, where a time series of light intensities was labeled as having either an exoplanet as cause or not, we want to predict the label of the time series of ecg data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reshape the dataset as we saw in the exoplanet problem</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;CREATING THE LSTM MODEL&quot;&quot;&quot;</span>

<span class="c1"># Create the model </span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">train_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm (LSTM)                  (None, 100)               40800     
_________________________________________________________________
dropout (Dropout)            (None, 100)               0         
_________________________________________________________________
dense (Dense)                (None, 1)                 101       
=================================================================
Total params: 40,901
Trainable params: 40,901
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compile the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span>
<span class="c1"># Fit the model</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
16/16 [==============================] - 3s 219ms/step - loss: 0.6538
Epoch 2/20
16/16 [==============================] - 3s 209ms/step - loss: 0.5434
Epoch 3/20
16/16 [==============================] - 3s 218ms/step - loss: 0.5166
Epoch 4/20
16/16 [==============================] - 4s 227ms/step - loss: 0.5212
Epoch 5/20
16/16 [==============================] - 4s 221ms/step - loss: 0.4497
Epoch 6/20
16/16 [==============================] - 3s 203ms/step - loss: 0.2577
Epoch 7/20
16/16 [==============================] - 4s 229ms/step - loss: 0.1959
Epoch 8/20
16/16 [==============================] - 3s 213ms/step - loss: 0.1835
Epoch 9/20
16/16 [==============================] - 4s 223ms/step - loss: 0.1437
Epoch 10/20
16/16 [==============================] - 4s 220ms/step - loss: 0.1226
Epoch 11/20
16/16 [==============================] - 3s 212ms/step - loss: 0.1292
Epoch 12/20
16/16 [==============================] - 3s 209ms/step - loss: 0.1150
Epoch 13/20
16/16 [==============================] - 3s 200ms/step - loss: 0.1058
Epoch 14/20
16/16 [==============================] - 3s 200ms/step - loss: 0.1096
Epoch 15/20
16/16 [==============================] - 3s 208ms/step - loss: 0.0956
Epoch 16/20
16/16 [==============================] - 4s 222ms/step - loss: 0.1145
Epoch 17/20
16/16 [==============================] - 4s 221ms/step - loss: 0.0931
Epoch 18/20
16/16 [==============================] - 3s 212ms/step - loss: 0.0782
Epoch 19/20
16/16 [==============================] - 3s 211ms/step - loss: 0.0708
Epoch 20/20
16/16 [==============================] - 4s 223ms/step - loss: 0.0839
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/ad_output_19_1.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict the labels of the testset</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="c1"># Compute the metrics</span>
<span class="n">accuracy_test_rnn</span><span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy = &#39;</span><span class="p">,</span><span class="n">accuracy_test_rnn</span><span class="p">)</span>

<span class="n">precision_test_rnn</span><span class="o">=</span><span class="n">precision_score</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Precision = &#39;</span><span class="p">,</span><span class="n">precision_test_rnn</span><span class="p">)</span>

<span class="n">recall_test_rnn</span><span class="o">=</span><span class="n">recall_score</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Recall = &#39;</span><span class="p">,</span><span class="n">recall_test_rnn</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Accuracy =  0.976
Precision =  0.9785714285714285
Recall =  0.9785714285714285
</pre></div>
</div>
</div>
<div class="section" id="autoencoder-for-anomaly-detection">
<h2>Autoencoder for anomaly detection<a class="headerlink" href="#autoencoder-for-anomaly-detection" title="Permalink to this headline">¶</a></h2>
<p>Usually we do not have access to well labeled datasets, but have to frame the problem as an unsupervised learning process. But how can we use an autoencoder in this setting? The objective of an autoencoder is to minimize the reconstruction error of a given input. We will therefore train an autoencoder solely on the normal ecg sequences, such that it reconstructs these examples with minimal error. The idea now is the following: Abnormal rhythms should have a higher reconstruction error as the normal sequences, allowing us to classify a rhythm as an anomaly if the reconstruction error is higher than a fixed threshold.</p>
<div class="section" id="build-the-model">
<h3>Build the model<a class="headerlink" href="#build-the-model" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AnomalyDetector</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AnomalyDetector</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># Define the encoder network</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)])</span>
    <span class="c1"># Define the decoder network</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">140</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)])</span>
    
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># Define how an evaluation of the network is performed</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">decoded</span>

<span class="n">autoencoder</span> <span class="o">=</span> <span class="n">AnomalyDetector</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compile the model</span>
<span class="n">autoencoder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mae&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Notice that the autoencoder is trained using only the normal ECGs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">normal_train_data</span><span class="p">,</span> <span class="n">normal_train_data</span><span class="p">,</span> 
          <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
          <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">normal_test_data</span><span class="p">,</span> <span class="n">normal_test_data</span><span class="p">),</span>
          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
10/10 [==============================] - 0s 11ms/step - loss: 0.0580 - val_loss: 0.0559
Epoch 2/20
10/10 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.0517
Epoch 3/20
10/10 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.0454
Epoch 4/20
10/10 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.0381
Epoch 5/20
10/10 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0321
Epoch 6/20
10/10 [==============================] - 0s 3ms/step - loss: 0.0303 - val_loss: 0.0280
Epoch 7/20
10/10 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0250
Epoch 8/20
10/10 [==============================] - 0s 3ms/step - loss: 0.0245 - val_loss: 0.0235
Epoch 9/20
10/10 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0226
Epoch 10/20
10/10 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0218
Epoch 11/20
10/10 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0212
Epoch 12/20
10/10 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0209
Epoch 13/20
10/10 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0206
Epoch 14/20
10/10 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.0204
Epoch 15/20
10/10 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.0202
Epoch 16/20
10/10 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.0201
Epoch 17/20
10/10 [==============================] - 0s 3ms/step - loss: 0.0204 - val_loss: 0.0200
Epoch 18/20
10/10 [==============================] - 0s 3ms/step - loss: 0.0203 - val_loss: 0.0199
Epoch 19/20
10/10 [==============================] - 0s 3ms/step - loss: 0.0201 - val_loss: 0.0198
Epoch 20/20
10/10 [==============================] - 0s 3ms/step - loss: 0.0200 - val_loss: 0.0197
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/ad_output_28_1.png" /></p>
<p>Let’s take a look at the signal after encoding and decoding by the autoencoder.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">encoded_imgs</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">normal_test_data</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">decoded_imgs</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoded_imgs</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">normal_test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">decoded_imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">140</span><span class="p">),</span> <span class="n">decoded_imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">normal_test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightcoral&#39;</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Input&quot;</span><span class="p">,</span> <span class="s2">&quot;Reconstruction&quot;</span><span class="p">,</span> <span class="s2">&quot;Error&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/ad_output_30_0.png" /></p>
<p>Creating a similar plot for an anomalous test example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">encoded_imgs</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">anomalous_test_data</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">decoded_imgs</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoded_imgs</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">anomalous_test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">decoded_imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">140</span><span class="p">),</span> <span class="n">decoded_imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">anomalous_test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightcoral&#39;</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Input&quot;</span><span class="p">,</span> <span class="s2">&quot;Reconstruction&quot;</span><span class="p">,</span> <span class="s2">&quot;Error&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/ad_output_32_0.png" /></p>
</div>
<div class="section" id="detect-anomalies">
<h3>Detect anomalies<a class="headerlink" href="#detect-anomalies" title="Permalink to this headline">¶</a></h3>
<p>We will detect anomalies by calculating whether the reconstruction loss is greater than a fixed threshold. For this we use the mean average error for normal examples from the training set, then classify future examples as anomalous if the reconstruction error is higher than one standard deviation from the training set.</p>
<p>Ploting the reconstruction error on normal ECGs from the training set</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">reconstructions</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">normal_train_data</span><span class="p">)</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mae</span><span class="p">(</span><span class="n">reconstructions</span><span class="p">,</span> <span class="n">normal_train_data</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Train loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;No of examples&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/ad_output_36_0.png" /></p>
<p>Choose a threshold value that is one standard deviation above the mean.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">threshold_ae</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Threshold: &quot;</span><span class="p">,</span> <span class="n">threshold_ae</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Threshold:  0.031951994
</pre></div>
</div>
<p>Note: There are other strategies you could use to select a threshold value above which test examples should be classified as anomalous, the correct approach will depend on your dataset.</p>
<p>Most anomalous examples in the test set have a greater reconstruction error than the threshold. By changing the threshold, we can adjust <a class="reference external" href="https://developers.google.com/machine-learning/glossary#precision">precision</a> and <a class="reference external" href="https://developers.google.com/machine-learning/glossary#recall">recall</a> of the classifier.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">reconstructions</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">anomalous_test_data</span><span class="p">)</span>
<span class="n">test_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mae</span><span class="p">(</span><span class="n">reconstructions</span><span class="p">,</span> <span class="n">anomalous_test_data</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">test_loss</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">threshold_ae</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Test loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;No of examples&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/ad_output_41_0.png" /></p>
<p>Classify an ECG as an anomaly if the reconstruction error is greater than the threshold.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
  <span class="n">reconstructions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mae</span><span class="p">(</span><span class="n">reconstructions</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_stats</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">)))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Precision = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">precision_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">)))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Recall = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">recall_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">autoencoder</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">threshold_ae</span><span class="p">)</span>
<span class="n">print_stats</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Accuracy = 0.942
Precision = 0.9921568627450981
Recall = 0.9035714285714286
</pre></div>
</div>
</div>
</div>
<div class="section" id="variational-autoencoder-for-anomaly-detection">
<h2>Variational Autoencoder for anomaly detection<a class="headerlink" href="#variational-autoencoder-for-anomaly-detection" title="Permalink to this headline">¶</a></h2>
<p>Autoencoders have a strong tendency to overfit on the training data. In class you got to know variational autoencoders, which are designed to mitigate this problem. First, define a function performing the sampling in the laten space using the reparametrization trick (this allows backpropagation of the gradient).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Sampling</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">z_mean</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">z_mean</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">z_mean</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">z_log_var</span><span class="p">)</span> <span class="o">*</span> <span class="n">epsilon</span>
</pre></div>
</div>
<p>Define the encoder part of the network, containing the sampling step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">encoder_model</span><span class="p">(</span><span class="n">normal_train_data</span><span class="p">):</span>
    <span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">normal_train_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">encoder_inputs</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># So far we just copied the network from above</span>
    <span class="c1"># Now we generate the latent space of mean and log-variance, in this case of dimension 8</span>
    <span class="n">z_mean</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;z_mean&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;z_log_var&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># Sample from these distributions</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">Sampling</span><span class="p">()([</span><span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
    
    <span class="n">encoder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="p">[</span><span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">z</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;encoder&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">encoder</span>
</pre></div>
</div>
<p>Define the decoding network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">decoder_model</span><span class="p">(</span><span class="n">normal_train_data</span><span class="p">):</span>
    <span class="c1"># Recreate the network we used for the &#39;normal&#39; autoencoder</span>
    <span class="n">latent_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">latent_inputs</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">normal_train_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">decoder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">latent_inputs</span><span class="p">,</span> <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;decoder&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">decoder</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">VAE</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VAE</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">z_mean</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">reconstruction_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">reconstruction</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">kl_loss</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">z_log_var</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">z_mean</span><span class="p">)</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z_log_var</span><span class="p">)</span>
            <span class="n">kl_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">kl_loss</span><span class="p">)</span>
            <span class="n">kl_loss</span> <span class="o">*=</span> <span class="o">-</span><span class="mf">0.5</span>
            <span class="n">total_loss</span> <span class="o">=</span> <span class="n">reconstruction_loss</span> <span class="o">+</span> <span class="n">kl_loss</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">total_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_weights</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_weights</span><span class="p">))</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">total_loss</span><span class="p">,</span>
            <span class="s2">&quot;reconstruction_loss&quot;</span><span class="p">:</span> <span class="n">reconstruction_loss</span><span class="p">,</span>
            <span class="s2">&quot;kl_loss&quot;</span><span class="p">:</span> <span class="n">kl_loss</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">decoded</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the encoder and decoder models</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder_model</span><span class="p">(</span><span class="n">normal_train_data</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder_model</span><span class="p">(</span><span class="n">normal_train_data</span><span class="p">)</span>

<span class="c1"># Get the combined model</span>
<span class="n">vae</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">)</span>

<span class="c1"># Compile the model</span>
<span class="n">vae</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">())</span>

<span class="c1"># Fit the model to the training set</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">normal_train_data</span><span class="p">,</span> <span class="n">normal_train_data</span><span class="p">,</span> 
          <span class="n">epochs</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> 
          <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
          <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 1/80
19/19 [==============================] - 0s 2ms/step - loss: 3.4000 - reconstruction_loss: 3.3788 - kl_loss: 0.0211
Epoch 2/80
19/19 [==============================] - 0s 2ms/step - loss: 2.4422 - reconstruction_loss: 2.4147 - kl_loss: 0.0276
Epoch 3/80
19/19 [==============================] - 0s 2ms/step - loss: 1.8251 - reconstruction_loss: 1.8168 - kl_loss: 0.0083
Epoch 4/80
19/19 [==============================] - 0s 2ms/step - loss: 1.4528 - reconstruction_loss: 1.4517 - kl_loss: 0.0011
Epoch 5/80
19/19 [==============================] - 0s 2ms/step - loss: 1.2076 - reconstruction_loss: 1.2075 - kl_loss: 7.9393e-05
Epoch 6/80
19/19 [==============================] - 0s 2ms/step - loss: 1.0510 - reconstruction_loss: 1.0509 - kl_loss: 1.9042e-04
Epoch 7/80
19/19 [==============================] - 0s 2ms/step - loss: 0.9681 - reconstruction_loss: 0.9668 - kl_loss: 0.0012
Epoch 8/80
19/19 [==============================] - 0s 2ms/step - loss: 0.9196 - reconstruction_loss: 0.9158 - kl_loss: 0.0038
Epoch 9/80
19/19 [==============================] - 0s 1ms/step - loss: 0.8778 - reconstruction_loss: 0.8753 - kl_loss: 0.0025
Epoch 10/80
19/19 [==============================] - 0s 2ms/step - loss: 0.8553 - reconstruction_loss: 0.8540 - kl_loss: 0.0013
Epoch 11/80
19/19 [==============================] - 0s 2ms/step - loss: 0.8275 - reconstruction_loss: 0.8269 - kl_loss: 6.4228e-04
Epoch 12/80
19/19 [==============================] - 0s 2ms/step - loss: 0.8161 - reconstruction_loss: 0.8156 - kl_loss: 4.9332e-04
Epoch 13/80
19/19 [==============================] - 0s 2ms/step - loss: 0.8008 - reconstruction_loss: 0.8003 - kl_loss: 5.8550e-04
Epoch 14/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7892 - reconstruction_loss: 0.7886 - kl_loss: 6.2893e-04
Epoch 15/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7894 - reconstruction_loss: 0.7889 - kl_loss: 4.8324e-04
Epoch 16/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7733 - reconstruction_loss: 0.7729 - kl_loss: 4.1221e-04
Epoch 17/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7698 - reconstruction_loss: 0.7696 - kl_loss: 2.4133e-04
Epoch 18/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7680 - reconstruction_loss: 0.7678 - kl_loss: 2.1751e-04
Epoch 19/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7605 - reconstruction_loss: 0.7603 - kl_loss: 2.3450e-04
Epoch 20/80
19/19 [==============================] - 0s 3ms/step - loss: 0.7568 - reconstruction_loss: 0.7565 - kl_loss: 3.2571e-04
Epoch 21/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7498 - reconstruction_loss: 0.7492 - kl_loss: 6.1112e-04
Epoch 22/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7457 - reconstruction_loss: 0.7451 - kl_loss: 5.8004e-04
Epoch 23/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7446 - reconstruction_loss: 0.7440 - kl_loss: 5.4937e-04
Epoch 24/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7377 - reconstruction_loss: 0.7372 - kl_loss: 4.9361e-04
Epoch 25/80
19/19 [==============================] - 0s 3ms/step - loss: 0.7379 - reconstruction_loss: 0.7370 - kl_loss: 8.2635e-04
Epoch 26/80
19/19 [==============================] - 0s 3ms/step - loss: 0.7379 - reconstruction_loss: 0.7358 - kl_loss: 0.0021
Epoch 27/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7344 - reconstruction_loss: 0.7325 - kl_loss: 0.0019
Epoch 28/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7314 - reconstruction_loss: 0.7304 - kl_loss: 0.0010
Epoch 29/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7297 - reconstruction_loss: 0.7291 - kl_loss: 5.8975e-04
Epoch 30/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7249 - reconstruction_loss: 0.7244 - kl_loss: 4.4925e-04
Epoch 31/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7277 - reconstruction_loss: 0.7273 - kl_loss: 3.7447e-04
Epoch 32/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7256 - reconstruction_loss: 0.7250 - kl_loss: 6.0518e-04
Epoch 33/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7267 - reconstruction_loss: 0.7259 - kl_loss: 8.1938e-04
Epoch 34/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7229 - reconstruction_loss: 0.7223 - kl_loss: 5.9131e-04
Epoch 35/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7193 - reconstruction_loss: 0.7189 - kl_loss: 4.0237e-04
Epoch 36/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7195 - reconstruction_loss: 0.7191 - kl_loss: 3.1504e-04
Epoch 37/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7187 - reconstruction_loss: 0.7184 - kl_loss: 3.0438e-04
Epoch 38/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7166 - reconstruction_loss: 0.7163 - kl_loss: 3.1537e-04
Epoch 39/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7180 - reconstruction_loss: 0.7177 - kl_loss: 2.9129e-04
Epoch 40/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7154 - reconstruction_loss: 0.7150 - kl_loss: 3.4823e-04
Epoch 41/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7140 - reconstruction_loss: 0.7134 - kl_loss: 5.5343e-04
Epoch 42/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7132 - reconstruction_loss: 0.7128 - kl_loss: 4.2403e-04
Epoch 43/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7124 - reconstruction_loss: 0.7121 - kl_loss: 3.0269e-04
Epoch 44/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7122 - reconstruction_loss: 0.7119 - kl_loss: 2.6272e-04
Epoch 45/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7113 - reconstruction_loss: 0.7110 - kl_loss: 3.0712e-04
Epoch 46/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7104 - reconstruction_loss: 0.7101 - kl_loss: 2.7785e-04
Epoch 47/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7101 - reconstruction_loss: 0.7099 - kl_loss: 2.5579e-04
Epoch 48/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7100 - reconstruction_loss: 0.7098 - kl_loss: 2.4659e-04
Epoch 49/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7091 - reconstruction_loss: 0.7089 - kl_loss: 2.4807e-04
Epoch 50/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7080 - reconstruction_loss: 0.7078 - kl_loss: 2.6399e-04
Epoch 51/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7065 - reconstruction_loss: 0.7063 - kl_loss: 2.6862e-04
Epoch 52/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7059 - reconstruction_loss: 0.7057 - kl_loss: 2.5128e-04
Epoch 53/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7054 - reconstruction_loss: 0.7051 - kl_loss: 2.3024e-04
Epoch 54/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7052 - reconstruction_loss: 0.7050 - kl_loss: 2.2133e-04
Epoch 55/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7051 - reconstruction_loss: 0.7048 - kl_loss: 2.3693e-04
Epoch 56/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7041 - reconstruction_loss: 0.7039 - kl_loss: 2.5301e-04
Epoch 57/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7030 - reconstruction_loss: 0.7028 - kl_loss: 2.5449e-04
Epoch 58/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7033 - reconstruction_loss: 0.7030 - kl_loss: 2.5160e-04
Epoch 59/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7019 - reconstruction_loss: 0.7016 - kl_loss: 2.4097e-04
Epoch 60/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7023 - reconstruction_loss: 0.7020 - kl_loss: 2.4464e-04
Epoch 61/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7027 - reconstruction_loss: 0.7024 - kl_loss: 2.4159e-04
Epoch 62/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7014 - reconstruction_loss: 0.7011 - kl_loss: 2.3552e-04
Epoch 63/80
19/19 [==============================] - 0s 2ms/step - loss: 0.7006 - reconstruction_loss: 0.7004 - kl_loss: 2.3707e-04
Epoch 64/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6992 - reconstruction_loss: 0.6989 - kl_loss: 2.3298e-04
Epoch 65/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6999 - reconstruction_loss: 0.6997 - kl_loss: 2.2947e-04
Epoch 66/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6991 - reconstruction_loss: 0.6988 - kl_loss: 2.3069e-04
Epoch 67/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6988 - reconstruction_loss: 0.6985 - kl_loss: 2.2607e-04
Epoch 68/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6979 - reconstruction_loss: 0.6977 - kl_loss: 2.2115e-04
Epoch 69/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6972 - reconstruction_loss: 0.6970 - kl_loss: 2.1557e-04
Epoch 70/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6971 - reconstruction_loss: 0.6968 - kl_loss: 2.0963e-04
Epoch 71/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6962 - reconstruction_loss: 0.6960 - kl_loss: 2.1004e-04
Epoch 72/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6960 - reconstruction_loss: 0.6958 - kl_loss: 2.0413e-04
Epoch 73/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6954 - reconstruction_loss: 0.6952 - kl_loss: 1.9985e-04
Epoch 74/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6952 - reconstruction_loss: 0.6950 - kl_loss: 1.9788e-04
Epoch 75/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6949 - reconstruction_loss: 0.6947 - kl_loss: 2.0535e-04
Epoch 76/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6945 - reconstruction_loss: 0.6943 - kl_loss: 2.0023e-04
Epoch 77/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6948 - reconstruction_loss: 0.6946 - kl_loss: 1.9286e-04
Epoch 78/80
19/19 [==============================] - 0s 1ms/step - loss: 0.6944 - reconstruction_loss: 0.6942 - kl_loss: 1.8995e-04
Epoch 79/80
19/19 [==============================] - 0s 1ms/step - loss: 0.6939 - reconstruction_loss: 0.6937 - kl_loss: 1.8839e-04
Epoch 80/80
19/19 [==============================] - 0s 2ms/step - loss: 0.6934 - reconstruction_loss: 0.6933 - kl_loss: 1.8521e-04
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/ad_output_53_1.png" /></p>
<p>Look at the reconstruction of a normal ecg sequence of the testset</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">encoded_imgs</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">normal_test_data</span><span class="p">)</span>
<span class="n">decoded_imgs</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoded_imgs</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">normal_test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">decoded_imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">140</span><span class="p">),</span> <span class="n">decoded_imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">normal_test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightcoral&#39;</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Input&quot;</span><span class="p">,</span> <span class="s2">&quot;Reconstruction&quot;</span><span class="p">,</span> <span class="s2">&quot;Error&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/ad_output_55_0.png" /></p>
<p>As before, we compute the threshold from the mean absolute error plus one standard deviation</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">reconstructions</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">normal_train_data</span><span class="p">)</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mae</span><span class="p">(</span><span class="n">reconstructions</span><span class="p">,</span> <span class="n">normal_train_data</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Train loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;No of examples&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/ad_output_57_0.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">threshold_vae</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Threshold: &quot;</span><span class="p">,</span> <span class="n">threshold_vae</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Threshold:  0.11696462
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">reconstructions</span> <span class="o">=</span> <span class="n">vae</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">anomalous_test_data</span><span class="p">)</span>
<span class="n">test_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mae</span><span class="p">(</span><span class="n">reconstructions</span><span class="p">,</span> <span class="n">anomalous_test_data</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">test_loss</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">threshold_vae</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Test loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;No of examples&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="png" src="../../_images/ad_output_59_0.png" /></p>
<p>Classify an ECG as an anomaly if the reconstruction error is greater than the threshold.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">vae</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">threshold_vae</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variational Autoencoder&quot;</span><span class="p">)</span>
<span class="n">print_stats</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Variational Autoencoder
Accuracy = 0.91
Precision = 0.9351851851851852
Recall = 0.9017857142857143
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">autoencoder</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">threshold_ae</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Autoencoder&quot;</span><span class="p">)</span>
<span class="n">print_stats</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Autoencoder
Accuracy = 0.942
Precision = 0.9921568627450981
Recall = 0.9035714285714286
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RNN as classifier&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy = &#39;</span><span class="p">,</span><span class="n">accuracy_test_rnn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Precision = &#39;</span><span class="p">,</span><span class="n">precision_test_rnn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Recall = &#39;</span><span class="p">,</span><span class="n">recall_test_rnn</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>RNN as classifier
Accuracy =  0.979
Precision =  0.9696428571428571
Recall =  0.9926873857404022
</pre></div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/unsupervised_learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="Molecule_gen_RNN.html" title="previous page">Exercise: Molecule Generation with an RNN</a>
    <a class='right-next' id="next-link" href="../interpretability/ml_interpretability.html" title="next page">Interpretability of Neural Networks</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Eliska<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>