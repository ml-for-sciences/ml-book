
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Restricted Boltzmann Machine &#8212; Machine Learning for Scientists</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Training an RNN without Supervision" href="ml_unsupervised-2.html" />
    <link rel="prev" title="Unsupervised Learning" href="ml_unsupervised.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/cluster.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning for Scientists</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../structuring_data/ml_without_neural_network.html">
   Structuring Data without Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-1.html">
     Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-2.html">
     Kernel PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-3.html">
     t-SNE as a Nonlinear Visualization Technique
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/ml_without_neural_network-4.html">
     Clustering Algorithms: the example of
     <span class="math notranslate nohighlight">
      \(k\)
     </span>
     -means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/pca.html">
     Exercise: Principle Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../structuring_data/Dimensionality_reduction.html">
     Exercise: Dimensionality Reduction
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs.html">
   Supervised Learning without Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-1.html">
     Linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-2.html">
     Binary Classification and Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/ml_supervised_wo_NNs-3.html">
     More than two classes: logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Linear-regression.html">
     Exercise: Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_wo_NNs/Classification.html">
     Exercise: Classification without Neural Networks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../supervised_learning_w_NNs/ml_supervised_w_NNs.html">
   Supervised Learning with Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/Classification-2.html">
     Exercise: Dense Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html">
     Exercise: Machine Learning Optimizers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html#exercise-learning-rate-scheduling">
     Exercise: Learning Rate Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/NN-opt-reg.html#exercise-regularizing-neural-networks">
     Exercise: Regularizing Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/CNNs.html">
     Exercise: Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../supervised_learning_w_NNs/exoplanets_RNN_CNN.html">
     Exercise: Discovery of Exoplanets with RNNs and CNNs
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="ml_unsupervised.html">
   Unsupervised Learning
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Restricted Boltzmann Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_unsupervised-2.html">
     Training an RNN without Supervision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_unsupervised-3.html">
     Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ml_unsupervised-4.html">
     Generative Adversarial Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Denoising.html">
     Exercise: Denoising with Restricted Boltzmann Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Molecule_gen_RNN.html">
     Exercise: Molecule Generation with an RNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Anomaly_Detection_RNN_AE_VAE.html">
     Exercise: Anomaly Detection
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../interpretability/ml_interpretability.html">
   Interpretability of Neural Networks
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-1.html">
     Dreaming and the Problem of Extrapolation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-2.html">
     Adversarial Attacks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/ml_interpretability-3.html">
     Interpreting Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interpretability/Transfer-learning-attacks.html">
     Exercise: Transfer Learning and Adversarial Attacks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning.html">
   Reinforcement Learning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-1.html">
     Exploration versus Exploitation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-2.html">
     Finite Markov Decision Process
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-3.html">
     Policies and Value Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-4.html">
     Temporal-difference Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../reinforcement_learning/ml_reinforcement-learning-5.html">
     Function Approximation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../conclusion/ml_conclusion.html">
   Concluding Remarks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  About us
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../about_us.html">
   Who we are
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/docs/unsupervised_learning/ml_unsupervised-1.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-an-rbm">
   Training an RBM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-signal-or-image-reconstruction-denoising">
   Example: signal or image reconstruction/denoising
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="restricted-boltzmann-machine">
<h1>Restricted Boltzmann Machine<a class="headerlink" href="#restricted-boltzmann-machine" title="Permalink to this headline">¶</a></h1>
<p><em>Restricted Boltzmann Machines</em> (RBM) are a class of generative stochastic neural networks. More specifically, given some (binary) input data <span class="math notranslate nohighlight">\(\mathbf{x}\in\{0,1\}^{n_v}\)</span>, an RBM can be trained to approximate the probability distribution of this input. Moreover, once the neural network is trained to approximate the distribution of the input, we can sample from the network, in other words we generate new instances from the learned probability distribution.</p>
<p>The RBM consists of two layers (see <a class="reference internal" href="#fig-rbm"><span class="std std-numref">Fig. 11</span></a>) of <em>binary units</em>. Each binary unit is a variable which can take the values <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>. We call the first (input) layer visible and the second layer hidden. The visible layer with input variables <span class="math notranslate nohighlight">\(\lbrace v_{1}, v_{2}, \dots v_{n_{\mathrm{v}}}\rbrace\)</span>, which we collect in the vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>, is connected to the hidden layer with variables <span class="math notranslate nohighlight">\(\{ h_{1}, h_{2}, \dots h_{n_{\mathrm{h}}}\}\)</span>, which we collect in the vector <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>. The role of the hidden layer is to mediate correlations between the units of the visible layer. In contrast to the neural networks we have seen in the previous chapter, the hidden layer is not followed by an output layer. Instead, the RBM represents a probability distribution <span class="math notranslate nohighlight">\(P_{\text{rbm}}(\mathbf{v})\)</span>, which depends on variational parameters represented by the weights and biases of a neural network. The RBM, as illustrated by the graph in <a class="reference internal" href="#fig-rbm"><span class="std std-numref">Fig. 11</span></a>, is a special case of a network structure known as a Boltzmann machine with the restriction that a unit in the visible layer is only connected to hidden units and vice versa, hence the name <em>restricted</em> Boltzmann machine.</p>
<div class="figure align-default" id="fig-rbm">
<img alt="../../_images/rbm.png" src="../../_images/rbm.png" />
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text"><strong>Restricted Boltzmann machine.</strong> Each of the three visible units and
five hidden units represents a variable that can take the values <span class="math notranslate nohighlight">\(\pm1\)</span>
and the connections between them represent the entries <span class="math notranslate nohighlight">\(W_{ij}\)</span> of the
weight matrix that enters the energy function <a class="reference internal" href="#equation-eqn-rbm-energy">(34)</a>.</span><a class="headerlink" href="#fig-rbm" title="Permalink to this image">¶</a></p>
</div>
<p>The structure of the RBM is motivated from statistical physics: To each choice of the binary vectors <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>, we assign a value we call the energy</p>
<div class="math notranslate nohighlight" id="equation-eqn-rbm-energy">
<span class="eqno">(34)<a class="headerlink" href="#equation-eqn-rbm-energy" title="Permalink to this equation">¶</a></span>\[E(\mathbf{v},\mathbf{h}) = -\sum_{i}a_{i}v_{i} - \sum_{j}b_{j}h_{j} - \sum_{ij} v_{i}W_{ij}h_{j},\]</div>
<p>where the vectors <span class="math notranslate nohighlight">\(\mathbf{a}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>, and the matrix <span class="math notranslate nohighlight">\(W\)</span> are the variational parameters of the model. Given the energy, the probability distribution over the configurations <span class="math notranslate nohighlight">\((\mathbf{v}, \mathbf{h})\)</span> is defined as</p>
<div class="math notranslate nohighlight" id="equation-eqn-rbm-joint-probability">
<span class="eqno">(35)<a class="headerlink" href="#equation-eqn-rbm-joint-probability" title="Permalink to this equation">¶</a></span>\[P_{\textrm{rbm}}(\mathbf{v},\mathbf{h}) = \frac{1}{Z}e^{-E(\mathbf{v},\mathbf{h})},\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-eqn-partition-function">
<span class="eqno">(36)<a class="headerlink" href="#equation-eqn-partition-function" title="Permalink to this equation">¶</a></span>\[Z = \sum_{\mathbf{v},\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h})}\]</div>
<p>is a normalisation factor called the partition function. The sum in Eq. <a class="reference internal" href="#equation-eqn-partition-function">(36)</a> runs over all binary vectors <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>, i.e., vectors with entries <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>. The probability that the model assigns to a visible vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> is then the marginal over the joint probability distribution Eq. <a class="reference internal" href="#equation-eqn-rbm-joint-probability">(35)</a>,</p>
<div class="math notranslate nohighlight" id="equation-eqn-rbm-visible-probability">
<span class="eqno">(37)<a class="headerlink" href="#equation-eqn-rbm-visible-probability" title="Permalink to this equation">¶</a></span>\[P_{\textrm{rbm}}(\mathbf{v}) = \sum_{\mathbf{h}} P_{\textrm{rbm}}(\mathbf{v},\mathbf{h}) = \frac{1}{Z}\sum_{h}e^{-E(\mathbf{v},\mathbf{h})}.\]</div>
<p>As a result of the restriction, the visible units, with the hidden units fixed, are mutually independent: given a choice of the hidden units <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>, we have an <strong>independent</strong> probability distribution for
<strong>each</strong> visible unit given by</p>
<div class="math notranslate nohighlight">
\[	P_{\textrm{rbm}}(v_{i} = 1 | \mathbf{h}) = \sigma(a_{i} + \sum_{j}W_{ij}h_{j}), \qquad i=1,\ldots, n_{\mathrm{v}},\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma(x) = 1/(1+e^{-x})\)</span> is the sigmoid function. Similarly, with the visible units fixed, the individual hidden units are also mutually independent with the probability distribution</p>
<div class="math notranslate nohighlight" id="equation-eqn-rbm-p-h-v">
<span class="eqno">(38)<a class="headerlink" href="#equation-eqn-rbm-p-h-v" title="Permalink to this equation">¶</a></span>\[P_{\textrm{rbm}}(h_{j} = 1 | \mathbf{v}) = \sigma(b_{j} + \sum_{i}v_{i}W_{ij})\qquad j=1,\ldots, n_{\mathrm{h}}. \]</div>
<p>The visible (hidden) units can thus be interpreted as artificial neurons connected to the hidden (visible) units with sigmoid activation function and bias <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> (<span class="math notranslate nohighlight">\(\mathbf{b}\)</span>). A direct consequence of this mutual independence is that sampling a vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> or <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> reduces to sampling every component individually. Notice that this simplification comes about due to the restriction that visible (hidden) units do not directly interact amongst themselves, i.e. there are no terms proportional to <span class="math notranslate nohighlight">\(v_i v_j\)</span> or <span class="math notranslate nohighlight">\(h_i h_j\)</span> in Eq. <a class="reference internal" href="#equation-eqn-rbm-energy">(34)</a>. In the following, we explain how one can train an RBM and discuss possible applications of RBMs.</p>
<div class="section" id="training-an-rbm">
<h2>Training an RBM<a class="headerlink" href="#training-an-rbm" title="Permalink to this headline">¶</a></h2>
<p>Consider a set of binary input data <span class="math notranslate nohighlight">\(\mathbf{x}_k\)</span>, <span class="math notranslate nohighlight">\(k=1,\ldots,M\)</span>, drawn from a probability distribution <span class="math notranslate nohighlight">\(P_{\textrm{data}}(\mathbf{x})\)</span>. The aim of the training is to tune the parameters <span class="math notranslate nohighlight">\(\lbrace \mathbf{a}, \mathbf{b}, W \rbrace\)</span> in an RBM such that after training <span class="math notranslate nohighlight">\(P_{\textrm{rbm}}(\mathbf{x}) \approx  P_{\textrm{data}}(\mathbf{x})\)</span>. The standard approach to solve this problem is the maximum likelihood principle, in other words we want to find the parameters <span class="math notranslate nohighlight">\(\lbrace \mathbf{a}, \mathbf{b}, W \rbrace\)</span> which maximize the probability that our model produces the data <span class="math notranslate nohighlight">\(\mathbf{x}_k\)</span>.</p>
<p>Maximizing the likelihood <span class="math notranslate nohighlight">\(\mathcal{L}(\mathbf{a},\mathbf{b},W) =  \prod P_{\textrm{rbm}}(\mathbf{x}_{k})\)</span> is equivalent to training the RBM using a loss function we have encountered before, the negative log-likelihood</p>
<div class="math notranslate nohighlight">
\[	L(\mathbf{a},\mathbf{b},W) = - \sum_{k=1}^{M} \log P_{\textrm{rbm}}(\mathbf{x}_{k}).\]</div>
<p>For the gradient descent, we need derivatives of the loss function of the form</p>
<div class="math notranslate nohighlight" id="equation-eqn-log-likelihood-derivative">
<span class="eqno">(39)<a class="headerlink" href="#equation-eqn-log-likelihood-derivative" title="Permalink to this equation">¶</a></span>\[\frac{\partial L(\mathbf{a},\mathbf{b},W)}{\partial W_{ij}} = -\sum_{k=1}^{M} \frac{\partial\log P_{\textrm{rbm}}(\mathbf{x}_{k})}{\partial W_{ij}}.\]</div>
<p>This derivative consists of two terms,</p>
<div class="math notranslate nohighlight" id="equation-eqn-rbm-derivatives">
<span class="eqno">(40)<a class="headerlink" href="#equation-eqn-rbm-derivatives" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
        \frac{\partial\log P_{\textrm{rbm}}(\mathbf{x})}{\partial W_{ij}} %&amp;= \frac{\partial}{\partial W_{ij}}\left( -\log \sum_{\bm{vh}} e^{E(\mathbf{v},\mathbf{h})} + \log \sum_{\mathbf{h}} e^{E(\mathbf{x},\mathbf{h})}  \right) \\
        %&amp;= -\frac{1}{Z}\sum_{\bm{vh}}v_{i}h_{j}e^{E(\mathbf{v},\mathbf{h})} +  \frac{1}{\sum_{\mathbf{h}}e^{E(\mathbf{x},\mathbf{h})}} \sum_{h}x_{i}h_{j}e^{E(\mathbf{x},\mathbf{h})}\\
        %&amp;= \sum_{h_{j}}x_{i}h_{j} P_{\textrm{rbm}}(h_{j}|\mathbf{x}) - \sum_{\mathbf{v},\mathbf{h}} v_{i} h_{j} P_{\textrm{rbm}}(\mathbf{v},\mathbf{h}) \\
        &amp;= x_{i}P_{\textrm{rbm}}(h_{j}=1|\mathbf{x}) - \sum_{\mathbf{v}} v_{i} P_{\textrm{rbm}}(h_{j}=1|\mathbf{v}) P_{\textrm{rbm}}(\mathbf{v})
\end{split}\end{split}\]</div>
<p>and similarly simple forms are found for the derivatives with respect to the components of <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. We can then iteratively update the parameters just as we have done in Chapter <a class="reference internal" href="../supervised_learning_w_NNs/ml_supervised_w_NNs.html#sec-supervised"><span class="std std-ref">Supervised Learning with Neural Networks</span></a>,</p>
<div class="math notranslate nohighlight">
\[W_{ij} \rightarrow W_{ij} - \eta \frac{\partial L(a,b,W)}{\partial W_{ij}}\]</div>
<p>with a sufficiently small learning rate <span class="math notranslate nohighlight">\(\eta\)</span>. As we have seen in the previous chapter in the context of backpropagation, we can reduce the computational cost by replacing the summation over the whole data set in Eq. <a class="reference internal" href="#equation-eqn-log-likelihood-derivative">(39)</a> with a summation over a small randomly chosen batch of samples. This reduction in the computational cost comes at the expense of noise, but at the same time it can help to improve generalization.</p>
<p>However, there is one more problem: The second summation in Eq. <a class="reference internal" href="#equation-eqn-rbm-derivatives">(40)</a>, which contains <span class="math notranslate nohighlight">\(2^{n_v}\)</span> terms, cannot be efficiently evaluated exactly. Instead, we have to approximate the sum by sampling the visible layer <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> from the marginal probability distribution <span class="math notranslate nohighlight">\(P_{\textrm{rbm}}(\mathbf{v})\)</span>. This sampling can be done using <em>Gibbs sampling</em> as follows:</p>
<div class="admonition-gibbs-sampling admonition" id="alg-gibbs-sampling">
<p class="admonition-title">Gibbs-Sampling</p>
<p><strong>Input:</strong> Any visible vector <span class="math notranslate nohighlight">\(\mathbf{v}(0)\)</span>  <br />
<strong>Output:</strong> Visible vector <span class="math notranslate nohighlight">\(\mathbf{v}(r)\)</span>  <br />
<strong>for:</strong> <span class="math notranslate nohighlight">\(n=1\)</span>\dots <span class="math notranslate nohighlight">\(r\)</span>  <br />
<span class="math notranslate nohighlight">\(\quad\)</span> sample <span class="math notranslate nohighlight">\(\mathbf{h}(n)\)</span> from <span class="math notranslate nohighlight">\(P_{\rm rbm}(\mathbf{h}|\mathbf{v}=\mathbf{v}(n-1))\)</span>  <br />
<span class="math notranslate nohighlight">\(\quad\)</span> sample <span class="math notranslate nohighlight">\(\mathbf{v}(n)\)</span> from <span class="math notranslate nohighlight">\(P_{\rm rbm}(\mathbf{v}|\mathbf{h}=\mathbf{h}(n))\)</span>  <br />
<strong>end</strong></p>
</div>
<p>With sufficiently many steps <span class="math notranslate nohighlight">\(r\)</span>, the vector <span class="math notranslate nohighlight">\(\mathbf{v}(r)\)</span> is an unbiased sample drawn from <span class="math notranslate nohighlight">\(P_{\textrm{rbm}}(\mathbf{v})\)</span>. By repeating the procedure, we can obtain multiple samples to estimate the summation. Note that this is still rather computationally expensive, requiring multiple evaluations on the model.</p>
<p>The key innovation which allows the training of an RBM to be computationally feasible was proposed by Geoffrey Hinton (2002). Instead of obtaining multiple samples, we simply perform the Gibbs sampling with <span class="math notranslate nohighlight">\(r\)</span> steps and estimate the summation with a single sample, in other words we replace the second summation in Eq. <a class="reference internal" href="#equation-eqn-rbm-derivatives">(40)</a> with</p>
<div class="math notranslate nohighlight">
\[\sum_{\mathbf{v}} v_{i} P_{\textrm{rbm}}(h_{j}=1|\mathbf{v}) P_{\textrm{rbm}}(\mathbf{v}) \rightarrow v'_{i} P_{\textrm{rbm}}(h_{j}=1|\mathbf{v}'),\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{v}' = \mathbf{v}(r)\)</span> is simply the sample obtained from <span class="math notranslate nohighlight">\(r\)</span>-step Gibbs sampling. With this modification, the gradient, Eq. <a class="reference internal" href="#equation-eqn-rbm-derivatives">(40)</a>, can be approximated as</p>
<div class="math notranslate nohighlight">
\[\frac{\partial\log P_{\textrm{rbm}}(\mathbf{x})}{\partial W_{ij}} \approx x_{i}P_{\textrm{rbm}}(h_{j}=1|\mathbf{x}) -  v'_{i} P_{\textrm{rbm}}(h_{j}=1|\mathbf{v}').\]</div>
<p>This method is known as <em>contrastive divergence</em>. Although the quantity computed is only a biased estimator of the gradient, this approach is found to work well in practice. The complete algorithm for training a RBM with <span class="math notranslate nohighlight">\(r\)</span>-step contrastive divergence can be summarised as follows:</p>
<div class="admonition-contrastive-divergence admonition" id="alg-contrastive-divergence">
<p class="admonition-title">Contrastive divergence</p>
<p><strong>Input:</strong> Dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \lbrace \ \mathbf{x}_{1}, \ \mathbf{x}_{2}, \dots \ \mathbf{x}_{M} \rbrace\)</span> drawn from a distribution <span class="math notranslate nohighlight">\(P(x)\)</span>} <br />
initialize the RBM weights <span class="math notranslate nohighlight">\(\lbrace \mathbf{a},\mathbf{b},W \rbrace\)</span> <br />
Initialize <span class="math notranslate nohighlight">\(\Delta W_{ij} = \Delta a_{i} = \Delta b_{j} =0\)</span> <br />
<strong>while:</strong> not converged <strong>do</strong> <br />
<span class="math notranslate nohighlight">\(\quad\)</span>  select a random batch <span class="math notranslate nohighlight">\(S\)</span> of samples from the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> <br />
<span class="math notranslate nohighlight">\(\quad\)</span> <strong>forall</strong> <span class="math notranslate nohighlight">\(\mathbf{x} \in S\)</span> <br />
<span class="math notranslate nohighlight">\(\quad\quad\)</span> Obtain <span class="math notranslate nohighlight">\(\ \mathbf{v}'\)</span> by <span class="math notranslate nohighlight">\(r\)</span>-step Gibbs sampling starting from <span class="math notranslate nohighlight">\(\ \mathbf{x}\)</span> <br />
<span class="math notranslate nohighlight">\(\quad\quad\)</span> <span class="math notranslate nohighlight">\(\Delta W_{ij} \leftarrow \Delta W_{ij} - x_{i}P_{\textrm{rbm}}(h_{j}=1|\ \mathbf{x}) +  v'_{i} P_{\textrm{rbm}}(h_{j}=1|\ \mathbf{h}')\)</span> <br />
<span class="math notranslate nohighlight">\(\quad\)</span> <strong>end</strong> <br />
<span class="math notranslate nohighlight">\(\quad\)</span>  <span class="math notranslate nohighlight">\(W_{ij} \leftarrow W_{ij} - \eta\Delta W_{ij}\)</span> <br />
<span class="math notranslate nohighlight">\(\quad\)</span> (and similarly for <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>) <br />
<strong>end</strong></p>
</div>
<p>Having trained the RBM to represent the underlying data distribution
<span class="math notranslate nohighlight">\(P(\mathbf{x})\)</span>, there are a few ways one can use the trained model:</p>
<ol class="simple">
<li><p><strong>Pretraining:</strong> We can use <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> as the initial weights
and biases for a deep network (c.f. Chapter 4), which is then
fine-tuned with gradient descent and backpropagation.</p></li>
<li><p><strong>Generative Modelling:</strong> As a generative model, a trained RBM can be
used to generate new samples via Gibbs sampling. Some potential uses of the
generative aspect of the RBM include <em>recommender systems</em> and
<em>image reconstruction</em>. In the following subsection, we provide an
example, where an RBM is used to reconstruct a noisy signal.</p></li>
</ol>
</div>
<div class="section" id="example-signal-or-image-reconstruction-denoising">
<h2>Example: signal or image reconstruction/denoising<a class="headerlink" href="#example-signal-or-image-reconstruction-denoising" title="Permalink to this headline">¶</a></h2>
<p>A major drawback of the simple RBMs for their application is the fact that they only take binary data as input. As an example, we thus look at simple periodic waveforms with 60 sample points. In particular, we use sawtooth, sine, and square waveforms. In order to have quasi-continuous data, we use eight bits for each point, such that our signal can take values from 0 to 255. Finally, we generate samples to train with a small variation in the maximum value, the periodicity, as well as the center point of each waveform.</p>
<p>After training the RBM using the contrastive divergence algorithm, we now have a model which represents the data distribution of the binarized waveforms. Consider now a signal which has been corrupted, meaning some parts of the waveform have not been received, in other words they are set to 0. By feeding this corrupted data into the RBM and performing a few iterations of Gibbs sampling, we can obtain a reconstruction of the signal, where the missing part has been repaired, as can been seen at the bottom of <a class="reference internal" href="#fig-rbm-reconstruction"><span class="std std-numref">Fig. 12</span></a>.</p>
<p>Note that the same procedure can be used to reconstruct or denoise images. Due to the limitation to binary data, however, the picture has to either be binarized, or the input size to the RBM becomes fairly large for high-resolution pictures. It is thus not surprising that while RBMs have been popular in the mid-2000s, they have largely been superseded by more modern and architectures such as <em>generative adversarial networks</em> which we shall explore later in the chapter. However, they still serve a pedagogical purpose and could also provide inspiration for future innovations, in particular in science. A recent example is the idea of using an RBM to represent a quantum mechanical state.</p>
<div class="figure align-default" id="fig-rbm-reconstruction">
<img alt="../../_images/rbm_reconstr.png" src="../../_images/rbm_reconstr.png" />
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text"><strong>Signal reconstruction.</strong> Using an RBM to repair a corrupted signal,
here a sine and a sawtooth
waveform.</span><a class="headerlink" href="#fig-rbm-reconstruction" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/unsupervised_learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="ml_unsupervised.html" title="previous page">Unsupervised Learning</a>
    <a class='right-next' id="next-link" href="ml_unsupervised-2.html" title="next page">Training an RNN without Supervision</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Eliska<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>