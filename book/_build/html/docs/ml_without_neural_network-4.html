
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear classifiers and their extensions &#8212; Machine Learning for Sciences</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Supervised Learning without Neural Networks {#sec: linear methods for supervised learning}" href="ml_without_neural_network-3.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/cluster.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning for Sciences</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_without_neural_network-1.html">
   Structuring Data without Neural Networks {#sec:structuring_data}
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ml_without_neural_network-3.html">
   Supervised Learning without Neural Networks {#sec: linear methods for supervised learning}
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Linear classifiers and their extensions
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/ml_without_neural_network-4.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#binary-classification-and-support-vector-machines">
   Binary classification and support vector machines
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-than-two-classes-logistic-regression">
   More than two classes: logistic regression
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-classifiers-and-their-extensions">
<h1>Linear classifiers and their extensions<a class="headerlink" href="#linear-classifiers-and-their-extensions" title="Permalink to this headline">¶</a></h1>
<div class="section" id="binary-classification-and-support-vector-machines">
<h2>Binary classification and support vector machines<a class="headerlink" href="#binary-classification-and-support-vector-machines" title="Permalink to this headline">¶</a></h2>
<p>In a classification problem, the aim is to categorize the inputs into
one of a finite set of classes. Formulated as a supervised learning
task, the dataset again consists of input-output pairs, i.e.
<span class="math notranslate nohighlight">\(\lbrace({x}_{1}, y_{1}), \dots, ({x}_{m}, y_{m})\rbrace\)</span> with
<span class="math notranslate nohighlight">\({x}\in \mathbb{R}^n\)</span>. However, unlike regression problems, the
output <span class="math notranslate nohighlight">\(y\)</span> is a discrete integer number representing one of the classes.
In a binary classification problem, in other words a problem with only
two classes, it is natural to choose <span class="math notranslate nohighlight">\(y\in\{-1, 1\}\)</span>.</p>
<p>We have introduced linear regression in the previous section as a method
for supervised learning when the output is a real number. Here, we will
see how we can use the same model for a binary classification task. If
we look at the regression problem, we first note that geometrically
$<span class="math notranslate nohighlight">\(\label{eqn: Univariate Linear Model B}
     f(\boldsymbol{x}|{\beta}) = \beta_0 + \sum_{j=1}^{n} \beta_{j}x_{j} = 0\)</span><span class="math notranslate nohighlight">\(
defines a hyperplane perpendicular to the vector with elements
\)</span>\beta_{j\geq1}<span class="math notranslate nohighlight">\(. If we fix the length \)</span>\sum_{j=1}^n \beta_j^2=1<span class="math notranslate nohighlight">\(, then
\)</span>f({x}|{\beta})<span class="math notranslate nohighlight">\( measures the (signed) distance of \)</span>{x}<span class="math notranslate nohighlight">\( to the
hyperplane with a sign depending on which side of the plane the point
\)</span>{x}<em>i<span class="math notranslate nohighlight">\( lies. To use this model as a classifier, we thus define
\)</span><span class="math notranslate nohighlight">\(F({x}|{\beta}) = \sign f({x}|{\beta}),
  \label{eq:binaryclassifier}\)</span><span class="math notranslate nohighlight">\( which yields \)</span>{+1, -1}<span class="math notranslate nohighlight">\(. If the two
classes are (completely) linearly separable, then the goal of the
classification is to find a hyperplane that separates the two classes in
feature space. Specifically, we look for parameters \)</span>{\beta}<span class="math notranslate nohighlight">\(, such
that \)</span><span class="math notranslate nohighlight">\(y_i \tilde{{x}}_i^T{\beta} &gt; M, \quad \forall i,
  \label{eq:separable}\)</span><span class="math notranslate nohighlight">\( where \)</span>M<span class="math notranslate nohighlight">\( is called the *margin*. The optimal
solution \)</span>\hat{{\beta}}<span class="math notranslate nohighlight">\( then maximizes this margin. Note that
instead of fixing the norm of \)</span>\beta</em>{j\geq1}<span class="math notranslate nohighlight">\( and maximizing \)</span>M<span class="math notranslate nohighlight">\(, it is
customary to minimize \)</span>\sum_{j=1}^n \beta_j^2<span class="math notranslate nohighlight">\( setting \)</span>M=1$ in Eq. .</p>
<p>In most cases, the two classes are not completely separable. In order to
still find a good classifier, we allow some of the points <span class="math notranslate nohighlight">\({x}_i\)</span> to
lie within the margin or even on the wrong side of the hyperplane. For
this purpose, we rewrite the optimization constraint Eq.  to
$<span class="math notranslate nohighlight">\(y_i \tilde{{x}}_i^T{\beta} &gt; (1-\xi_i), \textrm{with } \xi_i \geq 0, \quad \forall i.
  \label{eq:notseparable}\)</span><span class="math notranslate nohighlight">\( We can now define the optimization problem
as finding
\)</span><span class="math notranslate nohighlight">\(\min_{{\beta},\{\xi_i\}} \frac12 \sum_{j=1}^{n} \beta_j^2 + C\sum_i \xi_i
  \label{eq:optimalclassifierbeta}\)</span><span class="math notranslate nohighlight">\( subject to the constraint Eq. .
Note that the second term with hyperparameter \)</span>C<span class="math notranslate nohighlight">\( acts like a
regularizer, in particular a lasso regularizer. As we have seen in the
example of the previous section, such a regularizer tries to set as many
\)</span>\xi_i$ to zero as possible.</p>
<p><img alt="[Binary classification.]{} Hyperplane separating the two classesand margin  of the linear binary classifier. The support vectors aredenoted by a circle aroundthem.[]{data-label=&quot;fig:svm&quot;}" src="docs/figures/SVM_overlap" /></p>
<p>We can solve this constrained minimization problem by introducing
Lagrange multipliers <span class="math notranslate nohighlight">\(\alpha_i\)</span> and <span class="math notranslate nohighlight">\(\mu_i\)</span> and solving
$<span class="math notranslate nohighlight">\(\min_{\beta, \{\xi_i\}} \frac12 \sum_{j=1}^{n} \beta_j^2 + C\sum_i \xi_i - \sum_i \alpha_i [y_i \tilde{{x}}_i^T{\beta} - (1-\xi_i)] - \sum_i\mu_i\xi_i,
  \label{eq:svm_lagrange}\)</span><span class="math notranslate nohighlight">\( which yields the conditions
\)</span><span class="math notranslate nohighlight">\(\begin{aligned}
  \beta_j &amp;=&amp; \sum_i \alpha_i y_i x_{ij},\label{eq:svm_beta}\\
  0 &amp;=&amp; \sum_i \alpha_i y_i\\
  \alpha_i &amp;=&amp; C-\mu_i, \quad \forall i.
\label{eq:svm_derivatives}\end{aligned}\)</span><span class="math notranslate nohighlight">\( It is numerically simpler to
solve the dual problem
\)</span><span class="math notranslate nohighlight">\(\min_{\{\alpha_i\}} \frac12 \sum_{i,i'} \alpha_i \alpha_{i'} y_i y_{i'} {x}_i^T {x}_{i'} - \sum_i \alpha_i
  \label{eq:svm_dual}\)</span><span class="math notranslate nohighlight">\( subject to \)</span>\sum_i \alpha_i y_i =0<span class="math notranslate nohighlight">\( and
\)</span>0\leq \alpha_i \leq C<span class="math notranslate nohighlight">\( [^1]. Using Eq. , we can reexpress \)</span>\beta_j<span class="math notranslate nohighlight">\( to
find
\)</span><span class="math notranslate nohighlight">\(f({x}|\{\alpha_i\}) = \sum_i{}' \alpha_i y_i {x}^T {x}_i + \beta_0,
  \label{eq:svm_f}\)</span><span class="math notranslate nohighlight">\( where the sum only runs over the points \)</span>{x}_i<span class="math notranslate nohighlight">\(,
which lie within the margin, as all other points have \)</span>\alpha_i\equiv0<span class="math notranslate nohighlight">\(
\[see Eq. \]. These points are thus called the *support vectors* and are
denoted in Fig. \[fig:svm\] with a circle around them. Finally, note
that we can use Eq.  again to find \)</span>\beta_0$.</p>
<p><strong>The Kernel trick and support vector machines</strong><br />
We have seen in our discussion of PCA that most data is not separable
linearly. However, we have also seen how the kernel trick can help us in
such situations. In particular, we have seen how a non-linear function
<span class="math notranslate nohighlight">\({\Phi}({x})\)</span>, which we first apply to the data <span class="math notranslate nohighlight">\({x}\)</span>, can help
us separate data that is not linearly separable. Importantly, we never
actually use the non-linear function <span class="math notranslate nohighlight">\({\Phi}({x})\)</span>, but only the
kernel. Looking at the dual optimization problem Eq.  and the resulting
classifier Eq. , we see that, as in the case of Kernel PCA, only the
kernel <span class="math notranslate nohighlight">\(K({x}, {y}) = {\Phi}({x})^T{\Phi}({y})\)</span>
enters, simplifying the problem. This non-linear extension of the binary
classifier is called a <em>support vector machine</em>.</p>
</div>
<div class="section" id="more-than-two-classes-logistic-regression">
<h2>More than two classes: logistic regression<a class="headerlink" href="#more-than-two-classes-logistic-regression" title="Permalink to this headline">¶</a></h2>
<p>In the following, we are interested in the case of <span class="math notranslate nohighlight">\(p\)</span> classes with
<span class="math notranslate nohighlight">\(p&gt;2\)</span>. After the previous discussion, it seems natural for the output to
take the integer values <span class="math notranslate nohighlight">\(y = 1, \dots, p\)</span>. However, it turns out to be
helpful to use a different, so-called <em>one-hot encoding</em>. In this
encoding, the output <span class="math notranslate nohighlight">\(y\)</span> is instead represented by the <span class="math notranslate nohighlight">\(p\)</span>-dimensional
unit vector in <span class="math notranslate nohighlight">\(y\)</span> direction <span class="math notranslate nohighlight">\({e}^{(y)}\)</span>,
$<span class="math notranslate nohighlight">\(\label{eqn: One-Hot Encoding}
    y \longrightarrow {e}^{(y)} =
    \begin{bmatrix}
        e^{(y)}_1 \\
        \vdots \\
        e^{(y)}_y \\
        \vdots \\
        e^{(y)}_{p}
    \end{bmatrix}
    =
    \begin{bmatrix}
        0 \\
        \vdots \\
        1 \\
        \vdots \\
        0
    \end{bmatrix},\)</span><span class="math notranslate nohighlight">\( where \)</span>e^{(y)}_l = 1<span class="math notranslate nohighlight">\( if \)</span>l = y<span class="math notranslate nohighlight">\( and zero for all
other \)</span>l=1,\ldots, p$. A main advantage of this encoding is that we are
not forced to choose a potentially biasing ordering of the classes as we
would when arranging them along the ray of integers.</p>
<p>A linear approach to this problem then again mirrors the case for linear
regression. We fit a multi-variate linear model, Eq. , to the one-hot
encoded dataset
<span class="math notranslate nohighlight">\(\lbrace({x}_{1}, {e}^{(y_1)}), \dots, ({x}_{m}, {e}^{(y_m)})\rbrace\)</span>.
By minimising the RSS, Eq. , we obtain the solution
$<span class="math notranslate nohighlight">\(\hat{\beta} = (\widetilde{X}^{T}\widetilde{X})^{-1} \widetilde{X}^{T} Y,\)</span><span class="math notranslate nohighlight">\(
where \)</span>Y<span class="math notranslate nohighlight">\( is the \)</span>m<span class="math notranslate nohighlight">\( by \)</span>p<span class="math notranslate nohighlight">\( output matrix. The prediction given an input
\)</span>{x}<span class="math notranslate nohighlight">\( is then a \)</span>p<span class="math notranslate nohighlight">\(-dimensional vector
\)</span>{f}({x}|\hat{\beta}) = \tilde{{x}}^{T} \hat{\beta}<span class="math notranslate nohighlight">\(. On a
generic input \)</span>{x}<span class="math notranslate nohighlight">\(, it is obvious that the components of this
prediction vector would be real valued, rather than being one of the
one-hot basis vectors. To obtain a class prediction
\)</span>F({x}|\hat{\beta}) = 1, \dots, p<span class="math notranslate nohighlight">\(, we simply take the index of the
largest component of that vector, i.e.,
\)</span><span class="math notranslate nohighlight">\(F({x}|\hat{\beta}) = \textrm{argmax}_{k} f_{k}({x}|\hat{\beta}).\)</span><span class="math notranslate nohighlight">\(
The \)</span>\textrm{argmax}$ function is a non-linear function and is a first
example of what is referred to as <em>activation function</em>.</p>
<p>For numerical minimization, it is better to use a smooth activation
function. Such an activation function is given by the <em>softmax</em> function
$<span class="math notranslate nohighlight">\(F_k({x}|\hat{\beta})= \frac{e^{-f_k({x}|\hat{\beta})}}{\sum_{k'=1}^pe^{-f_{k'}({x}|\hat{\beta})}}.\)</span><span class="math notranslate nohighlight">\(
Importantly, the output of the softmax function is a probability
\)</span>P(y = k|{x})<span class="math notranslate nohighlight">\(, since \)</span>\sum_k F_k({x}|\hat{\beta}) = 1$. This
extended linear model is referred to as <em>logistic regression</em> <a class="footnote-reference brackets" href="#id2" id="id1">1</a>.</p>
<p>The current linear approach based on classification of one-hot encoded
data generally works poorly when there are more than two classes. We
will see in the next chapter that relatively straightforward non-linear
extensions of this approach can lead to much better results.</p>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Note that the softmax function for two classes is the logistic
function.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="ml_without_neural_network-3.html" title="previous page">Supervised Learning without Neural Networks {#sec: linear methods for supervised learning}</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Eliska<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>