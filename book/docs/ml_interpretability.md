(sec:interpretability)=
# Interpretability of Neural Networks

In particular for applications in science, we not only want to obtain a neural network that excels at performing a given task, but we also seek an understanding how the problem was solved. Ideally, we want to know underlying principles, deduce causal relations, identify abstracted notions. This is the topic of interpretability.

## Dreaming and the problem of extrapolation

Exploring the weights and intermediate activations of a neural network
in order to understand what the network has learnt, very quickly becomes
unfeasible or uninformative for large network architectures. In this
case, we can try a different approach, where we focus on the inputs to
the neural network, rather than the intermediate activations.

More precisely, let us consider a neural network classifier $\mathbf{f}$,
depending on the weights $W$, which maps an input $\mathbf{x}$ to a
probability distribution over $n$ classes
$\mathbf{f}(\mathbf{x}|W) \in \mathbb{R}^{n}$, i.e.,

```{math}
F_{i}(\mathbf{x}|\theta) \geq 0 \ \ \  \mathrm{and} \ \ \  \sum_{i} F_{i}(\mathbf{x}|\theta) = 1.
```

We want to minimise the distance between the output of the network $\mathbf{f(\mathbf{x})}$ and a chosen target output $\mathbf{y}_{\textrm{target}}$, which can be done by minimizing the loss function

```{math}
:label: eqn:dreaming-loss
    L = |\mathbf{f}(\mathbf{x}) - \mathbf{y}_{\textrm{target}}|^{2}.
```

However, unlike in supervised learning where the loss minimization was done with
respect to the weights $W$ of the network, here we are interested in
minimizing with respect to the input $\mathbf{x}$ while keeping the weights
$W$ fixed. This can be achieved using gradient descent, i.e.

```{math}
:label: eqn:dreaming-update
    \mathbf{x} \rightarrow \mathbf{x} - \eta \frac{\partial L}{\partial \mathbf{x}},
```

where $\eta$ is the learning rate. With a sufficient number of
iterations, our initial input $\mathbf{x}^{0}$ will be transformed into the
final input $\mathbf{x}^{*}$ such that

$$\mathbf{f(\mathbf{x}^*)} \approx \mathbf{y}_{\textrm{target}}.$$

By choosing the target output to correspond to a particular class, e.g.,
$\mathbf{y}_{\textrm{target}} = (1, 0, 0, \dots)$, we are then essentially
finding examples of inputs which the network would classify as belonging
to the chosen class. This procedure is called *dreaming*.

Let us apply this technique to a binary classification example. We
consider a dataset [^1] consisting of images of healthy and unhealthy
plant leaves. Some samples from the dataset are shown in the top row of
{numref}`fig:dreaming`. After training a deep convolutional network
to classify the leaves (reaching a test accuracy of around $95\%$), we
start with a random image as our initial input $\mathbf{x}^{0}$ and perform
gradient descent on the input, as described above, to arrive at the
final image $\mathbf{x}^{*}$ which our network confidently classifies.

```{figure} ../_static/lecture_specific/interpretability/dreaming_examples.png
:name: fig:dreaming

**Plant leaves.** Top: Some samples from the plants dataset. Bottom:
Samples generated by using the \"dreaming\" procedure starting from
random noise.
```

In bottom row of {numref}`fig:dreaming`, we show three examples produced using the 'dreaming' technique. On first sight, it might be astonishing that the final image actually does not even remotely resemble a leaf. How could it be that the network has such a high accuracy of around $95\%$, yet we have here a confidently classified image which is essentially just noise. Although this seem surprising, a closer inspection reveals the problem: The noisy image $\mathbf{x}^{*}$ looks nothing like the samples in the dataset with which we trained our network. By feeding this image into our network, we are asking it to make an extrapolation, which as can be seen leads to an uncontrolled behavior. This is a key issue which plagues most data driven machine learning approaches. With few exceptions, it is very difficult to train a model capable of performing reliable extrapolations. Since scientific research is often in the business of making extrapolations, this is an extremely important point of caution to keep in mind.

While it might seem obvious that any model should only be predictive for
data that 'resembles' those in the training set, the precise meaning of
'resembles' is actually more subtle than one imagines. For example, if
one trains a ML model using a dataset of images captured using a Canon
camera but subsequently decide to use the model to make predictions on
images taken with a Nikon camera, we could actually be in for a
surprise. Even though the images may 'resemble' each other to our naked
eye, the different cameras can have a different noise profile which
might not be perceptible to the human eye. We shall see in the next
section that even such minute image distortions can already be
sufficient to completely confuse our model.

## Adversarial attacks

As we have seen, it is possible to modify the input $\mathbf{x}$ so that the
corresponding model approximates a chosen target output. This concept
can also be applied to generate *adverserial examples*, i.e. images
which have been intentionally modified to cause a model to misclassify
it. In addition, we usually want the modification to be minimal or
almost imperceptible to the human eye.

One common method for generating adversarial examples is known as the
*fast gradient sign method*. Starting from an input $\mathbf{x}^{0}$ which
our model correctly classifies, we choose a target output $\mathbf{y}^{*}$
which corresponds to a wrong classification, and follow the procedure
described in the previous section with a slight modification. Instead of
updating the input according to
Eq. [](eqn:dreaming-update) we use the following update rule:

```{math}
\mathbf{x} \rightarrow \mathbf{x} - \eta \  \textrm{sign}\left(\frac{\partial L}{\partial \mathbf{x}}\right),
```

where $L$ is given be Eq. [](eqn:dreaming-loss). The $\textrm{sign}(\dots) \in \lbrace -1, 1 \rbrace$ both serves to enhance the signal and also acts as constraint to limit the size of the modification. By choosing $\eta = \frac{\epsilon}{T}$ and performing only $T$ iterations, we can then guarantee that each component of the final input $\mathbf{x}^{*}$ satisfies

```{math}
|x^{*}_{i} - x^{0}_{i}| \leq \epsilon,
```
which is important since we want our final image $\mathbf{x}^{*}$ to be only minimally modified. We summarize this algorithm as follows:

```{admonition} Fast Gradient Sign Method
:name: alg:FGSM
**Input:** A classification model $\mathbf{f}$, a loss function $L$, an initial image $\mathbf{x}^{0}$, a target label $\mathbf{y}_{\textrm{target}}$, perturbation size $\epsilon$ and number of iterations $T$  <br />
**Output:** Adversarial example $\mathbf{x}^{*}$ with $|x^{*}_{i} - x^{0}_{i}| \leq \epsilon$  <br />
$\eta = \epsilon/T$ <br />
**for:** i=1\dots T **do**  <br />
$\quad$ $\mathbf{x} = \mathbf{x} - \eta \ \textrm{sign}\left(\frac{\partial L}{\partial \mathbf{x}}\right)$  <br />
**end** 
```


This process of generating adversarial examples is called an
*adversarial attack*, which we can classify under two broad categories:
*white box* and *black box* attacks. In a white box attack, the attacker
has full access to the network $\mathbf{f}$ and is thus able to compute or
estimate the gradients with respect to the input. On the other hand, in
a black box attack, the adversarial examples are generated without using
the target network $\mathbf{f}$. In this case, a possible strategy for the
attacker is to train his own model $\mathbf{g}$, find an adversarial example
for his model and use it against his target $\mathbf{f}$ without actually
having access to it. Although it might seem surprising, this strategy
has been found to work albeit with a lower success rate as compared to
white box methods. We shall illustrate these concepts in the example
below.

```{figure} ../_static/lecture_specific/interpretability/white_box_example.png
:name: fig:white-box-attack

**Adversarial examples.** Generated using the fast gradient sign
method with $T=1$ iteration and $\epsilon = 0.01$. The target model is
Google's *InceptionV3* deep convolutional network with a test accuracy
of $\sim 95\%$ on the binary (\"Healthy\" vs \"Unhealthy\") plants
dataset.
```

### Example

We shall use the same plant leaves classification example as above. The target model $\mathbf{f}$ which we want to 'attack' is a *pretrained* model using Google's well known *InceptionV3* deep convolutional neural network containing over $20$ million parameters[^2]. The model achieved a test accuracy of $\sim 95\%$. Assuming we have access to the gradients of the model $\mathbf{f}$, we can then consider a white box attack. Starting from an image in the dataset which the target model correctly classifies and applying the fast gradient sign method (Alg. [](alg:FGSM)) with $\epsilon=0.01$ and $T=1$, we obtain an adversarial image which differs from the original image by almost imperceptible amount of noise as depicted on the left of {numref}`fig:white-box-attack`. Any human would still correctly identify the image but yet the network, which has around $95\%$ accuracy has completely failed.

```{figure} ../_static/lecture_specific/interpretability/black_box_attack.png
:name: fig:black-box-attack

**Black Box Adversarial Attack.**
```

If, however, the gradients and outputs of the target model $\mathbf{f}$ are
hidden, the above white box attack strategy becomes unfeasible. In this
case, we can adopt the following 'black box attack' strategy. We train a
secondary model $\mathbf{g}$, and then applying the FGSM algorithm to
$\mathbf{g}$ to generate adversarial examples for $\mathbf{g}$. Note that it is
not necessary for $\mathbf{g}$ to have the same network architecture as the
target model $\mathbf{f}$. In fact, it is possible that we do not even know
the architecture of our target model.

Let us consider another pretrained network based on *MobileNet* containing about $2$ million parameters. After retraining the top classification layer of this model to a test accuracy of $\sim 95\%$, we apply the FGSM algorithm to generate some adversarial examples. If we now test these examples on our target model $\mathbf{f}$, we notice a significant drop in the accuracy as shown on the graph on the right of {numref}`fig:black-box-attack`. The fact that the drop in accuracy is greater for the black box generated adversarial images as compared to images with random noise (of the same scale) added to it, shows that adversarial images have some degree of transferability between models. As a side note, on the left of {numref}`fig:black-box-attack` we observe that black box attacks are more effective when only $T=1$ iteration of the FGSM algorithm is used, contrary to the situation for the white box attack. This is because, with more iterations, the method has a tendency towards overfitting the secondary model, resulting in adversarial images which are less transferable.

These forms of attacks highlight a serious vulnerability of such data
driven machine learning techniques. Defending against such attack is an
active area of research but it is largely a cat and mouse game between
the attacker and defender.

## Interpreting autoencoders

Previously we have learned about a broad scope of application of
generative models. We have seen that autoencoders can serve as powerful
generative models in the scientific context by extracting the compressed
representation of the input and using it to generate new instances of
the problem. It turns out that in the simple enough problems one can
find a meaningful interpretation of the latent representation that may
be novel enough to help us get new insight into the problem we are
analyzing.

In 2020, the group of Renato Renner considered a machine learning
perspective on one of the most historically important problems in
physics: Copernicus heliocentric system of the solar orbits. Via series
of careful and precise measurements of positions of objects in the night
sky, Copernicus conjectured that Sun is the center of the solar system
and other planets are orbiting around it. Let us now ask the following
question: is it possible to build a neural network that receives the
same observation angles Copernicus did and deduces the same conclusion
from them?

```{figure} ../_static/lecture_specific/interpretability/copernicus.png
:name: fig:copernicus

**The Copernicus problem.** Relation between angles in heliocentric
and geocentric coordinate
system.
```

Renner group inputted into the autoencoder the angles of Mars and Sun as
observed from Earth ($\alpha_{ES}$ and $\alpha_{EM}$ in {numref}`fig:copernicus`) in certain times and asked the autoencoder
to predict the angles at other times. When analyzing the trained model
they realized that the two latent neurons included in their model are
storing information in the **heliocentric coordinates**! In particular,
one observes that the information stored in the latent space is a linear
combination of angles between Sun and Mars, $\gamma_{SM}$ and Sun and
Earth $\gamma_{SE}$. In other words, just like Copernicus, the
autoencoder has learned, that the most efficient way to store the
information given is to transform it into the heliocentric coordinate
system.

While this fascinating example is a great way to show the generative
models can be interpreted in some important cases, in general the
question of interpretability is still very much open and subject to
ongoing research. In the instances discussed earlier in this book, like
generation of molecules, where the input is compressed through several
layers of transformations requiring a complex dictionary and the
dimension of the latent space is high, interpreting latent space becomes
increasingly challenging.

[^1]: Source: https://data.mendeley.com/datasets/tywbtsjrjv/1

[^2]: This is an example of *transfer learning*. The base model,
    InceptionV3, has been trained on a different classification dataset,
    *ImageNet*, with over $1000$ classes. To apply this network to our
    binary classification problem, we simply replace the top layer with
    a simple duo-output dense softmax layer. We keep the weights of the
    base model fixed and only train the top layer.
